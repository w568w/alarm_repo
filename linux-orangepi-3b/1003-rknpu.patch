diff --git a/arch/arm64/boot/dts/rockchip/rk3566.dtsi b/arch/arm64/boot/dts/rockchip/rk3566.dtsi
index 6c4b17d27..0570451c0 100644
--- a/arch/arm64/boot/dts/rockchip/rk3566.dtsi
+++ b/arch/arm64/boot/dts/rockchip/rk3566.dtsi
@@ -4,6 +4,240 @@
 
 / {
 	compatible = "rockchip,rk3566";
+
+	rknpu: npu@fde40000 {
+		compatible = "rockchip,rk3568-rknpu", "rockchip,rknpu";
+		reg = <0x0 0xfde40000 0x0 0x10000>;
+		interrupts = <GIC_SPI 151 IRQ_TYPE_LEVEL_HIGH>;
+		clocks = <&scmi_clk 2>, <&cru CLK_NPU>, <&cru ACLK_NPU>, <&cru HCLK_NPU>;
+		clock-names = "scmi_clk", "clk", "aclk", "hclk";
+		assigned-clocks = <&cru CLK_NPU>;
+		assigned-clock-rates = <600000000>;
+		resets = <&cru SRST_A_NPU>, <&cru SRST_H_NPU>;
+		reset-names = "srst_a", "srst_h";
+		power-domains = <&power RK3568_PD_NPU>;
+		operating-points-v2 = <&npu_opp_table>;
+		iommus = <&rknpu_mmu>;
+		status = "disabled";
+	};
+
+	bus_npu: bus-npu {
+		compatible = "rockchip,rk3568-bus";
+		rockchip,busfreq-policy = "clkfreq";
+		clocks = <&scmi_clk 2>;
+		clock-names = "bus";
+		operating-points-v2 = <&bus_npu_opp_table>;
+		status = "disabled";
+	};
+
+	bus_npu_opp_table: bus-npu-opp-table {
+		compatible = "operating-points-v2";
+		opp-shared;
+
+		nvmem-cells = <&core_pvtm>;
+		nvmem-cell-names = "pvtm";
+		rockchip,pvtm-voltage-sel = <
+			0        84000   0
+			84001    91000   1
+			91001    100000  2
+		>;
+		rockchip,pvtm-ch = <0 5>;
+
+		opp-700000000 {
+			opp-hz = /bits/ 64 <700000000>;
+			opp-microvolt = <900000>;
+			opp-microvolt-L0 = <900000>;
+			opp-microvolt-L1 = <850000>;
+			opp-microvolt-L2 = <850000>;
+		};
+		opp-900000000 {
+			opp-hz = /bits/ 64 <900000000>;
+			opp-microvolt = <900000>;
+		};
+		opp-1000000000 {
+			opp-hz = /bits/ 64 <1000000000>;
+			opp-microvolt = <950000>;
+			opp-microvolt-L0 = <950000>;
+			opp-microvolt-L1 = <925000>;
+			opp-microvolt-L2 = <850000>;
+		};
+	};
+
+	rknpu_mmu: iommu@fde4b000 {
+		compatible = "rockchip,iommu-v2";
+		reg = <0x0 0xfde4b000 0x0 0x40>;
+		interrupts = <GIC_SPI 151 IRQ_TYPE_LEVEL_HIGH>;
+		interrupt-names = "rknpu_mmu";
+		clocks = <&cru ACLK_NPU>, <&cru HCLK_NPU>;
+		clock-names = "aclk", "iface";
+		power-domains = <&power RK3568_PD_NPU>;
+		#iommu-cells = <0>;
+		status = "disabled";
+	};
+
+	npu_opp_table: npu-opp-table {
+		compatible = "operating-points-v2";
+
+		mbist-vmin = <825000 900000 950000>;
+		nvmem-cells = <&npu_leakage>, <&core_pvtm>, <&mbist_vmin>, <&npu_opp_info>;
+		nvmem-cell-names = "leakage", "pvtm", "mbist-vmin", "opp-info";
+		rockchip,max-volt = <1000000>;
+		rockchip,temp-hysteresis = <5000>;
+		rockchip,low-temp = <0>;
+		rockchip,low-temp-adjust-volt = <
+			/* MHz    MHz    uV */
+			   0      1000    50000
+		>;
+		rockchip,pvtm-voltage-sel = <
+			0        84000   0
+			84001    87000   1
+			87001    91000   2
+			91001    100000  3
+		>;
+		rockchip,pvtm-ch = <0 5>;
+
+		opp-200000000 {
+			opp-hz = /bits/ 64 <200000000>;
+			opp-microvolt = <850000 850000 1000000>;
+		};
+		opp-300000000 {
+			opp-hz = /bits/ 64 <297000000>;
+			opp-microvolt = <850000 850000 1000000>;
+		};
+		opp-400000000 {
+			opp-hz = /bits/ 64 <400000000>;
+			opp-microvolt = <850000 850000 1000000>;
+		};
+		opp-600000000 {
+			opp-hz = /bits/ 64 <600000000>;
+			opp-microvolt = <850000 850000 1000000>;
+		};
+		opp-700000000 {
+			opp-hz = /bits/ 64 <700000000>;
+			opp-microvolt = <875000 875000 1000000>;
+			opp-microvolt-L0 = <875000 875000 1000000>;
+			opp-microvolt-L1 = <850000 850000 1000000>;
+			opp-microvolt-L2 = <850000 850000 1000000>;
+			opp-microvolt-L3 = <850000 850000 1000000>;
+		};
+		opp-800000000 {
+			opp-hz = /bits/ 64 <800000000>;
+			opp-microvolt = <925000 925000 1000000>;
+			opp-microvolt-L0 = <925000 925000 1000000>;
+			opp-microvolt-L1 = <900000 900000 1000000>;
+			opp-microvolt-L2 = <875000 875000 1000000>;
+			opp-microvolt-L3 = <875000 875000 1000000>;
+		};
+		opp-900000000 {
+			opp-hz = /bits/ 64 <900000000>;
+			opp-microvolt = <975000 975000 1000000>;
+			opp-microvolt-L0 = <975000 975000 1000000>;
+			opp-microvolt-L1 = <950000 950000 1000000>;
+			opp-microvolt-L2 = <925000 925000 1000000>;
+			opp-microvolt-L3 = <900000 900000 1000000>;
+		};
+		opp-1000000000 {
+			opp-hz = /bits/ 64 <1000000000>;
+			opp-microvolt = <1000000 1000000 1000000>;
+			opp-microvolt-L0 = <1000000 1000000 1000000>;
+			opp-microvolt-L1 = <975000 975000 1000000>;
+			opp-microvolt-L2 = <950000 950000 1000000>;
+			opp-microvolt-L3 = <925000 925000 1000000>;
+			status = "disabled";
+		};
+	};
+
+	otp: otp@fe38c000 {
+		compatible = "rockchip,rk3568-otp";
+		reg = <0x0 0xfe38c000 0x0 0x4000>;
+		#address-cells = <1>;
+		#size-cells = <1>;
+		clocks = <&cru CLK_OTPC_NS_USR>, <&cru CLK_OTPC_NS_SBPI>,
+			 <&cru PCLK_OTPC_NS>, <&cru PCLK_OTPPHY>;
+		clock-names = "usr", "sbpi", "apb", "phy";
+		resets = <&cru SRST_OTPPHY>;
+		reset-names = "otp_phy";
+
+		/* Data cells */
+		cpu_code: cpu-code@2 {
+			reg = <0x02 0x2>;
+		};
+		otp_cpu_version: cpu-version@8 {
+			reg = <0x08 0x1>;
+			bits = <3 3>;
+		};
+		mbist_vmin: mbist-vmin@9 {
+			reg = <0x09 0x1>;
+			bits = <0 4>;
+		};
+		otp_id: id@a {
+			reg = <0x0a 0x10>;
+		};
+		cpu_leakage: cpu-leakage@1a {
+			reg = <0x1a 0x1>;
+		};
+		log_leakage: log-leakage@1b {
+			reg = <0x1b 0x1>;
+		};
+		npu_leakage: npu-leakage@1c {
+			reg = <0x1c 0x1>;
+		};
+		gpu_leakage: gpu-leakage@1d {
+			reg = <0x1d 0x1>;
+		};
+		core_pvtm:core-pvtm@2a {
+			reg = <0x2a 0x2>;
+		};
+		cpu_tsadc_trim_l: cpu-tsadc-trim-l@2e {
+			reg = <0x2e 0x1>;
+		};
+		cpu_tsadc_trim_h: cpu-tsadc-trim-h@2f {
+			reg = <0x2f 0x1>;
+			bits = <0 4>;
+		};
+		gpu_tsadc_trim_l: npu-tsadc-trim-l@30 {
+			reg = <0x30 0x1>;
+		};
+		gpu_tsadc_trim_h: npu-tsadc-trim-h@31 {
+			reg = <0x31 0x1>;
+			bits = <0 4>;
+		};
+		tsadc_trim_base_frac: tsadc-trim-base-frac@31 {
+			reg = <0x31 0x1>;
+			bits = <4 4>;
+		};
+		tsadc_trim_base: tsadc-trim-base@32 {
+			reg = <0x32 0x1>;
+		};
+		cpu_opp_info: cpu-opp-info@36 {
+			reg = <0x36 0x6>;
+		};
+		gpu_opp_info: gpu-opp-info@3c {
+			reg = <0x3c 0x6>;
+		};
+		npu_opp_info: npu-opp-info@42 {
+			reg = <0x42 0x6>;
+		};
+		dmc_opp_info: dmc-opp-info@48 {
+			reg = <0x48 0x6>;
+		};
+	};
+
+	pvtm@fde90000 {
+		compatible = "rockchip,rk3568-npu-pvtm";
+		reg = <0x0 0xfde90000 0x0 0x100>;
+		#address-cells = <1>;
+		#size-cells = <0>;
+		pvtm@2 {
+			reg = <2>;
+			clocks = <&cru CLK_NPU_PVTM>, <&cru PCLK_NPU_PVTM>,
+				 <&cru HCLK_NPU_PRE>;
+			clock-names = "clk", "pclk", "hclk";
+			resets = <&cru SRST_NPU_PVTM>, <&cru SRST_P_NPU_PVTM>;
+			reset-names = "rts", "rst-p";
+			thermal-zone = "soc-thermal";
+		};
+	};
 };
 
 &pipegrf {
diff --git a/arch/arm64/boot/dts/rockchip/rk356x.dtsi b/arch/arm64/boot/dts/rockchip/rk356x.dtsi
index abee88911..f07e62ede 100644
--- a/arch/arm64/boot/dts/rockchip/rk356x.dtsi
+++ b/arch/arm64/boot/dts/rockchip/rk356x.dtsi
@@ -509,6 +509,15 @@ power: power-controller {
 			#address-cells = <1>;
 			#size-cells = <0>;
 
+			/* These power domains are grouped by VD_NPU */
+			pd_npu@RK3568_PD_NPU {
+				reg = <RK3568_PD_NPU>;
+				clocks = <&cru ACLK_NPU_PRE>,
+					 <&cru HCLK_NPU_PRE>,
+					 <&cru PCLK_NPU_PRE>;
+				pm_qos = <&qos_npu>;
+			};
+			
 			/* These power domains are grouped by VD_GPU */
 			power-domain@RK3568_PD_GPU {
 				reg = <RK3568_PD_GPU>;
diff --git a/drivers/Kconfig b/drivers/Kconfig
index efb66e25f..c0da5f7e8 100644
--- a/drivers/Kconfig
+++ b/drivers/Kconfig
@@ -243,4 +243,6 @@ source "drivers/hte/Kconfig"
 
 source "drivers/cdx/Kconfig"
 
+source "drivers/rknpu/Kconfig"
+
 endmenu
diff --git a/drivers/Makefile b/drivers/Makefile
index 1bec7819a..af95975a2 100644
--- a/drivers/Makefile
+++ b/drivers/Makefile
@@ -199,3 +199,5 @@ obj-$(CONFIG_DRM_ACCEL)		+= accel/
 obj-$(CONFIG_CDX_BUS)		+= cdx/
 
 obj-$(CONFIG_S390)		+= s390/
+
+obj-$(CONFIG_ROCKCHIP_RKNPU)	+= rknpu/
\ No newline at end of file
diff --git a/drivers/clk/rockchip/clk-pll.c b/drivers/clk/rockchip/clk-pll.c
index 2d42eb628..c8f22e0be 100644
--- a/drivers/clk/rockchip/clk-pll.c
+++ b/drivers/clk/rockchip/clk-pll.c
@@ -47,6 +47,56 @@ struct rockchip_clk_pll {
 #define to_rockchip_clk_pll_nb(nb) \
 			container_of(nb, struct rockchip_clk_pll, clk_nb)
 
+// TODO: a dummy implementation of deprecated rockchip_pll_clk_rate_to_scale
+// & rockchip_pll_clk_scale_to_rate
+int rockchip_pll_clk_rate_to_scale(struct clk *clk, unsigned long rate)
+{
+	const struct rockchip_pll_rate_table *rate_table;
+	struct clk *parent = clk_get_parent(clk);
+	struct rockchip_clk_pll *pll;
+	unsigned int i;
+
+	if (IS_ERR_OR_NULL(parent))
+		return -EINVAL;
+
+	pll = to_rockchip_clk_pll(__clk_get_hw(parent));
+	if (!pll)
+		return -EINVAL;
+
+	rate_table = pll->rate_table;
+	for (i = 0; i < pll->rate_count; i++) {
+		if (rate >= rate_table[i].rate)
+			return i;
+	}
+
+	return -EINVAL;
+}
+EXPORT_SYMBOL(rockchip_pll_clk_rate_to_scale);
+
+int rockchip_pll_clk_scale_to_rate(struct clk *clk, unsigned int scale)
+{
+	const struct rockchip_pll_rate_table *rate_table;
+	struct clk *parent = clk_get_parent(clk);
+	struct rockchip_clk_pll *pll;
+	unsigned int i;
+
+	if (IS_ERR_OR_NULL(parent))
+		return -EINVAL;
+
+	pll = to_rockchip_clk_pll(__clk_get_hw(parent));
+	if (!pll)
+		return -EINVAL;
+
+	rate_table = pll->rate_table;
+	for (i = 0; i < pll->rate_count; i++) {
+		if (i == scale)
+			return rate_table[i].rate;
+	}
+
+	return -EINVAL;
+}
+EXPORT_SYMBOL(rockchip_pll_clk_scale_to_rate);
+
 static const struct rockchip_pll_rate_table *rockchip_get_pll_settings(
 			    struct rockchip_clk_pll *pll, unsigned long rate)
 {
diff --git a/drivers/dma-buf/Makefile b/drivers/dma-buf/Makefile
index 70ec901ed..a12caf66b 100644
--- a/drivers/dma-buf/Makefile
+++ b/drivers/dma-buf/Makefile
@@ -7,6 +7,7 @@ obj-$(CONFIG_SYNC_FILE)		+= sync_file.o
 obj-$(CONFIG_SW_SYNC)		+= sw_sync.o sync_debug.o
 obj-$(CONFIG_UDMABUF)		+= udmabuf.o
 obj-$(CONFIG_DMABUF_SYSFS_STATS) += dma-buf-sysfs-stats.o
+obj-$(CONFIG_DMABUF_HEAPS_ROCKCHIP) += rk_heaps/
 
 dmabuf_selftests-y := \
 	selftest.o \
diff --git a/drivers/dma-buf/rk_heaps/Kconfig b/drivers/dma-buf/rk_heaps/Kconfig
new file mode 100644
index 000000000..6ca3fbe76
--- /dev/null
+++ b/drivers/dma-buf/rk_heaps/Kconfig
@@ -0,0 +1,48 @@
+# SPDX-License-Identifier: GPL-2.0-only
+menuconfig DMABUF_HEAPS_ROCKCHIP
+	bool "DMA-BUF Userland Memory Heaps for RockChip"
+	select DMA_SHARED_BUFFER
+	help
+	  Choose this option to enable the RockChip DMA-BUF userland memory heaps.
+	  This options creates per heap chardevs in /dev/rk_dma_heap/ which
+	  allows userspace to allocate dma-bufs that can be shared
+	  between drivers.
+
+config DMABUF_HEAPS_ROCKCHIP_CMA_HEAP
+	tristate "DMA-BUF RockChip CMA Heap"
+	depends on DMABUF_HEAPS_ROCKCHIP
+	help
+	  Choose this option to enable dma-buf RockChip CMA heap. This heap is backed
+	  by the Contiguous Memory Allocator (CMA). If your system has these
+	  regions, you should say Y here.
+
+config DMABUF_HEAPS_ROCKCHIP_CMA_ALIGNMENT
+	int "Maximum PAGE_SIZE order of alignment for RockChip CMA Heap"
+	range 0 12
+	depends on DMABUF_HEAPS_ROCKCHIP_CMA_HEAP
+	default 8
+	help
+	  DMA mapping framework by default aligns all buffers to the smallest
+	  PAGE_SIZE order which is greater than or equal to the requested buffer
+	  size. This works well for buffers up to a few hundreds kilobytes, but
+	  for larger buffers it just a memory waste. With this parameter you can
+	  specify the maximum PAGE_SIZE order for contiguous buffers. Larger
+	  buffers will be aligned only to this specified order. The order is
+	  expressed as a power of two multiplied by the PAGE_SIZE.
+
+	  For example, if your system defaults to 4KiB pages, the order value
+	  of 8 means that the buffers will be aligned up to 1MiB only.
+
+	  If unsure, leave the default value "8".
+
+config DMABUF_RK_HEAPS_DEBUG
+	bool "DMA-BUF RockChip Heap Debug"
+	depends on DMABUF_HEAPS_ROCKCHIP
+	help
+	  Choose this option to enable dma-buf RockChip heap debug.
+
+config DMABUF_RK_HEAPS_DEBUG_PRINT
+	bool "DMA-BUF RockChip Heap Debug print log enable"
+	depends on DMABUF_HEAPS_ROCKCHIP
+	help
+	  Choose this option to enable dma-buf RockChip heap debug.
diff --git a/drivers/dma-buf/rk_heaps/Makefile b/drivers/dma-buf/rk_heaps/Makefile
new file mode 100644
index 000000000..30d44bb7d
--- /dev/null
+++ b/drivers/dma-buf/rk_heaps/Makefile
@@ -0,0 +1,6 @@
+# SPDX-License-Identifier: GPL-2.0
+
+rk-cma-heap-objs := rk-dma-cma.o rk-cma-heap.o
+
+obj-$(CONFIG_DMABUF_HEAPS_ROCKCHIP) += rk-dma-heap.o
+obj-$(CONFIG_DMABUF_HEAPS_ROCKCHIP_CMA_HEAP) += rk-cma-heap.o
diff --git a/drivers/dma-buf/rk_heaps/rk-cma-heap.c b/drivers/dma-buf/rk_heaps/rk-cma-heap.c
new file mode 100644
index 000000000..6bd1ee47b
--- /dev/null
+++ b/drivers/dma-buf/rk_heaps/rk-cma-heap.c
@@ -0,0 +1,681 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * DMABUF CMA heap exporter
+ *
+ * Copyright (C) 2012, 2019, 2020 Linaro Ltd.
+ * Author: <benjamin.gaignard@linaro.org> for ST-Ericsson.
+ *
+ * Also utilizing parts of Andrew Davis' SRAM heap:
+ * Copyright (C) 2019 Texas Instruments Incorporated - http://www.ti.com/
+ *	Andrew F. Davis <afd@ti.com>
+ *
+ * Copyright (C) 2022 Rockchip Electronics Co. Ltd.
+ * Author: Simon Xue <xxm@rock-chips.com>
+ */
+
+#include <linux/cma.h>
+#include <linux/dma-buf.h>
+#include <linux/dma-map-ops.h>
+#include <linux/err.h>
+#include <linux/highmem.h>
+#include <linux/io.h>
+#include <linux/mm.h>
+#include <linux/module.h>
+#include <linux/scatterlist.h>
+#include <linux/slab.h>
+#include <linux/vmalloc.h>
+#include <uapi/linux/rk-dma-heap.h>
+#include <linux/proc_fs.h>
+#include "../../../mm/cma.h"
+#include "rk-dma-heap.h"
+
+struct rk_cma_heap {
+	struct rk_dma_heap *heap;
+	struct cma *cma;
+};
+
+struct rk_cma_heap_buffer {
+	struct rk_cma_heap *heap;
+	struct list_head attachments;
+	struct mutex lock;
+	unsigned long len;
+	struct page *cma_pages;
+	struct page **pages;
+	pgoff_t pagecount;
+	int vmap_cnt;
+	void *vaddr;
+	phys_addr_t phys;
+	bool attached;
+};
+
+struct rk_cma_heap_attachment {
+	struct device *dev;
+	struct sg_table table;
+	struct list_head list;
+	bool mapped;
+};
+
+static int rk_cma_heap_attach(struct dma_buf *dmabuf,
+			      struct dma_buf_attachment *attachment)
+{
+	struct rk_cma_heap_buffer *buffer = dmabuf->priv;
+	struct rk_cma_heap_attachment *a;
+	struct sg_table *table;
+	size_t size = buffer->pagecount << PAGE_SHIFT;
+	int ret;
+
+	a = kzalloc(sizeof(*a), GFP_KERNEL);
+	if (!a)
+		return -ENOMEM;
+
+	table = &a->table;
+
+	ret = sg_alloc_table(table, 1, GFP_KERNEL);
+	if (ret) {
+		kfree(a);
+		return ret;
+	}
+	sg_set_page(table->sgl, buffer->cma_pages, PAGE_ALIGN(size), 0);
+
+	a->dev = attachment->dev;
+	INIT_LIST_HEAD(&a->list);
+	a->mapped = false;
+
+	attachment->priv = a;
+
+	buffer->attached = true;
+
+	mutex_lock(&buffer->lock);
+	list_add(&a->list, &buffer->attachments);
+	mutex_unlock(&buffer->lock);
+
+	return 0;
+}
+
+static void rk_cma_heap_detach(struct dma_buf *dmabuf,
+			       struct dma_buf_attachment *attachment)
+{
+	struct rk_cma_heap_buffer *buffer = dmabuf->priv;
+	struct rk_cma_heap_attachment *a = attachment->priv;
+
+	mutex_lock(&buffer->lock);
+	list_del(&a->list);
+	mutex_unlock(&buffer->lock);
+
+	buffer->attached = false;
+
+	sg_free_table(&a->table);
+	kfree(a);
+}
+
+static struct sg_table *rk_cma_heap_map_dma_buf(struct dma_buf_attachment *attachment,
+						enum dma_data_direction direction)
+{
+	struct rk_cma_heap_attachment *a = attachment->priv;
+	struct sg_table *table = &a->table;
+	int attrs = attachment->dma_map_attrs;
+	int ret;
+
+	ret = dma_map_sgtable(attachment->dev, table, direction, attrs);
+	if (ret)
+		return ERR_PTR(-ENOMEM);
+	a->mapped = true;
+	return table;
+}
+
+static void rk_cma_heap_unmap_dma_buf(struct dma_buf_attachment *attachment,
+				      struct sg_table *table,
+				      enum dma_data_direction direction)
+{
+	struct rk_cma_heap_attachment *a = attachment->priv;
+	int attrs = attachment->dma_map_attrs;
+
+	a->mapped = false;
+	dma_unmap_sgtable(attachment->dev, table, direction, attrs);
+}
+
+static int
+rk_cma_heap_dma_buf_begin_cpu_access_partial(struct dma_buf *dmabuf,
+					     enum dma_data_direction direction,
+					     unsigned int offset,
+					     unsigned int len)
+{
+	struct rk_cma_heap_buffer *buffer = dmabuf->priv;
+	struct rk_cma_heap_attachment *a;
+
+	if (buffer->vmap_cnt)
+		invalidate_kernel_vmap_range(buffer->vaddr, buffer->len);
+
+	mutex_lock(&buffer->lock);
+	list_for_each_entry(a, &buffer->attachments, list) {
+		if (!a->mapped)
+			continue;
+		dma_sync_sgtable_for_cpu(a->dev, &a->table, direction);
+	}
+
+	/* For userspace that not attach yet */
+	if (buffer->phys && !buffer->attached)
+		dma_sync_single_for_cpu(rk_dma_heap_get_dev(buffer->heap->heap),
+					buffer->phys + offset,
+					len,
+					direction);
+	mutex_unlock(&buffer->lock);
+
+	return 0;
+}
+
+static int
+rk_cma_heap_dma_buf_end_cpu_access_partial(struct dma_buf *dmabuf,
+					   enum dma_data_direction direction,
+					   unsigned int offset,
+					   unsigned int len)
+{
+	struct rk_cma_heap_buffer *buffer = dmabuf->priv;
+	struct rk_cma_heap_attachment *a;
+
+	if (buffer->vmap_cnt)
+		flush_kernel_vmap_range(buffer->vaddr, buffer->len);
+
+	mutex_lock(&buffer->lock);
+	list_for_each_entry(a, &buffer->attachments, list) {
+		if (!a->mapped)
+			continue;
+		dma_sync_sgtable_for_device(a->dev, &a->table, direction);
+	}
+
+	/* For userspace that not attach yet */
+	if (buffer->phys && !buffer->attached)
+		dma_sync_single_for_device(rk_dma_heap_get_dev(buffer->heap->heap),
+					   buffer->phys + offset,
+					   len,
+					   direction);
+	mutex_unlock(&buffer->lock);
+
+	return 0;
+}
+
+static int rk_cma_heap_dma_buf_begin_cpu_access(struct dma_buf *dmabuf,
+						enum dma_data_direction dir)
+{
+	struct rk_cma_heap_buffer *buffer = dmabuf->priv;
+	unsigned int len = buffer->pagecount * PAGE_SIZE;
+
+	return rk_cma_heap_dma_buf_begin_cpu_access_partial(dmabuf, dir, 0, len);
+}
+
+static int rk_cma_heap_dma_buf_end_cpu_access(struct dma_buf *dmabuf,
+					      enum dma_data_direction dir)
+{
+	struct rk_cma_heap_buffer *buffer = dmabuf->priv;
+	unsigned int len = buffer->pagecount * PAGE_SIZE;
+
+	return rk_cma_heap_dma_buf_end_cpu_access_partial(dmabuf, dir, 0, len);
+}
+
+static int rk_cma_heap_mmap(struct dma_buf *dmabuf, struct vm_area_struct *vma)
+{
+	struct rk_cma_heap_buffer *buffer = dmabuf->priv;
+	size_t size = vma->vm_end - vma->vm_start;
+	int ret;
+
+	ret = remap_pfn_range(vma, vma->vm_start, __phys_to_pfn(buffer->phys),
+			      size, vma->vm_page_prot);
+	if (ret)
+		return -EAGAIN;
+
+	return 0;
+}
+
+static void *rk_cma_heap_do_vmap(struct rk_cma_heap_buffer *buffer)
+{
+	void *vaddr;
+	pgprot_t pgprot = PAGE_KERNEL;
+
+	vaddr = vmap(buffer->pages, buffer->pagecount, VM_MAP, pgprot);
+	if (!vaddr)
+		return ERR_PTR(-ENOMEM);
+
+	return vaddr;
+}
+
+static void *rk_cma_heap_vmap(struct dma_buf *dmabuf)
+{
+	struct rk_cma_heap_buffer *buffer = dmabuf->priv;
+	void *vaddr;
+
+	mutex_lock(&buffer->lock);
+	if (buffer->vmap_cnt) {
+		buffer->vmap_cnt++;
+		vaddr = buffer->vaddr;
+		goto out;
+	}
+
+	vaddr = rk_cma_heap_do_vmap(buffer);
+	if (IS_ERR(vaddr))
+		goto out;
+
+	buffer->vaddr = vaddr;
+	buffer->vmap_cnt++;
+out:
+	mutex_unlock(&buffer->lock);
+
+	return vaddr;
+}
+
+static void rk_cma_heap_vunmap(struct dma_buf *dmabuf, void *vaddr)
+{
+	struct rk_cma_heap_buffer *buffer = dmabuf->priv;
+
+	mutex_lock(&buffer->lock);
+	if (!--buffer->vmap_cnt) {
+		vunmap(buffer->vaddr);
+		buffer->vaddr = NULL;
+	}
+	mutex_unlock(&buffer->lock);
+}
+
+static void rk_cma_heap_remove_dmabuf_list(struct dma_buf *dmabuf)
+{
+	struct rk_dma_heap_dmabuf *buf;
+	struct rk_cma_heap_buffer *buffer = dmabuf->priv;
+	struct rk_cma_heap *cma_heap = buffer->heap;
+	struct rk_dma_heap *heap = cma_heap->heap;
+
+	mutex_lock(&heap->dmabuf_lock);
+	list_for_each_entry(buf, &heap->dmabuf_list, node) {
+		if (buf->dmabuf == dmabuf) {
+			dma_heap_print("<%s> free dmabuf<ino-%ld>@[%pa-%pa] to heap-<%s>\n",
+				       dmabuf->name,
+				       dmabuf->file->f_inode->i_ino,
+				       &buf->start, &buf->end,
+				       rk_dma_heap_get_name(heap));
+			list_del(&buf->node);
+			kfree(buf);
+			break;
+		}
+	}
+	mutex_unlock(&heap->dmabuf_lock);
+}
+
+static int rk_cma_heap_add_dmabuf_list(struct dma_buf *dmabuf, const char *name)
+{
+	struct rk_dma_heap_dmabuf *buf;
+	struct rk_cma_heap_buffer *buffer = dmabuf->priv;
+	struct rk_cma_heap *cma_heap = buffer->heap;
+	struct rk_dma_heap *heap = cma_heap->heap;
+
+	buf = kzalloc(sizeof(*buf), GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	INIT_LIST_HEAD(&buf->node);
+	buf->dmabuf = dmabuf;
+	buf->start = buffer->phys;
+	buf->end = buf->start + buffer->len - 1;
+	mutex_lock(&heap->dmabuf_lock);
+	list_add_tail(&buf->node, &heap->dmabuf_list);
+	mutex_unlock(&heap->dmabuf_lock);
+
+	dma_heap_print("<%s> alloc dmabuf<ino-%ld>@[%pa-%pa] from heap-<%s>\n",
+		       dmabuf->name, dmabuf->file->f_inode->i_ino,
+		       &buf->start, &buf->end, rk_dma_heap_get_name(heap));
+
+	return 0;
+}
+
+static int rk_cma_heap_remove_contig_list(struct rk_dma_heap *heap,
+					  struct page *page, const char *name)
+{
+	struct rk_dma_heap_contig_buf *buf;
+
+	mutex_lock(&heap->contig_lock);
+	list_for_each_entry(buf, &heap->contig_list, node) {
+		if (buf->start == page_to_phys(page)) {
+			dma_heap_print("<%s> free contig-buf@[%pa-%pa] to heap-<%s>\n",
+				       buf->orig_alloc, &buf->start, &buf->end,
+				       rk_dma_heap_get_name(heap));
+			list_del(&buf->node);
+			kfree(buf->orig_alloc);
+			kfree(buf);
+			break;
+		}
+	}
+	mutex_unlock(&heap->contig_lock);
+
+	return 0;
+}
+
+static int rk_cma_heap_add_contig_list(struct rk_dma_heap *heap,
+				       struct page *page, unsigned long size,
+				       const char *name)
+{
+	struct rk_dma_heap_contig_buf *buf;
+	const char *name_tmp;
+
+	buf = kzalloc(sizeof(*buf), GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	INIT_LIST_HEAD(&buf->node);
+	if (!name)
+		name_tmp = current->comm;
+	else
+		name_tmp = name;
+
+	buf->orig_alloc = kstrndup(name_tmp, RK_DMA_HEAP_NAME_LEN, GFP_KERNEL);
+	if (!buf->orig_alloc) {
+		kfree(buf);
+		return -ENOMEM;
+	}
+
+	buf->start = page_to_phys(page);
+	buf->end = buf->start + size - 1;
+	mutex_lock(&heap->contig_lock);
+	list_add_tail(&buf->node, &heap->contig_list);
+	mutex_unlock(&heap->contig_lock);
+
+	dma_heap_print("<%s> alloc contig-buf@[%pa-%pa] from heap-<%s>\n",
+		       buf->orig_alloc, &buf->start, &buf->end,
+		       rk_dma_heap_get_name(heap));
+
+	return 0;
+}
+
+static void rk_cma_heap_dma_buf_release(struct dma_buf *dmabuf)
+{
+	struct rk_cma_heap_buffer *buffer = dmabuf->priv;
+	struct rk_cma_heap *cma_heap = buffer->heap;
+	struct rk_dma_heap *heap = cma_heap->heap;
+
+	if (buffer->vmap_cnt > 0) {
+		WARN(1, "%s: buffer still mapped in the kernel\n", __func__);
+		vunmap(buffer->vaddr);
+	}
+
+	rk_cma_heap_remove_dmabuf_list(dmabuf);
+
+	/* free page list */
+	kfree(buffer->pages);
+	/* release memory */
+	cma_release(cma_heap->cma, buffer->cma_pages, buffer->pagecount);
+	rk_dma_heap_total_dec(heap, buffer->len);
+
+	kfree(buffer);
+}
+
+static const struct dma_buf_ops rk_cma_heap_buf_ops = {
+	.cache_sgt_mapping = true,
+	.attach = rk_cma_heap_attach,
+	.detach = rk_cma_heap_detach,
+	.map_dma_buf = rk_cma_heap_map_dma_buf,
+	.unmap_dma_buf = rk_cma_heap_unmap_dma_buf,
+	.begin_cpu_access = rk_cma_heap_dma_buf_begin_cpu_access,
+	.end_cpu_access = rk_cma_heap_dma_buf_end_cpu_access,
+	.begin_cpu_access_partial = rk_cma_heap_dma_buf_begin_cpu_access_partial,
+	.end_cpu_access_partial = rk_cma_heap_dma_buf_end_cpu_access_partial,
+	.mmap = rk_cma_heap_mmap,
+	.vmap = rk_cma_heap_vmap,
+	.vunmap = rk_cma_heap_vunmap,
+	.release = rk_cma_heap_dma_buf_release,
+};
+
+static struct dma_buf *rk_cma_heap_allocate(struct rk_dma_heap *heap,
+					    unsigned long len,
+					    unsigned long fd_flags,
+					    unsigned long heap_flags,
+					    const char *name)
+{
+	struct rk_cma_heap *cma_heap = rk_dma_heap_get_drvdata(heap);
+	struct rk_cma_heap_buffer *buffer;
+	DEFINE_DMA_BUF_EXPORT_INFO(exp_info);
+	size_t size = PAGE_ALIGN(len);
+	pgoff_t pagecount = size >> PAGE_SHIFT;
+	unsigned long align = get_order(size);
+	struct page *cma_pages;
+	struct dma_buf *dmabuf;
+	pgoff_t pg;
+	int ret = -ENOMEM;
+
+	buffer = kzalloc(sizeof(*buffer), GFP_KERNEL);
+	if (!buffer)
+		return ERR_PTR(-ENOMEM);
+
+	INIT_LIST_HEAD(&buffer->attachments);
+	mutex_init(&buffer->lock);
+	buffer->len = size;
+
+	if (align > CONFIG_DMABUF_HEAPS_ROCKCHIP_CMA_ALIGNMENT)
+		align = CONFIG_DMABUF_HEAPS_ROCKCHIP_CMA_ALIGNMENT;
+
+	cma_pages = cma_alloc(cma_heap->cma, pagecount, align, GFP_KERNEL);
+	if (!cma_pages)
+		goto free_buffer;
+
+	/* Clear the cma pages */
+	if (PageHighMem(cma_pages)) {
+		unsigned long nr_clear_pages = pagecount;
+		struct page *page = cma_pages;
+
+		while (nr_clear_pages > 0) {
+			void *vaddr = kmap_atomic(page);
+
+			memset(vaddr, 0, PAGE_SIZE);
+			kunmap_atomic(vaddr);
+			/*
+			 * Avoid wasting time zeroing memory if the process
+			 * has been killed by SIGKILL
+			 */
+			if (fatal_signal_pending(current))
+				goto free_cma;
+			page++;
+			nr_clear_pages--;
+		}
+	} else {
+		memset(page_address(cma_pages), 0, size);
+	}
+
+	buffer->pages = kmalloc_array(pagecount, sizeof(*buffer->pages),
+				      GFP_KERNEL);
+	if (!buffer->pages) {
+		ret = -ENOMEM;
+		goto free_cma;
+	}
+
+	for (pg = 0; pg < pagecount; pg++)
+		buffer->pages[pg] = &cma_pages[pg];
+
+	buffer->cma_pages = cma_pages;
+	buffer->heap = cma_heap;
+	buffer->pagecount = pagecount;
+
+	/* create the dmabuf */
+	exp_info.exp_name = rk_dma_heap_get_name(heap);
+	exp_info.ops = &rk_cma_heap_buf_ops;
+	exp_info.size = buffer->len;
+	exp_info.flags = fd_flags;
+	exp_info.priv = buffer;
+	dmabuf = dma_buf_export(&exp_info);
+	if (IS_ERR(dmabuf)) {
+		ret = PTR_ERR(dmabuf);
+		goto free_pages;
+	}
+
+	buffer->phys = page_to_phys(cma_pages);
+	dma_sync_single_for_cpu(rk_dma_heap_get_dev(heap), buffer->phys,
+				buffer->pagecount * PAGE_SIZE,
+				DMA_FROM_DEVICE);
+
+	ret = rk_cma_heap_add_dmabuf_list(dmabuf, name);
+	if (ret)
+		goto fail_dma_buf;
+
+	rk_dma_heap_total_inc(heap, buffer->len);
+
+	return dmabuf;
+
+fail_dma_buf:
+	dma_buf_put(dmabuf);
+free_pages:
+	kfree(buffer->pages);
+free_cma:
+	cma_release(cma_heap->cma, cma_pages, pagecount);
+free_buffer:
+	kfree(buffer);
+
+	return ERR_PTR(ret);
+}
+
+static struct page *rk_cma_heap_allocate_pages(struct rk_dma_heap *heap,
+					       size_t len, const char *name)
+{
+	struct rk_cma_heap *cma_heap = rk_dma_heap_get_drvdata(heap);
+	size_t size = PAGE_ALIGN(len);
+	pgoff_t pagecount = size >> PAGE_SHIFT;
+	unsigned long align = get_order(size);
+	struct page *page;
+	int ret;
+
+	if (align > CONFIG_DMABUF_HEAPS_ROCKCHIP_CMA_ALIGNMENT)
+		align = CONFIG_DMABUF_HEAPS_ROCKCHIP_CMA_ALIGNMENT;
+
+	page = cma_alloc(cma_heap->cma, pagecount, align, GFP_KERNEL);
+	if (!page)
+		return ERR_PTR(-ENOMEM);
+
+	ret = rk_cma_heap_add_contig_list(heap, page, size, name);
+	if (ret) {
+		cma_release(cma_heap->cma, page, pagecount);
+		return ERR_PTR(-EINVAL);
+	}
+
+	rk_dma_heap_total_inc(heap, size);
+
+	return page;
+}
+
+static void rk_cma_heap_free_pages(struct rk_dma_heap *heap,
+				   struct page *page, size_t len,
+				   const char *name)
+{
+	struct rk_cma_heap *cma_heap = rk_dma_heap_get_drvdata(heap);
+	pgoff_t pagecount = len >> PAGE_SHIFT;
+
+	rk_cma_heap_remove_contig_list(heap, page, name);
+
+	cma_release(cma_heap->cma, page, pagecount);
+
+	rk_dma_heap_total_dec(heap, len);
+}
+
+static const struct rk_dma_heap_ops rk_cma_heap_ops = {
+	.allocate = rk_cma_heap_allocate,
+	.alloc_contig_pages = rk_cma_heap_allocate_pages,
+	.free_contig_pages = rk_cma_heap_free_pages,
+};
+
+static int cma_procfs_show(struct seq_file *s, void *private);
+
+static int __rk_add_cma_heap(struct cma *cma, void *data)
+{
+	struct rk_cma_heap *cma_heap;
+	struct rk_dma_heap_export_info exp_info;
+
+	cma_heap = kzalloc(sizeof(*cma_heap), GFP_KERNEL);
+	if (!cma_heap)
+		return -ENOMEM;
+	cma_heap->cma = cma;
+
+	exp_info.name = cma_get_name(cma);
+	exp_info.ops = &rk_cma_heap_ops;
+	exp_info.priv = cma_heap;
+	exp_info.support_cma = true;
+
+	cma_heap->heap = rk_dma_heap_add(&exp_info);
+	if (IS_ERR(cma_heap->heap)) {
+		int ret = PTR_ERR(cma_heap->heap);
+
+		kfree(cma_heap);
+		return ret;
+	}
+
+	if (cma_heap->heap->procfs)
+		proc_create_single_data("alloc_bitmap", 0, cma_heap->heap->procfs,
+					cma_procfs_show, cma);
+
+	return 0;
+}
+
+static int __init rk_add_default_cma_heap(void)
+{
+	struct cma *cma = rk_dma_heap_get_cma();
+
+	if (WARN_ON(!cma))
+		return -EINVAL;
+
+	return __rk_add_cma_heap(cma, NULL);
+}
+
+#if defined(CONFIG_VIDEO_ROCKCHIP_THUNDER_BOOT_ISP) && !defined(CONFIG_INITCALL_ASYNC)
+subsys_initcall(rk_add_default_cma_heap);
+#else
+module_init(rk_add_default_cma_heap);
+#endif
+
+static void cma_procfs_format_array(char *buf, size_t bufsize, u32 *array, int array_size)
+{
+	int i = 0;
+
+	while (--array_size >= 0) {
+		size_t len;
+		char term = (array_size && (++i % 8)) ? ' ' : '\n';
+
+		len = snprintf(buf, bufsize, "%08X%c", *array++, term);
+		buf += len;
+		bufsize -= len;
+	}
+}
+
+static void cma_procfs_show_bitmap(struct seq_file *s, struct cma *cma)
+{
+	int elements = DIV_ROUND_UP(cma_bitmap_maxno(cma), BITS_PER_BYTE * sizeof(u32));
+	int size = elements * 9;
+	u32 *array = (u32 *)cma->bitmap;
+	char *buf;
+
+	buf = kmalloc(size + 1, GFP_KERNEL);
+	if (!buf)
+		return;
+
+	buf[size] = 0;
+
+	cma_procfs_format_array(buf, size + 1, array, elements);
+	seq_printf(s, "%s", buf);
+	kfree(buf);
+}
+
+static u64 cma_procfs_used_get(struct cma *cma)
+{
+	unsigned long used;
+
+	mutex_lock(&cma->lock);
+	used = bitmap_weight(cma->bitmap, (int)cma_bitmap_maxno(cma));
+	mutex_unlock(&cma->lock);
+
+	return (u64)used << cma->order_per_bit;
+}
+
+static int cma_procfs_show(struct seq_file *s, void *private)
+{
+	struct cma *cma = s->private;
+	u64 used = cma_procfs_used_get(cma);
+
+	seq_printf(s, "Total: %lu KiB\n", cma->count << (PAGE_SHIFT - 10));
+	seq_printf(s, " Used: %llu KiB\n\n", used << (PAGE_SHIFT - 10));
+
+	cma_procfs_show_bitmap(s, cma);
+
+	return 0;
+}
+
+MODULE_DESCRIPTION("RockChip DMA-BUF CMA Heap");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/dma-buf/rk_heaps/rk-dma-cma.c b/drivers/dma-buf/rk_heaps/rk-dma-cma.c
new file mode 100644
index 000000000..b6521f7dc
--- /dev/null
+++ b/drivers/dma-buf/rk_heaps/rk-dma-cma.c
@@ -0,0 +1,77 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Early setup for Rockchip DMA CMA
+ *
+ * Copyright (C) 2022 Rockchip Electronics Co. Ltd.
+ * Author: Simon Xue <xxm@rock-chips.com>
+ */
+
+#include <linux/cma.h>
+#include <linux/dma-map-ops.h>
+
+#include "rk-dma-heap.h"
+
+#define RK_DMA_HEAP_CMA_DEFAULT_SIZE SZ_32M
+
+static unsigned long rk_dma_heap_size __initdata;
+static unsigned long rk_dma_heap_base __initdata;
+
+static struct cma *rk_dma_heap_cma;
+
+static int __init early_dma_heap_cma(char *p)
+{
+	if (!p) {
+		pr_err("Config string not provided\n");
+		return -EINVAL;
+	}
+
+	rk_dma_heap_size = memparse(p, &p);
+	if (*p != '@')
+		return 0;
+
+	rk_dma_heap_base = memparse(p + 1, &p);
+
+	return 0;
+}
+early_param("rk_dma_heap_cma", early_dma_heap_cma);
+
+#ifndef CONFIG_DMA_CMA
+void __weak
+dma_contiguous_early_fixup(phys_addr_t base, unsigned long size)
+{
+}
+#endif
+
+int __init rk_dma_heap_cma_setup(void)
+{
+	unsigned long size;
+	int ret;
+	bool fix = false;
+
+	if (rk_dma_heap_size)
+		size = rk_dma_heap_size;
+	else
+		size = RK_DMA_HEAP_CMA_DEFAULT_SIZE;
+
+	if (rk_dma_heap_base)
+		fix = true;
+
+	ret = cma_declare_contiguous(rk_dma_heap_base, PAGE_ALIGN(size), 0x0,
+				     PAGE_SIZE, 0, fix, "rk-dma-heap-cma",
+				     &rk_dma_heap_cma);
+	if (ret)
+		return ret;
+
+#if !IS_ENABLED(CONFIG_CMA_INACTIVE)
+	/* Architecture specific contiguous memory fixup. */
+	dma_contiguous_early_fixup(cma_get_base(rk_dma_heap_cma),
+	cma_get_size(rk_dma_heap_cma));
+#endif
+
+	return 0;
+}
+
+struct cma *rk_dma_heap_get_cma(void)
+{
+	return rk_dma_heap_cma;
+}
diff --git a/drivers/dma-buf/rk_heaps/rk-dma-heap.c b/drivers/dma-buf/rk_heaps/rk-dma-heap.c
new file mode 100644
index 000000000..d93d7cd30
--- /dev/null
+++ b/drivers/dma-buf/rk_heaps/rk-dma-heap.c
@@ -0,0 +1,733 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Framework for userspace DMA-BUF allocations
+ *
+ * Copyright (C) 2011 Google, Inc.
+ * Copyright (C) 2019 Linaro Ltd.
+ * Copyright (C) 2022 Rockchip Electronics Co. Ltd.
+ * Author: Simon Xue <xxm@rock-chips.com>
+ */
+
+#include <linux/cma.h>
+#include <linux/cdev.h>
+#include <linux/debugfs.h>
+#include <linux/device.h>
+#include <linux/dma-buf.h>
+#include <linux/dma-resv.h>
+#include <linux/dma-map-ops.h>
+#include <linux/err.h>
+#include <linux/xarray.h>
+#include <linux/list.h>
+#include <linux/proc_fs.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/syscalls.h>
+#include <uapi/linux/rk-dma-heap.h>
+
+#include "rk-dma-heap.h"
+
+#define DEVNAME "rk_dma_heap"
+
+#define NUM_HEAP_MINORS 128
+
+static LIST_HEAD(rk_heap_list);
+static DEFINE_MUTEX(rk_heap_list_lock);
+static dev_t rk_dma_heap_devt;
+static struct class *rk_dma_heap_class;
+static DEFINE_XARRAY_ALLOC(rk_dma_heap_minors);
+struct proc_dir_entry *proc_rk_dma_heap_dir;
+
+#define K(size) ((unsigned long)((size) >> 10))
+
+static int rk_vmap_pfn_apply(pte_t *pte, unsigned long addr, void *private)
+{
+	struct rk_vmap_pfn_data *data = private;
+
+	*pte = pte_mkspecial(pfn_pte(data->pfn++, data->prot));
+	return 0;
+}
+
+void *rk_vmap_contig_pfn(unsigned long pfn, unsigned int count, pgprot_t prot)
+{
+	struct rk_vmap_pfn_data data = { .pfn = pfn, .prot = pgprot_nx(prot) };
+	struct vm_struct *area;
+
+	area = get_vm_area_caller(count * PAGE_SIZE, VM_MAP,
+			__builtin_return_address(0));
+	if (!area)
+		return NULL;
+	if (apply_to_page_range(&init_mm, (unsigned long)area->addr,
+			count * PAGE_SIZE, rk_vmap_pfn_apply, &data)) {
+		free_vm_area(area);
+		return NULL;
+	}
+	return area->addr;
+}
+
+int rk_dma_heap_set_dev(struct device *heap_dev)
+{
+	int err = 0;
+
+	if (!heap_dev)
+		return -EINVAL;
+
+	dma_coerce_mask_and_coherent(heap_dev, DMA_BIT_MASK(64));
+
+	if (!heap_dev->dma_parms) {
+		heap_dev->dma_parms = devm_kzalloc(heap_dev,
+						   sizeof(*heap_dev->dma_parms),
+						   GFP_KERNEL);
+		if (!heap_dev->dma_parms)
+			return -ENOMEM;
+
+		err = dma_set_max_seg_size(heap_dev, (unsigned int)DMA_BIT_MASK(64));
+		if (err) {
+			devm_kfree(heap_dev, heap_dev->dma_parms);
+			dev_err(heap_dev, "Failed to set DMA segment size, err:%d\n", err);
+			return err;
+		}
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(rk_dma_heap_set_dev);
+
+struct rk_dma_heap *rk_dma_heap_find(const char *name)
+{
+	struct rk_dma_heap *h;
+
+	mutex_lock(&rk_heap_list_lock);
+	list_for_each_entry(h, &rk_heap_list, list) {
+		if (!strcmp(h->name, name)) {
+			kref_get(&h->refcount);
+			mutex_unlock(&rk_heap_list_lock);
+			return h;
+		}
+	}
+	mutex_unlock(&rk_heap_list_lock);
+	return NULL;
+}
+EXPORT_SYMBOL_GPL(rk_dma_heap_find);
+
+void rk_dma_heap_buffer_free(struct dma_buf *dmabuf)
+{
+	dma_buf_put(dmabuf);
+}
+EXPORT_SYMBOL_GPL(rk_dma_heap_buffer_free);
+
+struct dma_buf *rk_dma_heap_buffer_alloc(struct rk_dma_heap *heap, size_t len,
+					 unsigned int fd_flags,
+					 unsigned int heap_flags,
+					 const char *name)
+{
+	struct dma_buf *dmabuf;
+
+	if (fd_flags & ~RK_DMA_HEAP_VALID_FD_FLAGS)
+		return ERR_PTR(-EINVAL);
+
+	if (heap_flags & ~RK_DMA_HEAP_VALID_HEAP_FLAGS)
+		return ERR_PTR(-EINVAL);
+	/*
+	 * Allocations from all heaps have to begin
+	 * and end on page boundaries.
+	 */
+	len = PAGE_ALIGN(len);
+	if (!len)
+		return ERR_PTR(-EINVAL);
+
+	dmabuf = heap->ops->allocate(heap, len, fd_flags, heap_flags, name);
+
+	if (IS_ENABLED(CONFIG_DMABUF_RK_HEAPS_DEBUG) && !IS_ERR(dmabuf))
+		dma_buf_set_name(dmabuf, name);
+
+	return dmabuf;
+}
+EXPORT_SYMBOL_GPL(rk_dma_heap_buffer_alloc);
+
+int rk_dma_heap_bufferfd_alloc(struct rk_dma_heap *heap, size_t len,
+			       unsigned int fd_flags,
+			       unsigned int heap_flags,
+			       const char *name)
+{
+	struct dma_buf *dmabuf;
+	int fd;
+
+	dmabuf = rk_dma_heap_buffer_alloc(heap, len, fd_flags, heap_flags,
+					  name);
+
+	if (IS_ERR(dmabuf))
+		return PTR_ERR(dmabuf);
+
+	fd = dma_buf_fd(dmabuf, fd_flags);
+	if (fd < 0) {
+		dma_buf_put(dmabuf);
+		/* just return, as put will call release and that will free */
+	}
+
+	return fd;
+
+}
+EXPORT_SYMBOL_GPL(rk_dma_heap_bufferfd_alloc);
+
+struct page *rk_dma_heap_alloc_contig_pages(struct rk_dma_heap *heap,
+					    size_t len, const char *name)
+{
+	if (!heap->support_cma) {
+		WARN_ON(!heap->support_cma);
+		return ERR_PTR(-EINVAL);
+	}
+
+	len = PAGE_ALIGN(len);
+	if (!len)
+		return ERR_PTR(-EINVAL);
+
+	return heap->ops->alloc_contig_pages(heap, len, name);
+}
+EXPORT_SYMBOL_GPL(rk_dma_heap_alloc_contig_pages);
+
+void rk_dma_heap_free_contig_pages(struct rk_dma_heap *heap,
+				   struct page *pages, size_t len,
+				   const char *name)
+{
+	if (!heap->support_cma) {
+		WARN_ON(!heap->support_cma);
+		return;
+	}
+
+	return heap->ops->free_contig_pages(heap, pages, len, name);
+}
+EXPORT_SYMBOL_GPL(rk_dma_heap_free_contig_pages);
+
+void rk_dma_heap_total_inc(struct rk_dma_heap *heap, size_t len)
+{
+	mutex_lock(&rk_heap_list_lock);
+	heap->total_size += len;
+	mutex_unlock(&rk_heap_list_lock);
+}
+
+void rk_dma_heap_total_dec(struct rk_dma_heap *heap, size_t len)
+{
+	mutex_lock(&rk_heap_list_lock);
+	if (WARN_ON(heap->total_size < len))
+		heap->total_size = 0;
+	else
+		heap->total_size -= len;
+	mutex_unlock(&rk_heap_list_lock);
+}
+
+static int rk_dma_heap_open(struct inode *inode, struct file *file)
+{
+	struct rk_dma_heap *heap;
+
+	heap = xa_load(&rk_dma_heap_minors, iminor(inode));
+	if (!heap) {
+		pr_err("dma_heap: minor %d unknown.\n", iminor(inode));
+		return -ENODEV;
+	}
+
+	/* instance data as context */
+	file->private_data = heap;
+	nonseekable_open(inode, file);
+
+	return 0;
+}
+
+static long rk_dma_heap_ioctl_allocate(struct file *file, void *data)
+{
+	struct rk_dma_heap_allocation_data *heap_allocation = data;
+	struct rk_dma_heap *heap = file->private_data;
+	int fd;
+
+	if (heap_allocation->fd)
+		return -EINVAL;
+
+	fd = rk_dma_heap_bufferfd_alloc(heap, heap_allocation->len,
+					heap_allocation->fd_flags,
+					heap_allocation->heap_flags, NULL);
+	if (fd < 0)
+		return fd;
+
+	heap_allocation->fd = fd;
+
+	return 0;
+}
+
+static unsigned int rk_dma_heap_ioctl_cmds[] = {
+	RK_DMA_HEAP_IOCTL_ALLOC,
+};
+
+static long rk_dma_heap_ioctl(struct file *file, unsigned int ucmd,
+			      unsigned long arg)
+{
+	char stack_kdata[128];
+	char *kdata = stack_kdata;
+	unsigned int kcmd;
+	unsigned int in_size, out_size, drv_size, ksize;
+	int nr = _IOC_NR(ucmd);
+	int ret = 0;
+
+	if (nr >= ARRAY_SIZE(rk_dma_heap_ioctl_cmds))
+		return -EINVAL;
+
+	/* Get the kernel ioctl cmd that matches */
+	kcmd = rk_dma_heap_ioctl_cmds[nr];
+
+	/* Figure out the delta between user cmd size and kernel cmd size */
+	drv_size = _IOC_SIZE(kcmd);
+	out_size = _IOC_SIZE(ucmd);
+	in_size = out_size;
+	if ((ucmd & kcmd & IOC_IN) == 0)
+		in_size = 0;
+	if ((ucmd & kcmd & IOC_OUT) == 0)
+		out_size = 0;
+	ksize = max(max(in_size, out_size), drv_size);
+
+	/* If necessary, allocate buffer for ioctl argument */
+	if (ksize > sizeof(stack_kdata)) {
+		kdata = kmalloc(ksize, GFP_KERNEL);
+		if (!kdata)
+			return -ENOMEM;
+	}
+
+	if (copy_from_user(kdata, (void __user *)arg, in_size) != 0) {
+		ret = -EFAULT;
+		goto err;
+	}
+
+	/* zero out any difference between the kernel/user structure size */
+	if (ksize > in_size)
+		memset(kdata + in_size, 0, ksize - in_size);
+
+	switch (kcmd) {
+	case RK_DMA_HEAP_IOCTL_ALLOC:
+		ret = rk_dma_heap_ioctl_allocate(file, kdata);
+		break;
+	default:
+		ret = -ENOTTY;
+		goto err;
+	}
+
+	if (copy_to_user((void __user *)arg, kdata, out_size) != 0)
+		ret = -EFAULT;
+err:
+	if (kdata != stack_kdata)
+		kfree(kdata);
+	return ret;
+}
+
+static const struct file_operations rk_dma_heap_fops = {
+	.owner          = THIS_MODULE,
+	.open		= rk_dma_heap_open,
+	.unlocked_ioctl = rk_dma_heap_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl	= rk_dma_heap_ioctl,
+#endif
+};
+
+/**
+ * rk_dma_heap_get_drvdata() - get per-subdriver data for the heap
+ * @heap: DMA-Heap to retrieve private data for
+ *
+ * Returns:
+ * The per-subdriver data for the heap.
+ */
+void *rk_dma_heap_get_drvdata(struct rk_dma_heap *heap)
+{
+	return heap->priv;
+}
+
+static void rk_dma_heap_release(struct kref *ref)
+{
+	struct rk_dma_heap *heap = container_of(ref, struct rk_dma_heap, refcount);
+	int minor = MINOR(heap->heap_devt);
+
+	/* Note, we already holding the rk_heap_list_lock here */
+	list_del(&heap->list);
+
+	device_destroy(rk_dma_heap_class, heap->heap_devt);
+	cdev_del(&heap->heap_cdev);
+	xa_erase(&rk_dma_heap_minors, minor);
+
+	kfree(heap);
+}
+
+void rk_dma_heap_put(struct rk_dma_heap *h)
+{
+	/*
+	 * Take the rk_heap_list_lock now to avoid racing with code
+	 * scanning the list and then taking a kref.
+	 */
+	mutex_lock(&rk_heap_list_lock);
+	kref_put(&h->refcount, rk_dma_heap_release);
+	mutex_unlock(&rk_heap_list_lock);
+}
+
+/**
+ * rk_dma_heap_get_dev() - get device struct for the heap
+ * @heap: DMA-Heap to retrieve device struct from
+ *
+ * Returns:
+ * The device struct for the heap.
+ */
+struct device *rk_dma_heap_get_dev(struct rk_dma_heap *heap)
+{
+	return heap->heap_dev;
+}
+
+/**
+ * rk_dma_heap_get_name() - get heap name
+ * @heap: DMA-Heap to retrieve private data for
+ *
+ * Returns:
+ * The char* for the heap name.
+ */
+const char *rk_dma_heap_get_name(struct rk_dma_heap *heap)
+{
+	return heap->name;
+}
+
+struct rk_dma_heap *rk_dma_heap_add(const struct rk_dma_heap_export_info *exp_info)
+{
+	struct rk_dma_heap *heap, *err_ret;
+	unsigned int minor;
+	int ret;
+
+	if (!exp_info->name || !strcmp(exp_info->name, "")) {
+		pr_err("rk_dma_heap: Cannot add heap without a name\n");
+		return ERR_PTR(-EINVAL);
+	}
+
+	if (!exp_info->ops || !exp_info->ops->allocate) {
+		pr_err("rk_dma_heap: Cannot add heap with invalid ops struct\n");
+		return ERR_PTR(-EINVAL);
+	}
+
+	/* check the name is unique */
+	heap = rk_dma_heap_find(exp_info->name);
+	if (heap) {
+		pr_err("rk_dma_heap: Already registered heap named %s\n",
+		       exp_info->name);
+		rk_dma_heap_put(heap);
+		return ERR_PTR(-EINVAL);
+	}
+
+	heap = kzalloc(sizeof(*heap), GFP_KERNEL);
+	if (!heap)
+		return ERR_PTR(-ENOMEM);
+
+	kref_init(&heap->refcount);
+	heap->name = exp_info->name;
+	heap->ops = exp_info->ops;
+	heap->priv = exp_info->priv;
+	heap->support_cma = exp_info->support_cma;
+	INIT_LIST_HEAD(&heap->dmabuf_list);
+	INIT_LIST_HEAD(&heap->contig_list);
+	mutex_init(&heap->dmabuf_lock);
+	mutex_init(&heap->contig_lock);
+
+	/* Find unused minor number */
+	ret = xa_alloc(&rk_dma_heap_minors, &minor, heap,
+		       XA_LIMIT(0, NUM_HEAP_MINORS - 1), GFP_KERNEL);
+	if (ret < 0) {
+		pr_err("rk_dma_heap: Unable to get minor number for heap\n");
+		err_ret = ERR_PTR(ret);
+		goto err0;
+	}
+
+	/* Create device */
+	heap->heap_devt = MKDEV(MAJOR(rk_dma_heap_devt), minor);
+
+	cdev_init(&heap->heap_cdev, &rk_dma_heap_fops);
+	ret = cdev_add(&heap->heap_cdev, heap->heap_devt, 1);
+	if (ret < 0) {
+		pr_err("dma_heap: Unable to add char device\n");
+		err_ret = ERR_PTR(ret);
+		goto err1;
+	}
+
+	heap->heap_dev = device_create(rk_dma_heap_class,
+				       NULL,
+				       heap->heap_devt,
+				       NULL,
+				       heap->name);
+	if (IS_ERR(heap->heap_dev)) {
+		pr_err("rk_dma_heap: Unable to create device\n");
+		err_ret = ERR_CAST(heap->heap_dev);
+		goto err2;
+	}
+
+	heap->procfs = proc_rk_dma_heap_dir;
+
+	/* Make sure it doesn't disappear on us */
+	heap->heap_dev = get_device(heap->heap_dev);
+
+	/* Add heap to the list */
+	mutex_lock(&rk_heap_list_lock);
+	list_add(&heap->list, &rk_heap_list);
+	mutex_unlock(&rk_heap_list_lock);
+
+	return heap;
+
+err2:
+	cdev_del(&heap->heap_cdev);
+err1:
+	xa_erase(&rk_dma_heap_minors, minor);
+err0:
+	kfree(heap);
+	return err_ret;
+}
+
+static char *rk_dma_heap_devnode(struct device *dev, umode_t *mode)
+{
+	return kasprintf(GFP_KERNEL, "rk_dma_heap/%s", dev_name(dev));
+}
+
+static int rk_dma_heap_dump_dmabuf(const struct dma_buf *dmabuf, void *data)
+{
+	struct rk_dma_heap *heap = (struct rk_dma_heap *)data;
+	struct rk_dma_heap_dmabuf *buf;
+	struct dma_buf_attachment *a;
+	phys_addr_t size;
+	int attach_count;
+	int ret;
+
+	if (!strcmp(dmabuf->exp_name, heap->name)) {
+		seq_printf(heap->s, "dma-heap:<%s> -dmabuf", heap->name);
+		mutex_lock(&heap->dmabuf_lock);
+		list_for_each_entry(buf, &heap->dmabuf_list, node) {
+			if (buf->dmabuf->file->f_inode->i_ino ==
+				dmabuf->file->f_inode->i_ino) {
+				seq_printf(heap->s,
+					   "\ti_ino = %ld\n",
+					   dmabuf->file->f_inode->i_ino);
+				size = buf->end - buf->start + 1;
+				seq_printf(heap->s,
+					   "\tAlloc by (%-20s)\t[%pa-%pa]\t%pa (%lu KiB)\n",
+					   dmabuf->name, &buf->start,
+					   &buf->end, &size, K(size));
+				seq_puts(heap->s, "\t\tAttached Devices:\n");
+				attach_count = 0;
+				ret = dma_resv_lock_interruptible(dmabuf->resv,
+								  NULL);
+				if (ret)
+					goto error_unlock;
+				list_for_each_entry(a, &dmabuf->attachments,
+						    node) {
+					seq_printf(heap->s, "\t\t%s\n",
+						   dev_name(a->dev));
+					attach_count++;
+				}
+				dma_resv_unlock(dmabuf->resv);
+				seq_printf(heap->s,
+					   "Total %d devices attached\n\n",
+					   attach_count);
+			}
+		}
+		mutex_unlock(&heap->dmabuf_lock);
+	}
+
+	return 0;
+error_unlock:
+	mutex_unlock(&heap->dmabuf_lock);
+	return ret;
+}
+
+static int rk_dma_heap_dump_contig(void *data)
+{
+	struct rk_dma_heap *heap = (struct rk_dma_heap *)data;
+	struct rk_dma_heap_contig_buf *buf;
+	phys_addr_t size;
+
+	mutex_lock(&heap->contig_lock);
+	list_for_each_entry(buf, &heap->contig_list, node) {
+		size = buf->end - buf->start + 1;
+		seq_printf(heap->s, "dma-heap:<%s> -non dmabuf\n", heap->name);
+		seq_printf(heap->s, "\tAlloc by (%-20s)\t[%pa-%pa]\t%pa (%lu KiB)\n",
+			   buf->orig_alloc, &buf->start, &buf->end, &size, K(size));
+	}
+	mutex_unlock(&heap->contig_lock);
+
+	return 0;
+}
+
+static ssize_t rk_total_pools_kb_show(struct kobject *kobj,
+				      struct kobj_attribute *attr, char *buf)
+{
+	struct rk_dma_heap *heap;
+	u64 total_pool_size = 0;
+
+	mutex_lock(&rk_heap_list_lock);
+	list_for_each_entry(heap, &rk_heap_list, list)
+		if (heap->ops->get_pool_size)
+			total_pool_size += heap->ops->get_pool_size(heap);
+	mutex_unlock(&rk_heap_list_lock);
+
+	return sysfs_emit(buf, "%llu\n", total_pool_size / 1024);
+}
+
+static struct kobj_attribute rk_total_pools_kb_attr =
+	__ATTR_RO(rk_total_pools_kb);
+
+static struct attribute *rk_dma_heap_sysfs_attrs[] = {
+	&rk_total_pools_kb_attr.attr,
+	NULL,
+};
+
+ATTRIBUTE_GROUPS(rk_dma_heap_sysfs);
+
+static struct kobject *rk_dma_heap_kobject;
+
+static int rk_dma_heap_sysfs_setup(void)
+{
+	int ret;
+
+	rk_dma_heap_kobject = kobject_create_and_add("rk_dma_heap",
+						     kernel_kobj);
+	if (!rk_dma_heap_kobject)
+		return -ENOMEM;
+
+	ret = sysfs_create_groups(rk_dma_heap_kobject,
+				  rk_dma_heap_sysfs_groups);
+	if (ret) {
+		kobject_put(rk_dma_heap_kobject);
+		return ret;
+	}
+
+	return 0;
+}
+
+static void rk_dma_heap_sysfs_teardown(void)
+{
+	kobject_put(rk_dma_heap_kobject);
+}
+
+#ifdef CONFIG_DEBUG_FS
+
+static struct dentry *rk_dma_heap_debugfs_dir;
+
+static int rk_dma_heap_debug_show(struct seq_file *s, void *unused)
+{
+	struct rk_dma_heap *heap;
+	unsigned long total = 0;
+
+	mutex_lock(&rk_heap_list_lock);
+	list_for_each_entry(heap, &rk_heap_list, list) {
+		heap->s = s;
+		get_each_dmabuf(rk_dma_heap_dump_dmabuf, heap);
+		rk_dma_heap_dump_contig(heap);
+		total += heap->total_size;
+	}
+	seq_printf(s, "\nTotal : 0x%lx (%lu KiB)\n", total, K(total));
+	mutex_unlock(&rk_heap_list_lock);
+
+	return 0;
+}
+DEFINE_SHOW_ATTRIBUTE(rk_dma_heap_debug);
+
+static int rk_dma_heap_init_debugfs(void)
+{
+	struct dentry *d;
+	int err = 0;
+
+	d = debugfs_create_dir("rk_dma_heap", NULL);
+	if (IS_ERR(d))
+		return PTR_ERR(d);
+
+	rk_dma_heap_debugfs_dir = d;
+
+	d = debugfs_create_file("dma_heap_info", 0444,
+				rk_dma_heap_debugfs_dir, NULL,
+				&rk_dma_heap_debug_fops);
+	if (IS_ERR(d)) {
+		dma_heap_print("rk_dma_heap : debugfs: failed to create node bufinfo\n");
+		debugfs_remove_recursive(rk_dma_heap_debugfs_dir);
+		rk_dma_heap_debugfs_dir = NULL;
+		err = PTR_ERR(d);
+	}
+
+	return err;
+}
+#else
+static inline int rk_dma_heap_init_debugfs(void)
+{
+	return 0;
+}
+#endif
+
+static int rk_dma_heap_proc_show(struct seq_file *s, void *unused)
+{
+	struct rk_dma_heap *heap;
+	unsigned long total = 0;
+
+	mutex_lock(&rk_heap_list_lock);
+	list_for_each_entry(heap, &rk_heap_list, list) {
+		heap->s = s;
+		get_each_dmabuf(rk_dma_heap_dump_dmabuf, heap);
+		rk_dma_heap_dump_contig(heap);
+		total += heap->total_size;
+	}
+	seq_printf(s, "\nTotal : 0x%lx (%lu KiB)\n", total, K(total));
+	mutex_unlock(&rk_heap_list_lock);
+
+	return 0;
+}
+
+static int rk_dma_heap_info_proc_open(struct inode *inode,
+						  struct file *file)
+{
+	return single_open(file, rk_dma_heap_proc_show, NULL);
+}
+
+static const struct proc_ops rk_dma_heap_info_proc_fops = {
+	.proc_open	= rk_dma_heap_info_proc_open,
+	.proc_read	= seq_read,
+	.proc_lseek	= seq_lseek,
+	.proc_release	= single_release,
+};
+
+static int rk_dma_heap_init_proc(void)
+{
+	proc_rk_dma_heap_dir = proc_mkdir("rk_dma_heap", NULL);
+	if (!proc_rk_dma_heap_dir) {
+		pr_err("create rk_dma_heap proc dir error\n");
+		return -ENOENT;
+	}
+
+	proc_create("dma_heap_info", 0644, proc_rk_dma_heap_dir,
+		    &rk_dma_heap_info_proc_fops);
+
+	return 0;
+}
+
+static int rk_dma_heap_init(void)
+{
+	int ret;
+
+	ret = rk_dma_heap_sysfs_setup();
+	if (ret)
+		return ret;
+
+	ret = alloc_chrdev_region(&rk_dma_heap_devt, 0, NUM_HEAP_MINORS,
+				  DEVNAME);
+	if (ret)
+		goto err_chrdev;
+
+	rk_dma_heap_class = class_create(THIS_MODULE, DEVNAME);
+	if (IS_ERR(rk_dma_heap_class)) {
+		ret = PTR_ERR(rk_dma_heap_class);
+		goto err_class;
+	}
+	rk_dma_heap_class->devnode = rk_dma_heap_devnode;
+
+	rk_dma_heap_init_debugfs();
+	rk_dma_heap_init_proc();
+
+	return 0;
+
+err_class:
+	unregister_chrdev_region(rk_dma_heap_devt, NUM_HEAP_MINORS);
+err_chrdev:
+	rk_dma_heap_sysfs_teardown();
+	return ret;
+}
+subsys_initcall(rk_dma_heap_init);
diff --git a/drivers/dma-buf/rk_heaps/rk-dma-heap.h b/drivers/dma-buf/rk_heaps/rk-dma-heap.h
new file mode 100644
index 000000000..3bc750b02
--- /dev/null
+++ b/drivers/dma-buf/rk_heaps/rk-dma-heap.h
@@ -0,0 +1,178 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * DMABUF Heaps Allocation Infrastructure
+ *
+ * Copyright (C) 2011 Google, Inc.
+ * Copyright (C) 2019 Linaro Ltd.
+ * Copyright (C) 2022 Rockchip Electronics Co. Ltd.
+ * Author: Simon Xue <xxm@rock-chips.com>
+ */
+
+#ifndef _RK_DMA_HEAPS_H
+#define _RK_DMA_HEAPS_H
+
+#include <linux/cdev.h>
+#include <linux/types.h>
+#include <linux/dma-buf.h>
+#include <linux/rk-dma-heap.h>
+
+#if defined(CONFIG_DMABUF_RK_HEAPS_DEBUG_PRINT)
+#define dma_heap_print(fmt, ...)	\
+	printk(KERN_INFO pr_fmt(fmt), ##__VA_ARGS__)
+#else
+#define dma_heap_print(fmt, ...)	\
+	no_printk(KERN_INFO pr_fmt(fmt), ##__VA_ARGS__)
+#endif
+
+#define RK_DMA_HEAP_NAME_LEN 16
+
+struct rk_vmap_pfn_data {
+	unsigned long	pfn; /* first pfn of contiguous */
+	pgprot_t	prot;
+};
+
+/**
+ * struct rk_dma_heap_ops - ops to operate on a given heap
+ * @allocate:		allocate dmabuf and return struct dma_buf ptr
+ * @get_pool_size:	if heap maintains memory pools, get pool size in bytes
+ *
+ * allocate returns dmabuf on success, ERR_PTR(-errno) on error.
+ */
+struct rk_dma_heap_ops {
+	struct dma_buf *(*allocate)(struct rk_dma_heap *heap,
+			unsigned long len,
+			unsigned long fd_flags,
+			unsigned long heap_flags,
+			const char *name);
+	struct page *(*alloc_contig_pages)(struct rk_dma_heap *heap,
+					   size_t len, const char *name);
+	void (*free_contig_pages)(struct rk_dma_heap *heap,
+				  struct page *pages, size_t len,
+				  const char *name);
+	long (*get_pool_size)(struct rk_dma_heap *heap);
+};
+
+/**
+ * struct rk_dma_heap_export_info - information needed to export a new dmabuf heap
+ * @name:	used for debugging/device-node name
+ * @ops:	ops struct for this heap
+ * @priv:	heap exporter private data
+ *
+ * Information needed to export a new dmabuf heap.
+ */
+struct rk_dma_heap_export_info {
+	const char *name;
+	const struct rk_dma_heap_ops *ops;
+	void *priv;
+	bool support_cma;
+};
+
+/**
+ * struct rk_dma_heap - represents a dmabuf heap in the system
+ * @name:		used for debugging/device-node name
+ * @ops:		ops struct for this heap
+ * @heap_devt		heap device node
+ * @list		list head connecting to list of heaps
+ * @heap_cdev		heap char device
+ * @heap_dev		heap device struct
+ *
+ * Represents a heap of memory from which buffers can be made.
+ */
+struct rk_dma_heap {
+	const char *name;
+	const struct rk_dma_heap_ops *ops;
+	void *priv;
+	dev_t heap_devt;
+	struct list_head list;
+	struct list_head dmabuf_list; /* dmabuf attach to this node */
+	struct mutex dmabuf_lock;
+	struct list_head contig_list; /* contig buffer attach to this node */
+	struct mutex contig_lock;
+	struct cdev heap_cdev;
+	struct kref refcount;
+	struct device *heap_dev;
+	bool support_cma;
+	struct seq_file *s;
+	struct proc_dir_entry *procfs;
+	unsigned long total_size;
+};
+
+struct rk_dma_heap_dmabuf {
+	struct list_head node;
+	struct dma_buf *dmabuf;
+	const char *orig_alloc;
+	phys_addr_t start;
+	phys_addr_t end;
+};
+
+struct rk_dma_heap_contig_buf {
+	struct list_head node;
+	const char *orig_alloc;
+	phys_addr_t start;
+	phys_addr_t end;
+};
+
+/**
+ * rk_dma_heap_get_drvdata() - get per-heap driver data
+ * @heap: DMA-Heap to retrieve private data for
+ *
+ * Returns:
+ * The per-heap data for the heap.
+ */
+void *rk_dma_heap_get_drvdata(struct rk_dma_heap *heap);
+
+/**
+ * rk_dma_heap_get_dev() - get device struct for the heap
+ * @heap: DMA-Heap to retrieve device struct from
+ *
+ * Returns:
+ * The device struct for the heap.
+ */
+struct device *rk_dma_heap_get_dev(struct rk_dma_heap *heap);
+
+/**
+ * rk_dma_heap_get_name() - get heap name
+ * @heap: DMA-Heap to retrieve private data for
+ *
+ * Returns:
+ * The char* for the heap name.
+ */
+const char *rk_dma_heap_get_name(struct rk_dma_heap *heap);
+
+/**
+ * rk_dma_heap_add - adds a heap to dmabuf heaps
+ * @exp_info:		information needed to register this heap
+ */
+struct rk_dma_heap *rk_dma_heap_add(const struct rk_dma_heap_export_info *exp_info);
+
+/**
+ * rk_dma_heap_put - drops a reference to a dmabuf heaps, potentially freeing it
+ * @heap:		heap pointer
+ */
+void rk_dma_heap_put(struct rk_dma_heap *heap);
+
+/**
+ * rk_vmap_contig_pfn - Map contiguous pfn to vm area
+ * @pfn:	indicate the first pfn of contig
+ * @count:	count of pfns
+ * @prot:	for mapping
+ */
+void *rk_vmap_contig_pfn(unsigned long pfn, unsigned int count,
+				 pgprot_t prot);
+/**
+ * rk_dma_heap_total_inc - Increase total buffer size
+ * @heap:	dma_heap to increase
+ * @len:	length to increase
+ */
+void rk_dma_heap_total_inc(struct rk_dma_heap *heap, size_t len);
+/**
+ * rk_dma_heap_total_dec - Decrease total buffer size
+ * @heap:	dma_heap to decrease
+ * @len:	length to decrease
+ */
+void rk_dma_heap_total_dec(struct rk_dma_heap *heap, size_t len);
+/**
+ * rk_dma_heap_get_cma - get cma structure
+ */
+struct cma *rk_dma_heap_get_cma(void);
+#endif /* _DMA_HEAPS_H */
diff --git a/drivers/rknpu/Kconfig b/drivers/rknpu/Kconfig
new file mode 100644
index 000000000..c3343eece
--- /dev/null
+++ b/drivers/rknpu/Kconfig
@@ -0,0 +1,60 @@
+# SPDX-License-Identifier: GPL-2.0
+menu "RKNPU"
+	depends on ARCH_ROCKCHIP
+
+config ROCKCHIP_RKNPU
+	tristate "ROCKCHIP_RKNPU"
+	depends on DRM || DMABUF_HEAPS_ROCKCHIP_CMA_HEAP
+	help
+	  rknpu module.
+
+if ROCKCHIP_RKNPU
+
+config ROCKCHIP_RKNPU_DEBUG_FS
+	bool "RKNPU debugfs"
+	depends on DEBUG_FS
+	default y
+	help
+	  Enable debugfs to debug RKNPU usage.
+
+config ROCKCHIP_RKNPU_PROC_FS
+	bool "RKNPU procfs"
+	depends on PROC_FS
+	help
+	  Enable procfs to debug RKNPU usage.
+
+config ROCKCHIP_RKNPU_FENCE
+	bool "RKNPU fence"
+	depends on SYNC_FILE
+	help
+	  Enable fence support for RKNPU.
+
+config ROCKCHIP_RKNPU_SRAM
+	bool "RKNPU SRAM"
+	depends on NO_GKI
+	help
+	  Enable RKNPU SRAM support
+
+choice
+	prompt "RKNPU memory manager"
+	default ROCKCHIP_RKNPU_DRM_GEM
+	help
+	  Select RKNPU memory manager
+
+config ROCKCHIP_RKNPU_DRM_GEM
+	bool "RKNPU DRM GEM"
+	depends on DRM
+	help
+	  Enable RKNPU memory manager by DRM GEM.
+
+config ROCKCHIP_RKNPU_DMA_HEAP
+	bool "RKNPU DMA heap"
+	depends on DMABUF_HEAPS_ROCKCHIP_CMA_HEAP
+	help
+	  Enable RKNPU memory manager by DMA Heap.
+
+endchoice
+
+endif
+
+endmenu
diff --git a/drivers/rknpu/Makefile b/drivers/rknpu/Makefile
new file mode 100644
index 000000000..41dacc931
--- /dev/null
+++ b/drivers/rknpu/Makefile
@@ -0,0 +1,15 @@
+# SPDX-License-Identifier: GPL-2.0
+obj-$(CONFIG_ROCKCHIP_RKNPU) += rknpu.o
+
+ccflags-y += -I$(srctree)/$(src)/include
+ccflags-y += -I$(src)/include
+ccflags-y += -Werror
+
+rknpu-y += rknpu_drv.o
+rknpu-y += rknpu_reset.o
+rknpu-y += rknpu_job.o
+rknpu-y += rknpu_debugger.o
+rknpu-$(CONFIG_ROCKCHIP_RKNPU_SRAM) += rknpu_mm.o
+rknpu-$(CONFIG_ROCKCHIP_RKNPU_FENCE) += rknpu_fence.o
+rknpu-$(CONFIG_ROCKCHIP_RKNPU_DRM_GEM) += rknpu_gem.o
+rknpu-$(CONFIG_ROCKCHIP_RKNPU_DMA_HEAP) += rknpu_mem.o
diff --git a/drivers/rknpu/include/rknpu_debugger.h b/drivers/rknpu/include/rknpu_debugger.h
new file mode 100644
index 000000000..3f4420d44
--- /dev/null
+++ b/drivers/rknpu/include/rknpu_debugger.h
@@ -0,0 +1,88 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#ifndef __LINUX_RKNPU_DEBUGGER_H_
+#define __LINUX_RKNPU_DEBUGGER_H_
+
+#include <linux/seq_file.h>
+
+/*
+ * struct rknpu_debugger - rknpu debugger information
+ *
+ * This structure represents a debugger to be created by the rknpu driver
+ * or core.
+ */
+struct rknpu_debugger {
+#ifdef CONFIG_ROCKCHIP_RKNPU_DEBUG_FS
+	/* Directory of debugfs file */
+	struct dentry *debugfs_dir;
+	struct list_head debugfs_entry_list;
+	struct mutex debugfs_lock;
+#endif
+#ifdef CONFIG_ROCKCHIP_RKNPU_PROC_FS
+	/* Directory of procfs file */
+	struct proc_dir_entry *procfs_dir;
+	struct list_head procfs_entry_list;
+	struct mutex procfs_lock;
+#endif
+};
+
+/*
+ * struct rknpu_debugger_list - debugfs/procfs info list entry
+ *
+ * This structure represents a debugfs/procfs file to be created by the npu
+ * driver or core.
+ */
+struct rknpu_debugger_list {
+	/* File name */
+	const char *name;
+	/*
+	 * Show callback. &seq_file->private will be set to the &struct
+	 * rknpu_debugger_node corresponding to the instance of this info
+	 * on a given &struct rknpu_debugger.
+	 */
+	int (*show)(struct seq_file *seq, void *data);
+	/*
+	 * Write callback. &seq_file->private will be set to the &struct
+	 * rknpu_debugger_node corresponding to the instance of this info
+	 * on a given &struct rknpu_debugger.
+	 */
+	ssize_t (*write)(struct file *file, const char __user *ubuf, size_t len,
+			 loff_t *offp);
+	/* Procfs/Debugfs private data. */
+	void *data;
+};
+
+/*
+ * struct rknpu_debugger_node - Nodes for debugfs/procfs
+ *
+ * This structure represents each instance of procfs/debugfs created from the
+ * template.
+ */
+struct rknpu_debugger_node {
+	struct rknpu_debugger *debugger;
+
+	/* template for this node. */
+	const struct rknpu_debugger_list *info_ent;
+
+	/* Each Procfs/Debugfs file. */
+#ifdef CONFIG_ROCKCHIP_RKNPU_DEBUG_FS
+	struct dentry *dent;
+#endif
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_PROC_FS
+	struct proc_dir_entry *pent;
+#endif
+
+	struct list_head list;
+};
+
+struct rknpu_device;
+
+int rknpu_debugger_init(struct rknpu_device *rknpu_dev);
+int rknpu_debugger_remove(struct rknpu_device *rknpu_dev);
+
+#endif /* __LINUX_RKNPU_FENCE_H_ */
diff --git a/drivers/rknpu/include/rknpu_drv.h b/drivers/rknpu/include/rknpu_drv.h
new file mode 100644
index 000000000..f5a6b269c
--- /dev/null
+++ b/drivers/rknpu/include/rknpu_drv.h
@@ -0,0 +1,159 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#ifndef __LINUX_RKNPU_DRV_H_
+#define __LINUX_RKNPU_DRV_H_
+
+#include <linux/completion.h>
+#include <linux/device.h>
+#include <linux/kref.h>
+#include <linux/platform_device.h>
+#include <linux/spinlock.h>
+#include <linux/regulator/consumer.h>
+#include <linux/version.h>
+#include <linux/hrtimer.h>
+#include <linux/miscdevice.h>
+
+#ifndef FPGA_PLATFORM
+#if KERNEL_VERSION(5, 10, 0) <= LINUX_VERSION_CODE
+#include <soc/rockchip/rockchip_opp_select.h>
+#endif
+#endif
+
+#include "rknpu_job.h"
+#include "rknpu_fence.h"
+#include "rknpu_debugger.h"
+#include "rknpu_mm.h"
+
+#define DRIVER_NAME "rknpu"
+#define DRIVER_DESC "RKNPU driver"
+#define DRIVER_DATE "20220829"
+#define DRIVER_MAJOR 0
+#define DRIVER_MINOR 8
+#define DRIVER_PATCHLEVEL 2
+
+#define LOG_TAG "RKNPU"
+
+/* sample interval: 1000ms */
+#define RKNPU_LOAD_INTERVAL 1000000000
+
+#define LOG_INFO(fmt, args...) pr_info(LOG_TAG ": " fmt, ##args)
+#if KERNEL_VERSION(5, 5, 0) <= LINUX_VERSION_CODE
+#define LOG_WARN(fmt, args...) pr_warn(LOG_TAG ": " fmt, ##args)
+#else
+#define LOG_WARN(fmt, args...) pr_warning(LOG_TAG ": " fmt, ##args)
+#endif
+#define LOG_DEBUG(fmt, args...) pr_devel(LOG_TAG ": " fmt, ##args)
+#define LOG_ERROR(fmt, args...) pr_err(LOG_TAG ": " fmt, ##args)
+
+#define LOG_DEV_INFO(dev, fmt, args...) dev_info(dev, LOG_TAG ": " fmt, ##args)
+#define LOG_DEV_WARN(dev, fmt, args...) dev_warn(dev, LOG_TAG ": " fmt, ##args)
+#define LOG_DEV_DEBUG(dev, fmt, args...) dev_dbg(dev, LOG_TAG ": " fmt, ##args)
+#define LOG_DEV_ERROR(dev, fmt, args...) dev_err(dev, LOG_TAG ": " fmt, ##args)
+
+struct npu_reset_data {
+	const char *srst_a_name;
+	const char *srst_h_name;
+};
+
+struct rknpu_config {
+	__u32 bw_priority_addr;
+	__u32 bw_priority_length;
+	__u64 dma_mask;
+	__u32 pc_data_amount_scale;
+	__u32 pc_task_number_bits;
+	__u32 pc_task_number_mask;
+	__u32 bw_enable;
+	const struct npu_irqs_data *irqs;
+	const struct npu_reset_data *resets;
+	int num_irqs;
+	int num_resets;
+};
+
+struct rknpu_timer {
+	__u32 busy_time;
+	__u32 busy_time_record;
+};
+
+struct rknpu_subcore_data {
+	struct list_head todo_list;
+	wait_queue_head_t job_done_wq;
+	struct rknpu_job *job;
+	int64_t task_num;
+	struct rknpu_timer timer;
+};
+
+/**
+ * RKNPU device
+ *
+ * @base: IO mapped base address for device
+ * @dev: Device instance
+ * @drm_dev: DRM device instance
+ */
+struct rknpu_device {
+	void __iomem *base[RKNPU_MAX_CORES];
+	struct device *dev;
+#ifdef CONFIG_ROCKCHIP_RKNPU_DRM_GEM
+	struct drm_device *drm_dev;
+#endif
+#ifdef CONFIG_ROCKCHIP_RKNPU_DMA_HEAP
+	struct miscdevice miscdev;
+	struct rk_dma_heap *heap;
+#endif
+	atomic_t sequence;
+	spinlock_t lock;
+	spinlock_t irq_lock;
+	struct mutex power_lock;
+	struct mutex reset_lock;
+	struct rknpu_subcore_data subcore_datas[RKNPU_MAX_CORES];
+	const struct rknpu_config *config;
+	void __iomem *bw_priority_base;
+	struct rknpu_fence_context *fence_ctx;
+	bool iommu_en;
+	struct reset_control *srst_a[RKNPU_MAX_CORES];
+	struct reset_control *srst_h[RKNPU_MAX_CORES];
+	struct clk_bulk_data *clks;
+	int num_clks;
+	struct regulator *vdd;
+	struct regulator *mem;
+	struct monitor_dev_info *mdev_info;
+	struct ipa_power_model_data *model_data;
+	struct thermal_cooling_device *devfreq_cooling;
+	struct devfreq *devfreq;
+	unsigned long ondemand_freq;
+#ifndef FPGA_PLATFORM
+#if KERNEL_VERSION(5, 10, 0) <= LINUX_VERSION_CODE
+	struct rockchip_opp_info opp_info;
+#endif
+#endif
+	unsigned long current_freq;
+	unsigned long current_volt;
+	int bypass_irq_handler;
+	int bypass_soft_reset;
+	bool soft_reseting;
+	struct device *genpd_dev_npu0;
+	struct device *genpd_dev_npu1;
+	struct device *genpd_dev_npu2;
+	bool multiple_domains;
+	atomic_t power_refcount;
+	atomic_t cmdline_power_refcount;
+	struct delayed_work power_off_work;
+	struct workqueue_struct *power_off_wq;
+	struct rknpu_debugger debugger;
+	struct hrtimer timer;
+	ktime_t kt;
+	phys_addr_t sram_start;
+	phys_addr_t sram_end;
+	uint32_t sram_size;
+	void __iomem *sram_base_io;
+	struct rknpu_mm *sram_mm;
+	unsigned long power_put_delay;
+};
+
+int rknpu_power_get(struct rknpu_device *rknpu_dev);
+int rknpu_power_put(struct rknpu_device *rknpu_dev);
+
+#endif /* __LINUX_RKNPU_DRV_H_ */
diff --git a/drivers/rknpu/include/rknpu_fence.h b/drivers/rknpu/include/rknpu_fence.h
new file mode 100644
index 000000000..164f6de41
--- /dev/null
+++ b/drivers/rknpu/include/rknpu_fence.h
@@ -0,0 +1,24 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#ifndef __LINUX_RKNPU_FENCE_H_
+#define __LINUX_RKNPU_FENCE_H_
+
+#include "rknpu_job.h"
+
+struct rknpu_fence_context {
+	unsigned int context;
+	unsigned int seqno;
+	spinlock_t spinlock;
+};
+
+int rknpu_fence_context_alloc(struct rknpu_device *rknpu_dev);
+
+int rknpu_fence_alloc(struct rknpu_job *job);
+
+int rknpu_fence_get_fd(struct rknpu_job *job);
+
+#endif /* __LINUX_RKNPU_FENCE_H_ */
diff --git a/drivers/rknpu/include/rknpu_gem.h b/drivers/rknpu/include/rknpu_gem.h
new file mode 100644
index 000000000..e39190d4d
--- /dev/null
+++ b/drivers/rknpu/include/rknpu_gem.h
@@ -0,0 +1,200 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#ifndef __LINUX_RKNPU_GEM_H
+#define __LINUX_RKNPU_GEM_H
+
+#include <linux/mm_types.h>
+#include <linux/version.h>
+
+#include <drm/drm_device.h>
+#include <drm/drm_vma_manager.h>
+#include <drm/drm_gem.h>
+#include <drm/drm_mode.h>
+
+#if KERNEL_VERSION(4, 14, 0) > LINUX_VERSION_CODE
+#include <drm/drm_mem_util.h>
+#endif
+
+#include "rknpu_mm.h"
+
+#define to_rknpu_obj(x) container_of(x, struct rknpu_gem_object, base)
+
+/*
+ * rknpu drm buffer structure.
+ *
+ * @base: a gem object.
+ *	- a new handle to this gem object would be created
+ *	by drm_gem_handle_create().
+ * @flags: indicate memory type to allocated buffer and cache attribute.
+ * @size: size requested from user, in bytes and this size is aligned
+ *	in page unit.
+ * @cookie: cookie returned by dma_alloc_attrs
+ * @kv_addr: kernel virtual address to allocated memory region.
+ * @dma_addr: bus address(accessed by dma) to allocated memory region.
+ *	- this address could be physical address without IOMMU and
+ *	device address with IOMMU.
+ * @pages: Array of backing pages.
+ * @sgt: Imported sg_table.
+ *
+ * P.S. this object would be transferred to user as kms_bo.handle so
+ *	user can access the buffer through kms_bo.handle.
+ */
+struct rknpu_gem_object {
+	struct drm_gem_object base;
+	unsigned int flags;
+	unsigned long size;
+	unsigned long sram_size;
+	struct rknpu_mm_obj *sram_obj;
+	dma_addr_t iova_start;
+	unsigned long iova_size;
+	void *cookie;
+	void __iomem *kv_addr;
+	dma_addr_t dma_addr;
+	unsigned long dma_attrs;
+	unsigned long num_pages;
+	struct page **pages;
+	struct sg_table *sgt;
+	struct drm_mm_node mm_node;
+};
+
+/* create a new buffer with gem object */
+struct rknpu_gem_object *rknpu_gem_object_create(struct drm_device *dev,
+						 unsigned int flags,
+						 unsigned long size,
+						 unsigned long sram_size);
+
+/* destroy a buffer with gem object */
+void rknpu_gem_object_destroy(struct rknpu_gem_object *rknpu_obj);
+
+/* request gem object creation and buffer allocation as the size */
+int rknpu_gem_create_ioctl(struct drm_device *dev, void *data,
+			   struct drm_file *file_priv);
+
+/* get fake-offset of gem object that can be used with mmap. */
+int rknpu_gem_map_ioctl(struct drm_device *dev, void *data,
+			struct drm_file *file_priv);
+
+int rknpu_gem_destroy_ioctl(struct drm_device *dev, void *data,
+			    struct drm_file *file_priv);
+
+/*
+ * get rknpu drm object,
+ * gem object reference count would be increased.
+ */
+static inline void rknpu_gem_object_get(struct drm_gem_object *obj)
+{
+#if KERNEL_VERSION(4, 13, 0) < LINUX_VERSION_CODE
+	drm_gem_object_get(obj);
+#else
+	drm_gem_object_reference(obj);
+#endif
+}
+
+/*
+ * put rknpu drm object acquired from rknpu_gem_object_find() or rknpu_gem_object_get(),
+ * gem object reference count would be decreased.
+ */
+static inline void rknpu_gem_object_put(struct drm_gem_object *obj)
+{
+#if KERNEL_VERSION(5, 9, 0) <= LINUX_VERSION_CODE
+	drm_gem_object_put(obj);
+#elif KERNEL_VERSION(4, 13, 0) < LINUX_VERSION_CODE
+	drm_gem_object_put_unlocked(obj);
+#else
+	drm_gem_object_unreference_unlocked(obj);
+#endif
+}
+
+/*
+ * get rknpu drm object from gem handle, this function could be used for
+ * other drivers such as 2d/3d acceleration drivers.
+ * with this function call, gem object reference count would be increased.
+ */
+static inline struct rknpu_gem_object *
+rknpu_gem_object_find(struct drm_file *filp, unsigned int handle)
+{
+	struct drm_gem_object *obj;
+
+	obj = drm_gem_object_lookup(filp, handle);
+	if (!obj) {
+		// DRM_ERROR("failed to lookup gem object.\n");
+		return NULL;
+	}
+
+	rknpu_gem_object_put(obj);
+
+	return to_rknpu_obj(obj);
+}
+
+/* get buffer information to memory region allocated by gem. */
+int rknpu_gem_get_ioctl(struct drm_device *dev, void *data,
+			struct drm_file *file_priv);
+
+/* free gem object. */
+void rknpu_gem_free_object(struct drm_gem_object *obj);
+
+/* create memory region for drm framebuffer. */
+int rknpu_gem_dumb_create(struct drm_file *file_priv, struct drm_device *dev,
+			  struct drm_mode_create_dumb *args);
+
+#if KERNEL_VERSION(4, 19, 0) > LINUX_VERSION_CODE
+/* map memory region for drm framebuffer to user space. */
+int rknpu_gem_dumb_map_offset(struct drm_file *file_priv,
+			      struct drm_device *dev, uint32_t handle,
+			      uint64_t *offset);
+#endif
+
+/* page fault handler and mmap fault address(virtual) to physical memory. */
+#if KERNEL_VERSION(4, 15, 0) <= LINUX_VERSION_CODE
+vm_fault_t rknpu_gem_fault(struct vm_fault *vmf);
+#elif KERNEL_VERSION(4, 14, 0) <= LINUX_VERSION_CODE
+int rknpu_gem_fault(struct vm_fault *vmf);
+#else
+int rknpu_gem_fault(struct vm_area_struct *vma, struct vm_fault *vmf);
+#endif
+
+/* set vm_flags and we can change the vm attribute to other one at here. */
+int rknpu_gem_mmap(struct file *filp, struct vm_area_struct *vma);
+
+/* low-level interface prime helpers */
+#if KERNEL_VERSION(4, 13, 0) <= LINUX_VERSION_CODE
+struct drm_gem_object *rknpu_gem_prime_import(struct drm_device *dev,
+					      struct dma_buf *dma_buf);
+#endif
+struct sg_table *rknpu_gem_prime_get_sg_table(struct drm_gem_object *obj);
+struct drm_gem_object *
+rknpu_gem_prime_import_sg_table(struct drm_device *dev,
+				struct dma_buf_attachment *attach,
+				struct sg_table *sgt);
+int rknpu_gem_prime_vmap(struct drm_gem_object *obj, struct iosys_map *map);
+void rknpu_gem_prime_vunmap(struct drm_gem_object *objb, struct iosys_map *map);
+int rknpu_gem_prime_mmap(struct drm_gem_object *obj,
+			 struct vm_area_struct *vma);
+
+int rknpu_gem_sync_ioctl(struct drm_device *dev, void *data,
+			 struct drm_file *file_priv);
+
+static inline void *rknpu_gem_alloc_page(size_t nr_pages)
+{
+#if KERNEL_VERSION(4, 13, 0) <= LINUX_VERSION_CODE
+	return kvmalloc_array(nr_pages, sizeof(struct page *),
+			      GFP_KERNEL | __GFP_ZERO);
+#else
+	return drm_calloc_large(nr_pages, sizeof(struct page *));
+#endif
+}
+
+static inline void rknpu_gem_free_page(void *pages)
+{
+#if KERNEL_VERSION(4, 13, 0) <= LINUX_VERSION_CODE
+	kvfree(pages);
+#else
+	drm_free_large(pages);
+#endif
+}
+
+#endif
diff --git a/drivers/rknpu/include/rknpu_ioctl.h b/drivers/rknpu/include/rknpu_ioctl.h
new file mode 100644
index 000000000..49d4442e6
--- /dev/null
+++ b/drivers/rknpu/include/rknpu_ioctl.h
@@ -0,0 +1,323 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#ifndef __LINUX_RKNPU_IOCTL_H
+#define __LINUX_RKNPU_IOCTL_H
+
+#include <linux/ioctl.h>
+#include <linux/types.h>
+
+#if !defined(__KERNEL__)
+#define __user
+#endif
+
+#ifndef __packed
+#define __packed __attribute__((packed))
+#endif
+
+#define RKNPU_OFFSET_VERSION 0x0
+#define RKNPU_OFFSET_VERSION_NUM 0x4
+#define RKNPU_OFFSET_PC_OP_EN 0x8
+#define RKNPU_OFFSET_PC_DATA_ADDR 0x10
+#define RKNPU_OFFSET_PC_DATA_AMOUNT 0x14
+#define RKNPU_OFFSET_PC_TASK_CONTROL 0x30
+#define RKNPU_OFFSET_PC_DMA_BASE_ADDR 0x34
+#define RKNPU_OFFSET_PC_TASK_STATUS 0x3c
+
+#define RKNPU_OFFSET_INT_MASK 0x20
+#define RKNPU_OFFSET_INT_CLEAR 0x24
+#define RKNPU_OFFSET_INT_STATUS 0x28
+#define RKNPU_OFFSET_INT_RAW_STATUS 0x2c
+
+#define RKNPU_OFFSET_CLR_ALL_RW_AMOUNT 0x8010
+#define RKNPU_OFFSET_DT_WR_AMOUNT 0x8034
+#define RKNPU_OFFSET_DT_RD_AMOUNT 0x8038
+#define RKNPU_OFFSET_WT_RD_AMOUNT 0x803c
+
+#define RKNPU_OFFSET_ENABLE_MASK 0xf008
+
+#define RKNPU_INT_CLEAR 0x1ffff
+
+#define RKNPU_PC_DATA_EXTRA_AMOUNT 4
+
+#define RKNPU_STR_HELPER(x) #x
+
+#define RKNPU_GET_DRV_VERSION_STRING(MAJOR, MINOR, PATCHLEVEL)                 \
+	RKNPU_STR_HELPER(MAJOR)                                                \
+	"." RKNPU_STR_HELPER(MINOR) "." RKNPU_STR_HELPER(PATCHLEVEL)
+#define RKNPU_GET_DRV_VERSION_CODE(MAJOR, MINOR, PATCHLEVEL)                   \
+	(MAJOR * 10000 + MINOR * 100 + PATCHLEVEL)
+#define RKNPU_GET_DRV_VERSION_MAJOR(CODE) (CODE / 10000)
+#define RKNPU_GET_DRV_VERSION_MINOR(CODE) ((CODE % 10000) / 100)
+#define RKNPU_GET_DRV_VERSION_PATCHLEVEL(CODE) (CODE % 100)
+
+/* memory type definitions. */
+enum e_rknpu_mem_type {
+	/* physically continuous memory and used as default. */
+	RKNPU_MEM_CONTIGUOUS = 0 << 0,
+	/* physically non-continuous memory. */
+	RKNPU_MEM_NON_CONTIGUOUS = 1 << 0,
+	/* non-cacheable mapping and used as default. */
+	RKNPU_MEM_NON_CACHEABLE = 0 << 1,
+	/* cacheable mapping. */
+	RKNPU_MEM_CACHEABLE = 1 << 1,
+	/* write-combine mapping. */
+	RKNPU_MEM_WRITE_COMBINE = 1 << 2,
+	/* dma attr kernel mapping */
+	RKNPU_MEM_KERNEL_MAPPING = 1 << 3,
+	/* iommu mapping */
+	RKNPU_MEM_IOMMU = 1 << 4,
+	/* zero mapping */
+	RKNPU_MEM_ZEROING = 1 << 5,
+	/* allocate secure buffer */
+	RKNPU_MEM_SECURE = 1 << 6,
+	/* allocate from non-dma32 zone */
+	RKNPU_MEM_NON_DMA32 = 1 << 7,
+	/* request SRAM */
+	RKNPU_MEM_TRY_ALLOC_SRAM = 1 << 8,
+	RKNPU_MEM_MASK = RKNPU_MEM_NON_CONTIGUOUS | RKNPU_MEM_CACHEABLE |
+			 RKNPU_MEM_WRITE_COMBINE | RKNPU_MEM_KERNEL_MAPPING |
+			 RKNPU_MEM_IOMMU | RKNPU_MEM_ZEROING |
+			 RKNPU_MEM_SECURE | RKNPU_MEM_NON_DMA32 |
+			 RKNPU_MEM_TRY_ALLOC_SRAM
+};
+
+/* sync mode definitions. */
+enum e_rknpu_mem_sync_mode {
+	RKNPU_MEM_SYNC_TO_DEVICE = 1 << 0,
+	RKNPU_MEM_SYNC_FROM_DEVICE = 1 << 1,
+	RKNPU_MEM_SYNC_MASK =
+		RKNPU_MEM_SYNC_TO_DEVICE | RKNPU_MEM_SYNC_FROM_DEVICE
+};
+
+/* job mode definitions. */
+enum e_rknpu_job_mode {
+	RKNPU_JOB_SLAVE = 0 << 0,
+	RKNPU_JOB_PC = 1 << 0,
+	RKNPU_JOB_BLOCK = 0 << 1,
+	RKNPU_JOB_NONBLOCK = 1 << 1,
+	RKNPU_JOB_PINGPONG = 1 << 2,
+	RKNPU_JOB_FENCE_IN = 1 << 3,
+	RKNPU_JOB_FENCE_OUT = 1 << 4,
+	RKNPU_JOB_MASK = RKNPU_JOB_PC | RKNPU_JOB_NONBLOCK |
+			 RKNPU_JOB_PINGPONG | RKNPU_JOB_FENCE_IN |
+			 RKNPU_JOB_FENCE_OUT
+};
+
+/* action definitions */
+enum e_rknpu_action {
+	RKNPU_GET_HW_VERSION = 0,
+	RKNPU_GET_DRV_VERSION = 1,
+	RKNPU_GET_FREQ = 2,
+	RKNPU_SET_FREQ = 3,
+	RKNPU_GET_VOLT = 4,
+	RKNPU_SET_VOLT = 5,
+	RKNPU_ACT_RESET = 6,
+	RKNPU_GET_BW_PRIORITY = 7,
+	RKNPU_SET_BW_PRIORITY = 8,
+	RKNPU_GET_BW_EXPECT = 9,
+	RKNPU_SET_BW_EXPECT = 10,
+	RKNPU_GET_BW_TW = 11,
+	RKNPU_SET_BW_TW = 12,
+	RKNPU_ACT_CLR_TOTAL_RW_AMOUNT = 13,
+	RKNPU_GET_DT_WR_AMOUNT = 14,
+	RKNPU_GET_DT_RD_AMOUNT = 15,
+	RKNPU_GET_WT_RD_AMOUNT = 16,
+	RKNPU_GET_TOTAL_RW_AMOUNT = 17,
+	RKNPU_GET_IOMMU_EN = 18,
+	RKNPU_SET_PROC_NICE = 19,
+	RKNPU_POWER_ON = 20,
+	RKNPU_POWER_OFF = 21,
+	RKNPU_GET_TOTAL_SRAM_SIZE = 22,
+	RKNPU_GET_FREE_SRAM_SIZE = 23,
+};
+
+/**
+ * User-desired buffer creation information structure.
+ *
+ * @handle: The handle of the created GEM object.
+ * @flags: user request for setting memory type or cache attributes.
+ * @size: user-desired memory allocation size.
+ *	- this size value would be page-aligned internally.
+ * @obj_addr: address of RKNPU memory object.
+ * @dma_addr: dma address that access by rknpu.
+ * @sram_size: user-desired sram memory allocation size.
+ *  - this size value would be page-aligned internally.
+ */
+struct rknpu_mem_create {
+	__u32 handle;
+	__u32 flags;
+	__u64 size;
+	__u64 obj_addr;
+	__u64 dma_addr;
+	__u64 sram_size;
+};
+
+/**
+ * A structure for getting a fake-offset that can be used with mmap.
+ *
+ * @handle: handle of gem object.
+ * @reserved: just padding to be 64-bit aligned.
+ * @offset: a fake-offset of gem object.
+ */
+struct rknpu_mem_map {
+	__u32 handle;
+	__u32 reserved;
+	__u64 offset;
+};
+
+/**
+ * For destroying DMA buffer
+ *
+ * @handle:	handle of the buffer.
+ * @reserved: reserved for padding.
+ * @obj_addr: rknpu_mem_object addr.
+ */
+struct rknpu_mem_destroy {
+	__u32 handle;
+	__u32 reserved;
+	__u64 obj_addr;
+};
+
+/**
+ * For synchronizing DMA buffer
+ *
+ * @flags: user request for setting memory type or cache attributes.
+ * @reserved: reserved for padding.
+ * @obj_addr: address of RKNPU memory object.
+ * @offset: offset in bytes from start address of buffer.
+ * @size: size of memory region.
+ *
+ */
+struct rknpu_mem_sync {
+	__u32 flags;
+	__u32 reserved;
+	__u64 obj_addr;
+	__u64 offset;
+	__u64 size;
+};
+
+/**
+ * struct rknpu_task structure for task information
+ *
+ * @flags: flags for task
+ * @op_idx: operator index
+ * @enable_mask: enable mask
+ * @int_mask: interrupt mask
+ * @int_clear: interrupt clear
+ * @int_status: interrupt status
+ * @regcfg_amount: register config number
+ * @regcfg_offset: offset for register config
+ * @regcmd_addr: address for register command
+ *
+ */
+struct rknpu_task {
+	__u32 flags;
+	__u32 op_idx;
+	__u32 enable_mask;
+	__u32 int_mask;
+	__u32 int_clear;
+	__u32 int_status;
+	__u32 regcfg_amount;
+	__u32 regcfg_offset;
+	__u64 regcmd_addr;
+} __packed;
+
+/**
+ * struct rknpu_subcore_task structure for subcore task index
+ *
+ * @task_start: task start index
+ * @task_number: task number
+ *
+ */
+struct rknpu_subcore_task {
+	__u32 task_start;
+	__u32 task_number;
+};
+
+/**
+ * struct rknpu_submit structure for job submit
+ *
+ * @flags: flags for job submit
+ * @timeout: submit timeout
+ * @task_start: task start index
+ * @task_number: task number
+ * @task_counter: task counter
+ * @priority: submit priority
+ * @task_obj_addr: address of task object
+ * @regcfg_obj_addr: address of register config object
+ * @task_base_addr: task base address
+ * @user_data: (optional) user data
+ * @core_mask: core mask of rknpu
+ * @fence_fd: dma fence fd
+ * @subcore_task: subcore task
+ *
+ */
+struct rknpu_submit {
+	__u32 flags;
+	__u32 timeout;
+	__u32 task_start;
+	__u32 task_number;
+	__u32 task_counter;
+	__s32 priority;
+	__u64 task_obj_addr;
+	__u64 regcfg_obj_addr;
+	__u64 task_base_addr;
+	__u64 user_data;
+	__u32 core_mask;
+	__s32 fence_fd;
+	struct rknpu_subcore_task subcore_task[5];
+};
+
+/**
+ * struct rknpu_task structure for action (GET, SET or ACT)
+ *
+ * @flags: flags for action
+ * @value: GET or SET value
+ *
+ */
+struct rknpu_action {
+	__u32 flags;
+	__u32 value;
+};
+
+#define RKNPU_ACTION 0x00
+#define RKNPU_SUBMIT 0x01
+#define RKNPU_MEM_CREATE 0x02
+#define RKNPU_MEM_MAP 0x03
+#define RKNPU_MEM_DESTROY 0x04
+#define RKNPU_MEM_SYNC 0x05
+
+#define RKNPU_IOC_MAGIC 'r'
+#define RKNPU_IOW(nr, type) _IOW(RKNPU_IOC_MAGIC, nr, type)
+#define RKNPU_IOR(nr, type) _IOR(RKNPU_IOC_MAGIC, nr, type)
+#define RKNPU_IOWR(nr, type) _IOWR(RKNPU_IOC_MAGIC, nr, type)
+
+#include <drm/drm.h>
+
+#define DRM_IOCTL_RKNPU_ACTION                                                 \
+	DRM_IOWR(DRM_COMMAND_BASE + RKNPU_ACTION, struct rknpu_action)
+#define DRM_IOCTL_RKNPU_SUBMIT                                                 \
+	DRM_IOWR(DRM_COMMAND_BASE + RKNPU_SUBMIT, struct rknpu_submit)
+#define DRM_IOCTL_RKNPU_MEM_CREATE                                             \
+	DRM_IOWR(DRM_COMMAND_BASE + RKNPU_MEM_CREATE, struct rknpu_mem_create)
+#define DRM_IOCTL_RKNPU_MEM_MAP                                                \
+	DRM_IOWR(DRM_COMMAND_BASE + RKNPU_MEM_MAP, struct rknpu_mem_map)
+#define DRM_IOCTL_RKNPU_MEM_DESTROY                                            \
+	DRM_IOWR(DRM_COMMAND_BASE + RKNPU_MEM_DESTROY, struct rknpu_mem_destroy)
+#define DRM_IOCTL_RKNPU_MEM_SYNC                                               \
+	DRM_IOWR(DRM_COMMAND_BASE + RKNPU_MEM_SYNC, struct rknpu_mem_sync)
+
+#define IOCTL_RKNPU_ACTION RKNPU_IOWR(RKNPU_ACTION, struct rknpu_action)
+#define IOCTL_RKNPU_SUBMIT RKNPU_IOWR(RKNPU_SUBMIT, struct rknpu_submit)
+#define IOCTL_RKNPU_MEM_CREATE                                                 \
+	RKNPU_IOWR(RKNPU_MEM_CREATE, struct rknpu_mem_create)
+#define IOCTL_RKNPU_MEM_MAP RKNPU_IOWR(RKNPU_MEM_MAP, struct rknpu_mem_map)
+#define IOCTL_RKNPU_MEM_DESTROY                                                \
+	RKNPU_IOWR(RKNPU_MEM_DESTROY, struct rknpu_mem_destroy)
+#define IOCTL_RKNPU_MEM_SYNC RKNPU_IOWR(RKNPU_MEM_SYNC, struct rknpu_mem_sync)
+
+#endif
diff --git a/drivers/rknpu/include/rknpu_job.h b/drivers/rknpu/include/rknpu_job.h
new file mode 100644
index 000000000..6ef52d439
--- /dev/null
+++ b/drivers/rknpu/include/rknpu_job.h
@@ -0,0 +1,78 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#ifndef __LINUX_RKNPU_JOB_H_
+#define __LINUX_RKNPU_JOB_H_
+
+#include <linux/spinlock.h>
+#include <linux/dma-fence.h>
+#include <linux/irq.h>
+
+#include <drm/drm_device.h>
+
+#include "rknpu_ioctl.h"
+
+#define RKNPU_MAX_CORES 3
+
+#define RKNPU_JOB_DONE (1 << 0)
+#define RKNPU_JOB_ASYNC (1 << 1)
+#define RKNPU_JOB_DETACHED (1 << 2)
+
+#define RKNPU_CORE_AUTO_MASK 0x00
+#define RKNPU_CORE0_MASK 0x01
+#define RKNPU_CORE1_MASK 0x02
+#define RKNPU_CORE2_MASK 0x04
+
+struct rknpu_job {
+	struct rknpu_device *rknpu_dev;
+	struct list_head head[RKNPU_MAX_CORES];
+	struct work_struct cleanup_work;
+	bool in_queue[RKNPU_MAX_CORES];
+	bool irq_entry[RKNPU_MAX_CORES];
+	unsigned int flags;
+	int ret;
+	struct rknpu_submit *args;
+	bool args_owner;
+	struct rknpu_task *first_task;
+	struct rknpu_task *last_task;
+	uint32_t int_mask[RKNPU_MAX_CORES];
+	uint32_t int_status[RKNPU_MAX_CORES];
+	struct dma_fence *fence;
+	ktime_t timestamp;
+	uint32_t use_core_num;
+	uint32_t run_count;
+	uint32_t interrupt_count;
+	ktime_t hw_recoder_time;
+};
+
+irqreturn_t rknpu_core0_irq_handler(int irq, void *data);
+irqreturn_t rknpu_core1_irq_handler(int irq, void *data);
+irqreturn_t rknpu_core2_irq_handler(int irq, void *data);
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_DRM_GEM
+int rknpu_submit_ioctl(struct drm_device *dev, void *data,
+		       struct drm_file *file_priv);
+#endif
+#ifdef CONFIG_ROCKCHIP_RKNPU_DMA_HEAP
+int rknpu_submit_ioctl(struct rknpu_device *rknpu_dev, unsigned long data);
+#endif
+
+int rknpu_get_hw_version(struct rknpu_device *rknpu_dev, uint32_t *version);
+
+int rknpu_get_bw_priority(struct rknpu_device *rknpu_dev, uint32_t *priority,
+			  uint32_t *expect, uint32_t *tw);
+
+int rknpu_set_bw_priority(struct rknpu_device *rknpu_dev, uint32_t priority,
+			  uint32_t expect, uint32_t tw);
+
+int rknpu_clear_rw_amount(struct rknpu_device *rknpu_dev);
+
+int rknpu_get_rw_amount(struct rknpu_device *rknpu_dev, uint32_t *dt_wr,
+			uint32_t *dt_rd, uint32_t *wd_rd);
+
+int rknpu_get_total_rw_amount(struct rknpu_device *rknpu_dev, uint32_t *amount);
+
+#endif /* __LINUX_RKNPU_JOB_H_ */
diff --git a/drivers/rknpu/include/rknpu_mem.h b/drivers/rknpu/include/rknpu_mem.h
new file mode 100644
index 000000000..925535c85
--- /dev/null
+++ b/drivers/rknpu/include/rknpu_mem.h
@@ -0,0 +1,43 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#ifndef __LINUX_RKNPU_MEM_H
+#define __LINUX_RKNPU_MEM_H
+
+#include <linux/mm_types.h>
+#include <linux/version.h>
+
+/*
+ * rknpu DMA buffer structure.
+ *
+ * @flags: indicate memory type to allocated buffer and cache attribute.
+ * @size: size requested from user, in bytes and this size is aligned
+ *	in page unit.
+ * @kv_addr: kernel virtual address to allocated memory region.
+ * @dma_addr: bus address(accessed by dma) to allocated memory region.
+ *	- this address could be physical address without IOMMU and
+ *	device address with IOMMU.
+ * @pages: Array of backing pages.
+ * @sgt: Imported sg_table.
+ * @dmabuf: buffer for this attachment.
+ * @owner: Is this memory internally allocated.
+ */
+struct rknpu_mem_object {
+	unsigned long flags;
+	unsigned long size;
+	void __iomem *kv_addr;
+	dma_addr_t dma_addr;
+	struct page **pages;
+	struct sg_table *sgt;
+	struct dma_buf *dmabuf;
+	unsigned int owner;
+};
+
+int rknpu_mem_create_ioctl(struct rknpu_device *rknpu_dev, unsigned long data);
+int rknpu_mem_destroy_ioctl(struct rknpu_device *rknpu_dev, unsigned long data);
+int rknpu_mem_sync_ioctl(struct rknpu_device *rknpu_dev, unsigned long data);
+
+#endif
diff --git a/drivers/rknpu/include/rknpu_mm.h b/drivers/rknpu/include/rknpu_mm.h
new file mode 100644
index 000000000..b764892d1
--- /dev/null
+++ b/drivers/rknpu/include/rknpu_mm.h
@@ -0,0 +1,61 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#ifndef __LINUX_RKNPU_MM_H
+#define __LINUX_RKNPU_MM_H
+
+#include <linux/mutex.h>
+#include <linux/seq_file.h>
+#include <linux/iommu.h>
+#include <linux/dma-iommu.h>
+#include <linux/iova.h>
+
+#include "rknpu_drv.h"
+
+struct rknpu_mm {
+	void *bitmap;
+	struct mutex lock;
+	unsigned int chunk_size;
+	unsigned int total_chunks;
+	unsigned int free_chunks;
+};
+
+struct rknpu_mm_obj {
+	uint32_t range_start;
+	uint32_t range_end;
+};
+
+int rknpu_mm_create(unsigned int mem_size, unsigned int chunk_size,
+		    struct rknpu_mm **mm);
+
+void rknpu_mm_destroy(struct rknpu_mm *mm);
+
+int rknpu_mm_alloc(struct rknpu_mm *mm, unsigned int size,
+		   struct rknpu_mm_obj **mm_obj);
+
+int rknpu_mm_free(struct rknpu_mm *mm, struct rknpu_mm_obj *mm_obj);
+
+int rknpu_mm_dump(struct seq_file *m, void *data);
+
+enum iommu_dma_cookie_type {
+	IOMMU_DMA_IOVA_COOKIE,
+	IOMMU_DMA_MSI_COOKIE,
+};
+
+struct rknpu_iommu_dma_cookie {
+	enum iommu_dma_cookie_type type;
+
+	/* Full allocator for IOMMU_DMA_IOVA_COOKIE */
+	struct iova_domain iovad;
+};
+
+dma_addr_t rknpu_iommu_dma_alloc_iova(struct iommu_domain *domain, size_t size,
+				      u64 dma_limit, struct device *dev);
+
+void rknpu_iommu_dma_free_iova(struct rknpu_iommu_dma_cookie *cookie,
+			       dma_addr_t iova, size_t size);
+
+#endif
diff --git a/drivers/rknpu/include/rknpu_reset.h b/drivers/rknpu/include/rknpu_reset.h
new file mode 100644
index 000000000..b80e29b32
--- /dev/null
+++ b/drivers/rknpu/include/rknpu_reset.h
@@ -0,0 +1,18 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#ifndef __LINUX_RKNPU_RESET_H
+#define __LINUX_RKNPU_RESET_H
+
+#include <linux/reset.h>
+
+#include "rknpu_drv.h"
+
+int rknpu_reset_get(struct rknpu_device *rknpu_dev);
+
+int rknpu_soft_reset(struct rknpu_device *rknpu_dev);
+
+#endif
diff --git a/drivers/rknpu/rknpu_debugger.c b/drivers/rknpu/rknpu_debugger.c
new file mode 100644
index 000000000..195e96a33
--- /dev/null
+++ b/drivers/rknpu/rknpu_debugger.c
@@ -0,0 +1,591 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#include <linux/slab.h>
+#include <linux/delay.h>
+#include <linux/syscalls.h>
+#include <linux/debugfs.h>
+#include <linux/proc_fs.h>
+#include <linux/devfreq.h>
+#include <linux/clk.h>
+#include <asm/div64.h>
+
+#ifndef FPGA_PLATFORM
+#include <../drivers/devfreq/governor.h>
+#endif
+
+#include "rknpu_drv.h"
+#include "rknpu_mm.h"
+#include "rknpu_reset.h"
+#include "rknpu_debugger.h"
+
+#define RKNPU_DEBUGGER_ROOT_NAME "rknpu"
+
+#if defined(CONFIG_ROCKCHIP_RKNPU_DEBUG_FS) ||                                 \
+	defined(CONFIG_ROCKCHIP_RKNPU_PROC_FS)
+static int rknpu_version_show(struct seq_file *m, void *data)
+{
+	seq_printf(m, "%s: v%d.%d.%d\n", DRIVER_DESC, DRIVER_MAJOR,
+		   DRIVER_MINOR, DRIVER_PATCHLEVEL);
+
+	return 0;
+}
+
+static int rknpu_load_show(struct seq_file *m, void *data)
+{
+	struct rknpu_debugger_node *node = m->private;
+	struct rknpu_debugger *debugger = node->debugger;
+	struct rknpu_device *rknpu_dev =
+		container_of(debugger, struct rknpu_device, debugger);
+	struct rknpu_subcore_data *subcore_data = NULL;
+	unsigned long flags;
+	int i;
+	int load;
+	uint64_t busy_time_total, div_value;
+
+	seq_puts(m, "NPU load: ");
+	for (i = 0; i < rknpu_dev->config->num_irqs; i++) {
+		subcore_data = &rknpu_dev->subcore_datas[i];
+
+		if (rknpu_dev->config->num_irqs > 1)
+			seq_printf(m, " Core%d: ", i);
+
+		spin_lock_irqsave(&rknpu_dev->irq_lock, flags);
+
+		busy_time_total = subcore_data->timer.busy_time_record;
+
+		spin_unlock_irqrestore(&rknpu_dev->irq_lock, flags);
+
+		div_value = (RKNPU_LOAD_INTERVAL / 100000);
+		do_div(busy_time_total, div_value);
+		load = busy_time_total;
+
+		if (rknpu_dev->config->num_irqs > 1)
+			seq_printf(m, "%2.d%%,", load);
+		else
+			seq_printf(m, "%2.d%%", load);
+	}
+	seq_puts(m, "\n");
+
+	return 0;
+}
+
+static int rknpu_power_show(struct seq_file *m, void *data)
+{
+	struct rknpu_debugger_node *node = m->private;
+	struct rknpu_debugger *debugger = node->debugger;
+	struct rknpu_device *rknpu_dev =
+		container_of(debugger, struct rknpu_device, debugger);
+
+	if (atomic_read(&rknpu_dev->power_refcount) > 0)
+		seq_puts(m, "on\n");
+	else
+		seq_puts(m, "off\n");
+
+	return 0;
+}
+
+static ssize_t rknpu_power_set(struct file *file, const char __user *ubuf,
+			       size_t len, loff_t *offp)
+{
+	struct seq_file *priv = file->private_data;
+	struct rknpu_debugger_node *node = priv->private;
+	struct rknpu_debugger *debugger = node->debugger;
+	struct rknpu_device *rknpu_dev =
+		container_of(debugger, struct rknpu_device, debugger);
+	char buf[8];
+
+	if (len > sizeof(buf) - 1)
+		return -EINVAL;
+	if (copy_from_user(buf, ubuf, len))
+		return -EFAULT;
+	buf[len - 1] = '\0';
+
+	if (strcmp(buf, "on") == 0) {
+		atomic_inc(&rknpu_dev->cmdline_power_refcount);
+		rknpu_power_get(rknpu_dev);
+		LOG_INFO("rknpu power is on!");
+	} else if (strcmp(buf, "off") == 0) {
+		if (atomic_read(&rknpu_dev->power_refcount) > 0 &&
+		    atomic_dec_if_positive(
+			    &rknpu_dev->cmdline_power_refcount) >= 0) {
+			atomic_sub(
+				atomic_read(&rknpu_dev->cmdline_power_refcount),
+				&rknpu_dev->power_refcount);
+			atomic_set(&rknpu_dev->cmdline_power_refcount, 0);
+			rknpu_power_put(rknpu_dev);
+		}
+		if (atomic_read(&rknpu_dev->power_refcount) <= 0)
+			LOG_INFO("rknpu power is off!");
+	} else {
+		LOG_ERROR("rknpu power node params is invalid!");
+	}
+
+	return len;
+}
+
+static int rknpu_power_put_delay_show(struct seq_file *m, void *data)
+{
+	struct rknpu_debugger_node *node = m->private;
+	struct rknpu_debugger *debugger = node->debugger;
+	struct rknpu_device *rknpu_dev =
+		container_of(debugger, struct rknpu_device, debugger);
+
+	seq_printf(m, "%lu\n", rknpu_dev->power_put_delay);
+
+	return 0;
+}
+
+static ssize_t rknpu_power_put_delay_set(struct file *file,
+					 const char __user *ubuf, size_t len,
+					 loff_t *offp)
+{
+	struct seq_file *priv = file->private_data;
+	struct rknpu_debugger_node *node = priv->private;
+	struct rknpu_debugger *debugger = node->debugger;
+	struct rknpu_device *rknpu_dev =
+		container_of(debugger, struct rknpu_device, debugger);
+	char buf[16];
+	unsigned long power_put_delay = 0;
+	int ret = 0;
+
+	if (len > sizeof(buf) - 1)
+		return -EINVAL;
+	if (copy_from_user(buf, ubuf, len))
+		return -EFAULT;
+	buf[len - 1] = '\0';
+
+	ret = kstrtoul(buf, 10, &power_put_delay);
+	if (ret) {
+		LOG_ERROR("failed to parse power put delay string: %s\n", buf);
+		return -EFAULT;
+	}
+
+	rknpu_dev->power_put_delay = power_put_delay;
+
+	LOG_INFO("set rknpu power put delay time %lums\n",
+		 rknpu_dev->power_put_delay);
+
+	return len;
+}
+
+static int rknpu_freq_show(struct seq_file *m, void *data)
+{
+	struct rknpu_debugger_node *node = m->private;
+	struct rknpu_debugger *debugger = node->debugger;
+	struct rknpu_device *rknpu_dev =
+		container_of(debugger, struct rknpu_device, debugger);
+	unsigned long current_freq = 0;
+
+	rknpu_power_get(rknpu_dev);
+
+	current_freq = clk_get_rate(rknpu_dev->clks[0].clk);
+
+	rknpu_power_put(rknpu_dev);
+
+	seq_printf(m, "%lu\n", current_freq);
+
+	return 0;
+}
+
+static ssize_t rknpu_freq_set(struct file *file, const char __user *ubuf,
+			      size_t len, loff_t *offp)
+{
+	struct seq_file *priv = file->private_data;
+	struct rknpu_debugger_node *node = priv->private;
+	struct rknpu_debugger *debugger = node->debugger;
+	struct rknpu_device *rknpu_dev =
+		container_of(debugger, struct rknpu_device, debugger);
+	unsigned long current_freq = 0;
+	char buf[16];
+	unsigned long freq = 0;
+	int ret = 0;
+
+	if (len > sizeof(buf) - 1)
+		return -EINVAL;
+	if (copy_from_user(buf, ubuf, len))
+		return -EFAULT;
+	buf[len - 1] = '\0';
+
+	ret = kstrtoul(buf, 10, &freq);
+	if (ret) {
+		LOG_ERROR("failed to parse freq string: %s\n", buf);
+		return -EFAULT;
+	}
+
+	if (!rknpu_dev->devfreq)
+		return -EFAULT;
+
+	rknpu_power_get(rknpu_dev);
+
+	current_freq = clk_get_rate(rknpu_dev->clks[0].clk);
+	if (freq != current_freq) {
+		rknpu_dev->ondemand_freq = freq;
+		mutex_lock(&rknpu_dev->devfreq->lock);
+		update_devfreq(rknpu_dev->devfreq);
+		mutex_unlock(&rknpu_dev->devfreq->lock);
+	}
+
+	rknpu_power_put(rknpu_dev);
+
+	return len;
+}
+
+static int rknpu_volt_show(struct seq_file *m, void *data)
+{
+	struct rknpu_debugger_node *node = m->private;
+	struct rknpu_debugger *debugger = node->debugger;
+	struct rknpu_device *rknpu_dev =
+		container_of(debugger, struct rknpu_device, debugger);
+	unsigned long current_volt = 0;
+
+	current_volt = regulator_get_voltage(rknpu_dev->vdd);
+
+	seq_printf(m, "%lu\n", current_volt);
+
+	return 0;
+}
+
+static int rknpu_reset_show(struct seq_file *m, void *data)
+{
+	struct rknpu_debugger_node *node = m->private;
+	struct rknpu_debugger *debugger = node->debugger;
+	struct rknpu_device *rknpu_dev =
+		container_of(debugger, struct rknpu_device, debugger);
+
+	if (!rknpu_dev->bypass_soft_reset)
+		seq_puts(m, "on\n");
+	else
+		seq_puts(m, "off\n");
+
+	return 0;
+}
+
+static ssize_t rknpu_reset_set(struct file *file, const char __user *ubuf,
+			       size_t len, loff_t *offp)
+{
+	struct seq_file *priv = file->private_data;
+	struct rknpu_debugger_node *node = priv->private;
+	struct rknpu_debugger *debugger = node->debugger;
+	struct rknpu_device *rknpu_dev =
+		container_of(debugger, struct rknpu_device, debugger);
+	char buf[8];
+
+	if (len > sizeof(buf) - 1)
+		return -EINVAL;
+	if (copy_from_user(buf, ubuf, len))
+		return -EFAULT;
+	buf[len - 1] = '\0';
+
+	if (strcmp(buf, "1") == 0 &&
+	    atomic_read(&rknpu_dev->power_refcount) > 0)
+		rknpu_soft_reset(rknpu_dev);
+	else if (strcmp(buf, "on") == 0)
+		rknpu_dev->bypass_soft_reset = 0;
+	else if (strcmp(buf, "off") == 0)
+		rknpu_dev->bypass_soft_reset = 1;
+
+	return len;
+}
+
+static struct rknpu_debugger_list rknpu_debugger_root_list[] = {
+	{ "version", rknpu_version_show, NULL, NULL },
+	{ "load", rknpu_load_show, NULL, NULL },
+	{ "power", rknpu_power_show, rknpu_power_set, NULL },
+	{ "freq", rknpu_freq_show, rknpu_freq_set, NULL },
+	{ "volt", rknpu_volt_show, NULL, NULL },
+	{ "delayms", rknpu_power_put_delay_show, rknpu_power_put_delay_set,
+	  NULL },
+	{ "reset", rknpu_reset_show, rknpu_reset_set, NULL },
+#ifdef CONFIG_ROCKCHIP_RKNPU_SRAM
+	{ "mm", rknpu_mm_dump, NULL, NULL },
+#endif
+};
+
+static ssize_t rknpu_debugger_write(struct file *file, const char __user *ubuf,
+				    size_t len, loff_t *offp)
+{
+	struct seq_file *priv = file->private_data;
+	struct rknpu_debugger_node *node = priv->private;
+
+	if (node->info_ent->write)
+		return node->info_ent->write(file, ubuf, len, offp);
+	else
+		return len;
+}
+
+static int rknpu_debugfs_open(struct inode *inode, struct file *file)
+{
+	struct rknpu_debugger_node *node = inode->i_private;
+
+	return single_open(file, node->info_ent->show, node);
+}
+
+static const struct file_operations rknpu_debugfs_fops = {
+	.owner = THIS_MODULE,
+	.open = rknpu_debugfs_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+	.write = rknpu_debugger_write,
+};
+#endif /* #if defined(CONFIG_ROCKCHIP_RKNPU_DEBUG_FS) || defined(CONFIG_ROCKCHIP_RKNPU_PROC_FS) */
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_DEBUG_FS
+static int rknpu_debugfs_remove_files(struct rknpu_debugger *debugger)
+{
+	struct rknpu_debugger_node *pos, *q;
+	struct list_head *entry_list;
+
+	mutex_lock(&debugger->debugfs_lock);
+
+	/* Delete debugfs entry list */
+	entry_list = &debugger->debugfs_entry_list;
+	list_for_each_entry_safe(pos, q, entry_list, list) {
+		if (pos->dent == NULL)
+			continue;
+		list_del(&pos->list);
+		kfree(pos);
+		pos = NULL;
+	}
+
+	/* Delete all debugfs node in this directory */
+	debugfs_remove_recursive(debugger->debugfs_dir);
+	debugger->debugfs_dir = NULL;
+
+	mutex_unlock(&debugger->debugfs_lock);
+
+	return 0;
+}
+
+static int rknpu_debugfs_create_files(const struct rknpu_debugger_list *files,
+				      int count, struct dentry *root,
+				      struct rknpu_debugger *debugger)
+{
+	int i;
+	struct dentry *ent;
+	struct rknpu_debugger_node *tmp;
+
+	for (i = 0; i < count; i++) {
+		tmp = kmalloc(sizeof(struct rknpu_debugger_node), GFP_KERNEL);
+		if (tmp == NULL) {
+			LOG_ERROR(
+				"Cannot alloc node path /sys/kernel/debug/%pd/%s\n",
+				root, files[i].name);
+			goto MALLOC_FAIL;
+		}
+
+		tmp->info_ent = &files[i];
+		tmp->debugger = debugger;
+
+		ent = debugfs_create_file(files[i].name, S_IFREG | S_IRUGO,
+					  root, tmp, &rknpu_debugfs_fops);
+		if (!ent) {
+			LOG_ERROR("Cannot create /sys/kernel/debug/%pd/%s\n",
+				  root, files[i].name);
+			goto CREATE_FAIL;
+		}
+
+		tmp->dent = ent;
+
+		mutex_lock(&debugger->debugfs_lock);
+		list_add_tail(&tmp->list, &debugger->debugfs_entry_list);
+		mutex_unlock(&debugger->debugfs_lock);
+	}
+
+	return 0;
+
+CREATE_FAIL:
+	kfree(tmp);
+MALLOC_FAIL:
+	rknpu_debugfs_remove_files(debugger);
+
+	return -1;
+}
+
+static int rknpu_debugfs_remove(struct rknpu_debugger *debugger)
+{
+	rknpu_debugfs_remove_files(debugger);
+
+	return 0;
+}
+
+static int rknpu_debugfs_init(struct rknpu_debugger *debugger)
+{
+	int ret;
+
+	debugger->debugfs_dir =
+		debugfs_create_dir(RKNPU_DEBUGGER_ROOT_NAME, NULL);
+	if (IS_ERR_OR_NULL(debugger->debugfs_dir)) {
+		LOG_ERROR("failed on mkdir /sys/kernel/debug/%s\n",
+			  RKNPU_DEBUGGER_ROOT_NAME);
+		debugger->debugfs_dir = NULL;
+		return -EIO;
+	}
+
+	ret = rknpu_debugfs_create_files(rknpu_debugger_root_list,
+					 ARRAY_SIZE(rknpu_debugger_root_list),
+					 debugger->debugfs_dir, debugger);
+	if (ret) {
+		LOG_ERROR(
+			"Could not install rknpu_debugger_root_list debugfs\n");
+		goto CREATE_FAIL;
+	}
+
+	return 0;
+
+CREATE_FAIL:
+	rknpu_debugfs_remove(debugger);
+
+	return ret;
+}
+#endif /* #ifdef CONFIG_ROCKCHIP_RKNPU_DEBUG_FS */
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_PROC_FS
+static int rknpu_procfs_open(struct inode *inode, struct file *file)
+{
+	struct rknpu_debugger_node *node = pde_data(inode);
+
+	return single_open(file, node->info_ent->show, node);
+}
+
+static const struct proc_ops rknpu_procfs_fops = {
+	.proc_open = rknpu_procfs_open,
+	.proc_read = seq_read,
+	.proc_lseek = seq_lseek,
+	.proc_release = single_release,
+	.proc_write = rknpu_debugger_write,
+};
+
+static int rknpu_procfs_remove_files(struct rknpu_debugger *debugger)
+{
+	struct rknpu_debugger_node *pos, *q;
+	struct list_head *entry_list;
+
+	mutex_lock(&debugger->procfs_lock);
+
+	/* Delete procfs entry list */
+	entry_list = &debugger->procfs_entry_list;
+	list_for_each_entry_safe(pos, q, entry_list, list) {
+		if (pos->pent == NULL)
+			continue;
+		list_del(&pos->list);
+		kfree(pos);
+		pos = NULL;
+	}
+
+	/* Delete all procfs node in this directory */
+	proc_remove(debugger->procfs_dir);
+	debugger->procfs_dir = NULL;
+
+	mutex_unlock(&debugger->procfs_lock);
+
+	return 0;
+}
+
+static int rknpu_procfs_create_files(const struct rknpu_debugger_list *files,
+				     int count, struct proc_dir_entry *root,
+				     struct rknpu_debugger *debugger)
+{
+	int i;
+	struct proc_dir_entry *ent;
+	struct rknpu_debugger_node *tmp;
+
+	for (i = 0; i < count; i++) {
+		tmp = kmalloc(sizeof(struct rknpu_debugger_node), GFP_KERNEL);
+		if (tmp == NULL) {
+			LOG_ERROR("Cannot alloc node path for /proc/%s/%s\n",
+				  RKNPU_DEBUGGER_ROOT_NAME, files[i].name);
+			goto MALLOC_FAIL;
+		}
+
+		tmp->info_ent = &files[i];
+		tmp->debugger = debugger;
+
+		ent = proc_create_data(files[i].name, S_IFREG | S_IRUGO, root,
+				       &rknpu_procfs_fops, tmp);
+		if (!ent) {
+			LOG_ERROR("Cannot create /proc/%s/%s\n",
+				  RKNPU_DEBUGGER_ROOT_NAME, files[i].name);
+			goto CREATE_FAIL;
+		}
+
+		tmp->pent = ent;
+
+		mutex_lock(&debugger->procfs_lock);
+		list_add_tail(&tmp->list, &debugger->procfs_entry_list);
+		mutex_unlock(&debugger->procfs_lock);
+	}
+
+	return 0;
+
+CREATE_FAIL:
+	kfree(tmp);
+MALLOC_FAIL:
+	rknpu_procfs_remove_files(debugger);
+	return -1;
+}
+
+static int rknpu_procfs_remove(struct rknpu_debugger *debugger)
+{
+	rknpu_procfs_remove_files(debugger);
+
+	return 0;
+}
+
+static int rknpu_procfs_init(struct rknpu_debugger *debugger)
+{
+	int ret;
+
+	debugger->procfs_dir = proc_mkdir(RKNPU_DEBUGGER_ROOT_NAME, NULL);
+	if (IS_ERR_OR_NULL(debugger->procfs_dir)) {
+		pr_err("failed on mkdir /proc/%s\n", RKNPU_DEBUGGER_ROOT_NAME);
+		debugger->procfs_dir = NULL;
+		return -EIO;
+	}
+
+	ret = rknpu_procfs_create_files(rknpu_debugger_root_list,
+					ARRAY_SIZE(rknpu_debugger_root_list),
+					debugger->procfs_dir, debugger);
+	if (ret) {
+		pr_err("Could not install rknpu_debugger_root_list procfs\n");
+		goto CREATE_FAIL;
+	}
+
+	return 0;
+
+CREATE_FAIL:
+	rknpu_procfs_remove(debugger);
+
+	return ret;
+}
+#endif /* #ifdef CONFIG_ROCKCHIP_RKNPU_PROC_FS */
+
+int rknpu_debugger_init(struct rknpu_device *rknpu_dev)
+{
+#ifdef CONFIG_ROCKCHIP_RKNPU_DEBUG_FS
+	mutex_init(&rknpu_dev->debugger.debugfs_lock);
+	INIT_LIST_HEAD(&rknpu_dev->debugger.debugfs_entry_list);
+	rknpu_debugfs_init(&rknpu_dev->debugger);
+#endif
+#ifdef CONFIG_ROCKCHIP_RKNPU_PROC_FS
+	mutex_init(&rknpu_dev->debugger.procfs_lock);
+	INIT_LIST_HEAD(&rknpu_dev->debugger.procfs_entry_list);
+	rknpu_procfs_init(&rknpu_dev->debugger);
+#endif
+	return 0;
+}
+
+int rknpu_debugger_remove(struct rknpu_device *rknpu_dev)
+{
+#ifdef CONFIG_ROCKCHIP_RKNPU_DEBUG_FS
+	rknpu_debugfs_remove(&rknpu_dev->debugger);
+#endif
+#ifdef CONFIG_ROCKCHIP_RKNPU_PROC_FS
+	rknpu_procfs_remove(&rknpu_dev->debugger);
+#endif
+	return 0;
+}
diff --git a/drivers/rknpu/rknpu_drv.c b/drivers/rknpu/rknpu_drv.c
new file mode 100644
index 000000000..574284012
--- /dev/null
+++ b/drivers/rknpu/rknpu_drv.c
@@ -0,0 +1,1907 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#include <linux/dma-buf.h>
+#include <linux/dma-mapping.h>
+#include <linux/fs.h>
+#include <linux/interrupt.h>
+#include <linux/irq.h>
+#include <linux/irqdomain.h>
+#include <linux/iopoll.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+#include <linux/of_irq.h>
+#include <linux/of_platform.h>
+#include <linux/of_reserved_mem.h>
+#include <linux/platform_device.h>
+#include <linux/printk.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/time.h>
+#include <linux/uaccess.h>
+#include <linux/ktime.h>
+#include <linux/delay.h>
+#include <linux/wait.h>
+#include <linux/sched.h>
+#include <linux/clk.h>
+#include <linux/clk-provider.h>
+#include <linux/pm_domain.h>
+#include <linux/pm_runtime.h>
+#include <linux/devfreq_cooling.h>
+#include <linux/regmap.h>
+#include <linux/dma-iommu.h>
+#include <linux/of_address.h>
+
+#ifndef FPGA_PLATFORM
+#include <soc/rockchip/rockchip_iommu.h>
+#include <soc/rockchip/rockchip_opp_select.h>
+#include <soc/rockchip/rockchip_system_monitor.h>
+#include <soc/rockchip/rockchip_ipa.h>
+#include <../drivers/devfreq/governor.h>
+#endif
+
+#include "rknpu_ioctl.h"
+#include "rknpu_reset.h"
+#include "rknpu_fence.h"
+#include "rknpu_drv.h"
+#include "rknpu_gem.h"
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_DRM_GEM
+#include <drm/drm_device.h>
+#include <drm/drm_ioctl.h>
+#include <drm/drm_file.h>
+#include <drm/drm_drv.h>
+#include <drm/drm_gem.h>
+#include "rknpu_gem.h"
+#endif
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_DMA_HEAP
+#include <linux/rk-dma-heap.h>
+#include "rknpu_mem.h"
+#endif
+
+#define POWER_DOWN_FREQ 200000000
+#define NPU_MMU_DISABLED_POLL_PERIOD_US 1000
+#define NPU_MMU_DISABLED_POLL_TIMEOUT_US 20000
+
+static int bypass_irq_handler;
+module_param(bypass_irq_handler, int, 0644);
+MODULE_PARM_DESC(bypass_irq_handler,
+		 "bypass RKNPU irq handler if set it to 1, disabled by default");
+
+static int bypass_soft_reset;
+module_param(bypass_soft_reset, int, 0644);
+MODULE_PARM_DESC(bypass_soft_reset,
+		 "bypass RKNPU soft reset if set it to 1, disabled by default");
+
+struct npu_irqs_data {
+	const char *name;
+	irqreturn_t (*irq_hdl)(int irq, void *ctx);
+};
+
+static const struct npu_irqs_data rk356x_npu_irqs[] = {
+	{ "npu_irq", rknpu_core0_irq_handler }
+};
+
+static const struct npu_irqs_data rk3588_npu_irqs[] = {
+	{ "npu0_irq", rknpu_core0_irq_handler },
+	{ "npu1_irq", rknpu_core1_irq_handler },
+	{ "npu2_irq", rknpu_core2_irq_handler }
+};
+
+static const struct npu_irqs_data rv110x_npu_irqs[] = {
+	{ "npu_irq", rknpu_core0_irq_handler }
+};
+
+static const struct npu_reset_data rk356x_npu_resets[] = { { "srst_a",
+							     "srst_h" } };
+
+static const struct npu_reset_data rk3588_npu_resets[] = {
+	{ "srst_a0", "srst_h0" },
+	{ "srst_a1", "srst_h1" },
+	{ "srst_a2", "srst_h2" }
+};
+
+static const struct npu_reset_data rv110x_npu_resets[] = { { "srst_a",
+							     "srst_h" } };
+
+static const struct rknpu_config rk356x_rknpu_config = {
+	.bw_priority_addr = 0xfe180008,
+	.bw_priority_length = 0x10,
+	.dma_mask = DMA_BIT_MASK(32),
+	.pc_data_amount_scale = 1,
+	.pc_task_number_bits = 12,
+	.pc_task_number_mask = 0xfff,
+	.bw_enable = 1,
+	.irqs = rk356x_npu_irqs,
+	.resets = rk356x_npu_resets,
+	.num_irqs = ARRAY_SIZE(rk356x_npu_irqs),
+	.num_resets = ARRAY_SIZE(rk356x_npu_resets)
+};
+
+static const struct rknpu_config rk3588_rknpu_config = {
+	.bw_priority_addr = 0x0,
+	.bw_priority_length = 0x0,
+	.dma_mask = DMA_BIT_MASK(40),
+	.pc_data_amount_scale = 2,
+	.pc_task_number_bits = 12,
+	.pc_task_number_mask = 0xfff,
+	.bw_enable = 0,
+	.irqs = rk3588_npu_irqs,
+	.resets = rk3588_npu_resets,
+	.num_irqs = ARRAY_SIZE(rk3588_npu_irqs),
+	.num_resets = ARRAY_SIZE(rk3588_npu_resets)
+};
+
+static const struct rknpu_config rv1106_rknpu_config = {
+	.bw_priority_addr = 0x0,
+	.bw_priority_length = 0x0,
+	.dma_mask = DMA_BIT_MASK(32),
+	.pc_data_amount_scale = 2,
+	.pc_task_number_bits = 16,
+	.pc_task_number_mask = 0xffff,
+	.bw_enable = 1,
+	.irqs = rv110x_npu_irqs,
+	.resets = rv110x_npu_resets,
+	.num_irqs = ARRAY_SIZE(rv110x_npu_irqs),
+	.num_resets = ARRAY_SIZE(rv110x_npu_resets)
+};
+
+/* driver probe and init */
+static const struct of_device_id rknpu_of_match[] = {
+	{
+		.compatible = "rockchip,rknpu",
+		.data = &rk356x_rknpu_config,
+	},
+	{
+		.compatible = "rockchip,rk3568-rknpu",
+		.data = &rk356x_rknpu_config,
+	},
+	{
+		.compatible = "rockchip,rk3588-rknpu",
+		.data = &rk3588_rknpu_config,
+	},
+	{
+		.compatible = "rockchip,rv1106-rknpu",
+		.data = &rv1106_rknpu_config,
+	},
+	{},
+};
+
+static int rknpu_get_drv_version(uint32_t *version)
+{
+	*version = RKNPU_GET_DRV_VERSION_CODE(DRIVER_MAJOR, DRIVER_MINOR,
+					      DRIVER_PATCHLEVEL);
+	return 0;
+}
+
+static int rknpu_power_on(struct rknpu_device *rknpu_dev);
+static int rknpu_power_off(struct rknpu_device *rknpu_dev);
+
+static void rknpu_power_off_delay_work(struct work_struct *power_off_work)
+{
+	struct rknpu_device *rknpu_dev =
+		container_of(to_delayed_work(power_off_work),
+			     struct rknpu_device, power_off_work);
+	mutex_lock(&rknpu_dev->power_lock);
+	if (atomic_dec_if_positive(&rknpu_dev->power_refcount) == 0)
+		rknpu_power_off(rknpu_dev);
+	mutex_unlock(&rknpu_dev->power_lock);
+}
+
+int rknpu_power_get(struct rknpu_device *rknpu_dev)
+{
+	int ret = 0;
+
+	cancel_delayed_work(&rknpu_dev->power_off_work);
+	mutex_lock(&rknpu_dev->power_lock);
+	if (atomic_inc_return(&rknpu_dev->power_refcount) == 1)
+		ret = rknpu_power_on(rknpu_dev);
+	mutex_unlock(&rknpu_dev->power_lock);
+
+	return ret;
+}
+
+int rknpu_power_put(struct rknpu_device *rknpu_dev)
+{
+	int ret = 0;
+
+	mutex_lock(&rknpu_dev->power_lock);
+	if (atomic_dec_if_positive(&rknpu_dev->power_refcount) == 0)
+		ret = rknpu_power_off(rknpu_dev);
+	mutex_unlock(&rknpu_dev->power_lock);
+
+	return ret;
+}
+
+static int rknpu_power_put_delay(struct rknpu_device *rknpu_dev)
+{
+	mutex_lock(&rknpu_dev->power_lock);
+	if (atomic_read(&rknpu_dev->power_refcount) == 1)
+		queue_delayed_work(
+			rknpu_dev->power_off_wq, &rknpu_dev->power_off_work,
+			msecs_to_jiffies(rknpu_dev->power_put_delay));
+	else
+		atomic_dec_if_positive(&rknpu_dev->power_refcount);
+	mutex_unlock(&rknpu_dev->power_lock);
+	return 0;
+}
+
+static int rknpu_action(struct rknpu_device *rknpu_dev,
+			struct rknpu_action *args)
+{
+	int ret = -EINVAL;
+
+	switch (args->flags) {
+	case RKNPU_GET_HW_VERSION:
+		ret = rknpu_get_hw_version(rknpu_dev, &args->value);
+		break;
+	case RKNPU_GET_DRV_VERSION:
+		ret = rknpu_get_drv_version(&args->value);
+		break;
+	case RKNPU_GET_FREQ:
+		args->value = clk_get_rate(rknpu_dev->clks[0].clk);
+		ret = 0;
+		break;
+	case RKNPU_SET_FREQ:
+		break;
+	case RKNPU_GET_VOLT:
+		args->value = regulator_get_voltage(rknpu_dev->vdd);
+		ret = 0;
+		break;
+	case RKNPU_SET_VOLT:
+		break;
+	case RKNPU_ACT_RESET:
+		ret = rknpu_soft_reset(rknpu_dev);
+		break;
+	case RKNPU_GET_BW_PRIORITY:
+		ret = rknpu_get_bw_priority(rknpu_dev, &args->value, NULL,
+					    NULL);
+		break;
+	case RKNPU_SET_BW_PRIORITY:
+		ret = rknpu_set_bw_priority(rknpu_dev, args->value, 0, 0);
+		break;
+	case RKNPU_GET_BW_EXPECT:
+		ret = rknpu_get_bw_priority(rknpu_dev, NULL, &args->value,
+					    NULL);
+		break;
+	case RKNPU_SET_BW_EXPECT:
+		ret = rknpu_set_bw_priority(rknpu_dev, 0, args->value, 0);
+		break;
+	case RKNPU_GET_BW_TW:
+		ret = rknpu_get_bw_priority(rknpu_dev, NULL, NULL,
+					    &args->value);
+		break;
+	case RKNPU_SET_BW_TW:
+		ret = rknpu_set_bw_priority(rknpu_dev, 0, 0, args->value);
+		break;
+	case RKNPU_ACT_CLR_TOTAL_RW_AMOUNT:
+		ret = rknpu_clear_rw_amount(rknpu_dev);
+		break;
+	case RKNPU_GET_DT_WR_AMOUNT:
+		ret = rknpu_get_rw_amount(rknpu_dev, &args->value, NULL, NULL);
+		break;
+	case RKNPU_GET_DT_RD_AMOUNT:
+		ret = rknpu_get_rw_amount(rknpu_dev, NULL, &args->value, NULL);
+		break;
+	case RKNPU_GET_WT_RD_AMOUNT:
+		ret = rknpu_get_rw_amount(rknpu_dev, NULL, NULL, &args->value);
+		break;
+	case RKNPU_GET_TOTAL_RW_AMOUNT:
+		ret = rknpu_get_total_rw_amount(rknpu_dev, &args->value);
+		break;
+	case RKNPU_GET_IOMMU_EN:
+		args->value = rknpu_dev->iommu_en;
+		ret = 0;
+		break;
+	case RKNPU_SET_PROC_NICE:
+		set_user_nice(current, *(int32_t *)&args->value);
+		ret = 0;
+		break;
+	case RKNPU_GET_TOTAL_SRAM_SIZE:
+		if (rknpu_dev->sram_mm)
+			args->value = rknpu_dev->sram_mm->total_chunks *
+				      rknpu_dev->sram_mm->chunk_size;
+		else
+			args->value = 0;
+		ret = 0;
+		break;
+	case RKNPU_GET_FREE_SRAM_SIZE:
+		if (rknpu_dev->sram_mm)
+			args->value = rknpu_dev->sram_mm->free_chunks *
+				      rknpu_dev->sram_mm->chunk_size;
+		else
+			args->value = 0;
+		ret = 0;
+		break;
+	default:
+		ret = -EINVAL;
+		break;
+	}
+
+	return ret;
+}
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_DMA_HEAP
+static int rknpu_open(struct inode *inode, struct file *file)
+{
+	return nonseekable_open(inode, file);
+}
+
+static int rknpu_release(struct inode *inode, struct file *file)
+{
+	return 0;
+}
+
+static int rknpu_action_ioctl(struct rknpu_device *rknpu_dev,
+			      unsigned long data)
+{
+	struct rknpu_action args;
+	int ret = -EINVAL;
+
+	if (unlikely(copy_from_user(&args, (struct rknpu_action *)data,
+				    sizeof(struct rknpu_action)))) {
+		LOG_ERROR("%s: copy_from_user failed\n", __func__);
+		ret = -EFAULT;
+		return ret;
+	}
+
+	ret = rknpu_action(rknpu_dev, &args);
+
+	if (unlikely(copy_to_user((struct rknpu_action *)data, &args,
+				  sizeof(struct rknpu_action)))) {
+		LOG_ERROR("%s: copy_to_user failed\n", __func__);
+		ret = -EFAULT;
+		return ret;
+	}
+
+	return ret;
+}
+
+static long rknpu_ioctl(struct file *file, uint32_t cmd, unsigned long arg)
+{
+	long ret = -EINVAL;
+	struct rknpu_device *rknpu_dev =
+		container_of(file->private_data, struct rknpu_device, miscdev);
+
+	rknpu_power_get(rknpu_dev);
+
+	switch (cmd) {
+	case IOCTL_RKNPU_ACTION:
+		ret = rknpu_action_ioctl(rknpu_dev, arg);
+		break;
+	case IOCTL_RKNPU_SUBMIT:
+		ret = rknpu_submit_ioctl(rknpu_dev, arg);
+		break;
+	case IOCTL_RKNPU_MEM_CREATE:
+		ret = rknpu_mem_create_ioctl(rknpu_dev, arg);
+		break;
+	case RKNPU_MEM_MAP:
+		break;
+	case IOCTL_RKNPU_MEM_DESTROY:
+		ret = rknpu_mem_destroy_ioctl(rknpu_dev, arg);
+		break;
+	case IOCTL_RKNPU_MEM_SYNC:
+		ret = rknpu_mem_sync_ioctl(rknpu_dev, arg);
+		break;
+	default:
+		break;
+	}
+
+	rknpu_power_put_delay(rknpu_dev);
+
+	return ret;
+}
+const struct file_operations rknpu_fops = {
+	.owner = THIS_MODULE,
+	.open = rknpu_open,
+	.release = rknpu_release,
+	.unlocked_ioctl = rknpu_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl = rknpu_ioctl,
+#endif
+};
+#endif
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_DRM_GEM
+static int rknpu_action_ioctl(struct drm_device *dev, void *data,
+			      struct drm_file *file_priv)
+{
+	struct rknpu_device *rknpu_dev = dev_get_drvdata(dev->dev);
+
+	return rknpu_action(rknpu_dev, (struct rknpu_action *)data);
+}
+
+#define RKNPU_IOCTL(func)                                                      \
+	static int __##func(struct drm_device *dev, void *data,                \
+			    struct drm_file *file_priv)                        \
+	{                                                                      \
+		struct rknpu_device *rknpu_dev = dev_get_drvdata(dev->dev);    \
+		int ret = -EINVAL;                                             \
+		rknpu_power_get(rknpu_dev);                                    \
+		ret = func(dev, data, file_priv);                              \
+		rknpu_power_put_delay(rknpu_dev);                              \
+		return ret;                                                    \
+	}
+
+RKNPU_IOCTL(rknpu_action_ioctl);
+RKNPU_IOCTL(rknpu_submit_ioctl);
+RKNPU_IOCTL(rknpu_gem_create_ioctl);
+RKNPU_IOCTL(rknpu_gem_map_ioctl);
+RKNPU_IOCTL(rknpu_gem_destroy_ioctl);
+RKNPU_IOCTL(rknpu_gem_sync_ioctl);
+
+static const struct drm_ioctl_desc rknpu_ioctls[] = {
+	DRM_IOCTL_DEF_DRV(RKNPU_ACTION, __rknpu_action_ioctl, DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(RKNPU_SUBMIT, __rknpu_submit_ioctl, DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(RKNPU_MEM_CREATE, __rknpu_gem_create_ioctl,
+			  DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(RKNPU_MEM_MAP, __rknpu_gem_map_ioctl,
+			  DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(RKNPU_MEM_DESTROY, __rknpu_gem_destroy_ioctl,
+			  DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(RKNPU_MEM_SYNC, __rknpu_gem_sync_ioctl,
+			  DRM_RENDER_ALLOW),
+};
+
+static const struct file_operations rknpu_drm_driver_fops = {
+	.owner = THIS_MODULE,
+	.open = drm_open,
+	.mmap = rknpu_gem_mmap,
+	.poll = drm_poll,
+	.read = drm_read,
+	.unlocked_ioctl = drm_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl = drm_compat_ioctl,
+#endif
+	.release = drm_release,
+	.llseek = noop_llseek,
+};
+
+static struct drm_driver rknpu_drm_driver = {
+#if KERNEL_VERSION(5, 4, 0) <= LINUX_VERSION_CODE
+	.driver_features = DRIVER_GEM | DRIVER_RENDER,
+#else
+	.driver_features = DRIVER_GEM | DRIVER_PRIME | DRIVER_RENDER,
+#endif
+	.dumb_create = rknpu_gem_dumb_create,
+#if KERNEL_VERSION(4, 19, 0) > LINUX_VERSION_CODE
+	.dumb_map_offset = rknpu_gem_dumb_map_offset,
+#else
+	.dumb_map_offset = drm_gem_dumb_map_offset,
+#endif
+	// Has forced a default implementation since 6.4,
+	// so they are no longer necessary fields of the structure.
+	// .dumb_destroy = drm_gem_dumb_destroy,
+	// .prime_handle_to_fd = drm_gem_prime_handle_to_fd,
+	// .prime_fd_to_handle = drm_gem_prime_fd_to_handle,
+	// .gem_prime_export = drm_gem_prime_export,
+#if KERNEL_VERSION(4, 13, 0) <= LINUX_VERSION_CODE
+	.gem_prime_import = rknpu_gem_prime_import,
+#else
+	.gem_prime_import = drm_gem_prime_import,
+#endif
+	.gem_prime_import_sg_table = rknpu_gem_prime_import_sg_table,
+	.ioctls = rknpu_ioctls,
+	.num_ioctls = ARRAY_SIZE(rknpu_ioctls),
+	.fops = &rknpu_drm_driver_fops,
+	.name = DRIVER_NAME,
+	.desc = DRIVER_DESC,
+	.date = DRIVER_DATE,
+	.major = DRIVER_MAJOR,
+	.minor = DRIVER_MINOR,
+	.patchlevel = DRIVER_PATCHLEVEL,
+};
+
+
+#endif
+
+static enum hrtimer_restart hrtimer_handler(struct hrtimer *timer)
+{
+	struct rknpu_device *rknpu_dev =
+		container_of(timer, struct rknpu_device, timer);
+	struct rknpu_subcore_data *subcore_data = NULL;
+	struct rknpu_job *job = NULL;
+	ktime_t now = ktime_get();
+	unsigned long flags;
+	int i;
+
+	for (i = 0; i < rknpu_dev->config->num_irqs; i++) {
+		subcore_data = &rknpu_dev->subcore_datas[i];
+
+		spin_lock_irqsave(&rknpu_dev->irq_lock, flags);
+
+		job = subcore_data->job;
+		if (job) {
+			subcore_data->timer.busy_time +=
+				ktime_us_delta(now, job->hw_recoder_time);
+			job->hw_recoder_time = ktime_get();
+		}
+
+		subcore_data->timer.busy_time_record =
+			subcore_data->timer.busy_time;
+		subcore_data->timer.busy_time = 0;
+		spin_unlock_irqrestore(&rknpu_dev->irq_lock, flags);
+	}
+
+	hrtimer_forward_now(timer, rknpu_dev->kt);
+	return HRTIMER_RESTART;
+}
+
+static void rknpu_init_timer(struct rknpu_device *rknpu_dev)
+{
+	rknpu_dev->kt = ktime_set(0, RKNPU_LOAD_INTERVAL);
+	hrtimer_init(&rknpu_dev->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	rknpu_dev->timer.function = hrtimer_handler;
+	hrtimer_start(&rknpu_dev->timer, rknpu_dev->kt, HRTIMER_MODE_REL);
+}
+
+static void rknpu_cancel_timer(struct rknpu_device *rknpu_dev)
+{
+	hrtimer_cancel(&rknpu_dev->timer);
+}
+
+static bool rknpu_is_iommu_enable(struct device *dev)
+{
+	struct device_node *iommu = NULL;
+
+	iommu = of_parse_phandle(dev->of_node, "iommus", 0);
+	if (!iommu) {
+		LOG_DEV_INFO(
+			dev,
+			"rknpu iommu device-tree entry not found!, using non-iommu mode\n");
+		return false;
+	}
+
+	if (!of_device_is_available(iommu)) {
+		LOG_DEV_INFO(dev,
+			     "rknpu iommu is disabled, using non-iommu mode\n");
+		of_node_put(iommu);
+		return false;
+	}
+	of_node_put(iommu);
+
+	LOG_DEV_INFO(dev, "rknpu iommu is enabled, using iommu mode\n");
+
+	return true;
+}
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_DRM_GEM
+static int rknpu_drm_probe(struct rknpu_device *rknpu_dev)
+{
+	struct device *dev = rknpu_dev->dev;
+	struct drm_device *drm_dev = NULL;
+	int ret = -EINVAL;
+
+	drm_dev = drm_dev_alloc(&rknpu_drm_driver, dev);
+	if (IS_ERR(drm_dev))
+		return PTR_ERR(drm_dev);
+
+	/* register the DRM device */
+	ret = drm_dev_register(drm_dev, 0);
+	if (ret < 0)
+		goto err_free_drm;
+
+	drm_dev->dev_private = rknpu_dev;
+	rknpu_dev->drm_dev = drm_dev;
+
+	return 0;
+
+err_free_drm:
+#if KERNEL_VERSION(4, 15, 0) <= LINUX_VERSION_CODE
+	drm_dev_put(drm_dev);
+#else
+	drm_dev_unref(drm_dev);
+#endif
+
+	return ret;
+}
+
+static void rknpu_drm_remove(struct rknpu_device *rknpu_dev)
+{
+	struct drm_device *drm_dev = rknpu_dev->drm_dev;
+
+	drm_dev_unregister(drm_dev);
+
+#if KERNEL_VERSION(4, 15, 0) <= LINUX_VERSION_CODE
+	drm_dev_put(drm_dev);
+#else
+	drm_dev_unref(drm_dev);
+#endif
+}
+#endif
+
+static int rknpu_power_on(struct rknpu_device *rknpu_dev)
+{
+	struct device *dev = rknpu_dev->dev;
+	int ret = -EINVAL;
+
+#ifndef FPGA_PLATFORM
+	if (rknpu_dev->vdd) {
+		ret = regulator_enable(rknpu_dev->vdd);
+		if (ret) {
+			LOG_DEV_ERROR(
+				dev,
+				"failed to enable vdd reg for rknpu, ret: %d\n",
+				ret);
+			return ret;
+		}
+	}
+
+	if (rknpu_dev->mem) {
+		ret = regulator_enable(rknpu_dev->mem);
+		if (ret) {
+			LOG_DEV_ERROR(
+				dev,
+				"failed to enable mem reg for rknpu, ret: %d\n",
+				ret);
+			return ret;
+		}
+	}
+#endif
+
+	ret = clk_bulk_prepare_enable(rknpu_dev->num_clks, rknpu_dev->clks);
+	if (ret) {
+		LOG_DEV_ERROR(dev, "failed to enable clk for rknpu, ret: %d\n",
+			      ret);
+		return ret;
+	}
+
+#ifndef FPGA_PLATFORM
+#if KERNEL_VERSION(5, 10, 0) <= LINUX_VERSION_CODE
+	rockchip_monitor_volt_adjust_lock(rknpu_dev->mdev_info);
+#endif
+#endif
+
+	if (rknpu_dev->multiple_domains) {
+		if (rknpu_dev->genpd_dev_npu0) {
+#if KERNEL_VERSION(5, 5, 0) < LINUX_VERSION_CODE
+			ret = pm_runtime_resume_and_get(
+				rknpu_dev->genpd_dev_npu0);
+#else
+			ret = pm_runtime_get_sync(rknpu_dev->genpd_dev_npu0);
+#endif
+			if (ret < 0) {
+				LOG_DEV_ERROR(
+					dev,
+					"failed to get pm runtime for npu0, ret: %d\n",
+					ret);
+				goto out;
+			}
+		}
+		if (rknpu_dev->genpd_dev_npu1) {
+#if KERNEL_VERSION(5, 5, 0) < LINUX_VERSION_CODE
+			ret = pm_runtime_resume_and_get(
+				rknpu_dev->genpd_dev_npu1);
+#else
+			ret = pm_runtime_get_sync(rknpu_dev->genpd_dev_npu1);
+#endif
+			if (ret < 0) {
+				LOG_DEV_ERROR(
+					dev,
+					"failed to get pm runtime for npu1, ret: %d\n",
+					ret);
+				goto out;
+			}
+		}
+		if (rknpu_dev->genpd_dev_npu2) {
+#if KERNEL_VERSION(5, 5, 0) < LINUX_VERSION_CODE
+			ret = pm_runtime_resume_and_get(
+				rknpu_dev->genpd_dev_npu2);
+#else
+			ret = pm_runtime_get_sync(rknpu_dev->genpd_dev_npu2);
+#endif
+			if (ret < 0) {
+				LOG_DEV_ERROR(
+					dev,
+					"failed to get pm runtime for npu2, ret: %d\n",
+					ret);
+				goto out;
+			}
+		}
+	}
+	ret = pm_runtime_get_sync(dev);
+	if (ret < 0) {
+		LOG_DEV_ERROR(dev,
+			      "failed to get pm runtime for rknpu, ret: %d\n",
+			      ret);
+	}
+
+out:
+#ifndef FPGA_PLATFORM
+#if KERNEL_VERSION(5, 10, 0) <= LINUX_VERSION_CODE
+	rockchip_monitor_volt_adjust_unlock(rknpu_dev->mdev_info);
+#endif
+#endif
+
+	return ret;
+}
+
+static int rknpu_power_off(struct rknpu_device *rknpu_dev)
+{
+	struct device *dev = rknpu_dev->dev;
+
+#ifndef FPGA_PLATFORM
+	int ret;
+	// bool val;
+
+#if KERNEL_VERSION(5, 10, 0) <= LINUX_VERSION_CODE
+	rockchip_monitor_volt_adjust_lock(rknpu_dev->mdev_info);
+#endif
+#endif
+
+	pm_runtime_put_sync(dev);
+
+	if (rknpu_dev->multiple_domains) {
+#ifndef FPGA_PLATFORM
+		/*
+		 * Because IOMMU's runtime suspend callback is asynchronous,
+		 * So it may be executed after the NPU is turned off after PD/CLK/VD,
+		 * and the runtime suspend callback has a register access.
+		 * If the PD/VD/CLK is closed, the register access will crash.
+		 * As a workaround, it's safe to close pd stuff until iommu disabled.
+		 * If pm runtime framework can handle this issue in the future, remove
+		 * this.
+		 */
+		// ret = readx_poll_timeout(rockchip_iommu_is_enabled, dev, val,
+		// 			 !val, NPU_MMU_DISABLED_POLL_PERIOD_US,
+		// 			 NPU_MMU_DISABLED_POLL_TIMEOUT_US);
+
+		// TODO: implement rockchip_iommu_is_enabled
+		ret = 0;
+		if (ret) {
+			LOG_DEV_ERROR(dev, "iommu still enabled\n");
+			pm_runtime_get_sync(dev);
+#if KERNEL_VERSION(5, 10, 0) <= LINUX_VERSION_CODE
+			rockchip_monitor_volt_adjust_unlock(
+				rknpu_dev->mdev_info);
+#endif
+			return ret;
+		}
+#else
+		if (rknpu_dev->iommu_en)
+			msleep(20);
+#endif
+		if (rknpu_dev->genpd_dev_npu2)
+			pm_runtime_put_sync(rknpu_dev->genpd_dev_npu2);
+		if (rknpu_dev->genpd_dev_npu1)
+			pm_runtime_put_sync(rknpu_dev->genpd_dev_npu1);
+		if (rknpu_dev->genpd_dev_npu0)
+			pm_runtime_put_sync(rknpu_dev->genpd_dev_npu0);
+	}
+
+#ifndef FPGA_PLATFORM
+#if KERNEL_VERSION(5, 10, 0) <= LINUX_VERSION_CODE
+	rockchip_monitor_volt_adjust_unlock(rknpu_dev->mdev_info);
+#endif
+#endif
+
+	clk_bulk_disable_unprepare(rknpu_dev->num_clks, rknpu_dev->clks);
+
+#ifndef FPGA_PLATFORM
+	if (rknpu_dev->vdd)
+		regulator_disable(rknpu_dev->vdd);
+
+	if (rknpu_dev->mem)
+		regulator_disable(rknpu_dev->mem);
+#endif
+
+	return 0;
+}
+
+#ifndef FPGA_PLATFORM
+static struct monitor_dev_profile npu_mdevp = {
+	.type = MONITOR_TPYE_DEV,
+	.low_temp_adjust = rockchip_monitor_dev_low_temp_adjust,
+	.high_temp_adjust = rockchip_monitor_dev_high_temp_adjust,
+#if KERNEL_VERSION(5, 10, 0) <= LINUX_VERSION_CODE
+	.update_volt = rockchip_monitor_check_rate_volt,
+#endif
+};
+
+#if KERNEL_VERSION(5, 10, 0) <= LINUX_VERSION_CODE
+// TODO: reimplemnt this function
+// static int npu_opp_helper(struct dev_pm_set_opp_data *data)
+// {
+// 	struct device *dev = data->dev;
+// 	struct dev_pm_opp_supply *old_supply_vdd = &data->old_opp.supplies[0];
+// 	struct dev_pm_opp_supply *old_supply_mem = &data->old_opp.supplies[1];
+// 	struct dev_pm_opp_supply *new_supply_vdd = &data->new_opp.supplies[0];
+// 	struct dev_pm_opp_supply *new_supply_mem = &data->new_opp.supplies[1];
+// 	struct regulator *vdd_reg = data->regulators[0];
+// 	struct regulator *mem_reg = data->regulators[1];
+// 	struct clk *clk = data->clk;
+// 	struct rknpu_device *rknpu_dev = dev_get_drvdata(dev);
+// 	struct rockchip_opp_info *opp_info = &rknpu_dev->opp_info;
+// 	unsigned long old_freq = data->old_opp.rate;
+// 	unsigned long new_freq = data->new_opp.rate;
+// 	bool is_set_rm = true;
+// 	bool is_set_clk = true;
+// 	u32 target_rm = UINT_MAX;
+// 	int ret = 0;
+
+// 	if (!pm_runtime_active(dev)) {
+// 		is_set_rm = false;
+// 		if (opp_info->scmi_clk)
+// 			is_set_clk = false;
+// 	}
+
+// 	ret = clk_bulk_prepare_enable(opp_info->num_clks, opp_info->clks);
+// 	if (ret < 0) {
+// 		LOG_DEV_ERROR(dev, "failed to enable opp clks\n");
+// 		return ret;
+// 	}
+// 	rockchip_get_read_margin(dev, opp_info, new_supply_vdd->u_volt,
+// 				 &target_rm);
+
+// 	/* Change frequency */
+// 	LOG_DEV_DEBUG(dev, "switching OPP: %lu Hz --> %lu Hz\n", old_freq,
+// 		      new_freq);
+// 	/* Scaling up? Scale voltage before frequency */
+// 	if (new_freq >= old_freq) {
+// 		rockchip_set_intermediate_rate(dev, opp_info, clk, old_freq,
+// 					       new_freq, true, is_set_clk);
+// 		ret = regulator_set_voltage(mem_reg, new_supply_mem->u_volt,
+// 					    INT_MAX);
+// 		if (ret) {
+// 			LOG_DEV_ERROR(dev,
+// 				      "failed to set volt %lu uV for mem reg\n",
+// 				      new_supply_mem->u_volt);
+// 			goto restore_voltage;
+// 		}
+// 		ret = regulator_set_voltage(vdd_reg, new_supply_vdd->u_volt,
+// 					    INT_MAX);
+// 		if (ret) {
+// 			LOG_DEV_ERROR(dev,
+// 				      "failed to set volt %lu uV for vdd reg\n",
+// 				      new_supply_vdd->u_volt);
+// 			goto restore_voltage;
+// 		}
+// 		rockchip_set_read_margin(dev, opp_info, target_rm, is_set_rm);
+// 		if (is_set_clk && clk_set_rate(clk, new_freq)) {
+// 			ret = -EINVAL;
+// 			LOG_DEV_ERROR(dev, "failed to set clk rate: %d\n", ret);
+// 			goto restore_rm;
+// 		}
+// 		/* Scaling down? Scale voltage after frequency */
+// 	} else {
+// 		rockchip_set_intermediate_rate(dev, opp_info, clk, old_freq,
+// 					       new_freq, false, is_set_clk);
+// 		rockchip_set_read_margin(dev, opp_info, target_rm, is_set_rm);
+// 		if (is_set_clk && clk_set_rate(clk, new_freq)) {
+// 			ret = -EINVAL;
+// 			LOG_DEV_ERROR(dev, "failed to set clk rate: %d\n", ret);
+// 			goto restore_rm;
+// 		}
+// 		ret = regulator_set_voltage(vdd_reg, new_supply_vdd->u_volt,
+// 					    INT_MAX);
+// 		if (ret) {
+// 			LOG_DEV_ERROR(dev,
+// 				      "failed to set volt %lu uV for vdd reg\n",
+// 				      new_supply_vdd->u_volt);
+// 			goto restore_freq;
+// 		}
+// 		ret = regulator_set_voltage(mem_reg, new_supply_mem->u_volt,
+// 					    INT_MAX);
+// 		if (ret) {
+// 			LOG_DEV_ERROR(dev,
+// 				      "failed to set volt %lu uV for mem reg\n",
+// 				      new_supply_mem->u_volt);
+// 			goto restore_freq;
+// 		}
+// 	}
+
+// 	clk_bulk_disable_unprepare(opp_info->num_clks, opp_info->clks);
+
+// 	return 0;
+
+// restore_freq:
+// 	if (is_set_clk && clk_set_rate(clk, old_freq))
+// 		LOG_DEV_ERROR(dev, "failed to restore old-freq %lu Hz\n",
+// 			      old_freq);
+// restore_rm:
+// 	rockchip_get_read_margin(dev, opp_info, old_supply_vdd->u_volt,
+// 				 &target_rm);
+// 	rockchip_set_read_margin(dev, opp_info, opp_info->current_rm,
+// 				 is_set_rm);
+// restore_voltage:
+// 	regulator_set_voltage(mem_reg, old_supply_mem->u_volt, INT_MAX);
+// 	regulator_set_voltage(vdd_reg, old_supply_vdd->u_volt, INT_MAX);
+// 	clk_bulk_disable_unprepare(opp_info->num_clks, opp_info->clks);
+
+// 	return ret;
+// }
+
+static int npu_devfreq_target(struct device *dev, unsigned long *freq,
+			      u32 flags)
+{
+	struct rknpu_device *rknpu_dev = dev_get_drvdata(dev);
+	struct dev_pm_opp *opp;
+	unsigned long opp_volt;
+	int ret = 0;
+
+	if (!npu_mdevp.is_checked)
+		return -EINVAL;
+
+	opp = devfreq_recommended_opp(dev, freq, flags);
+	if (IS_ERR(opp))
+		return PTR_ERR(opp);
+	opp_volt = dev_pm_opp_get_voltage(opp);
+	dev_pm_opp_put(opp);
+
+#if KERNEL_VERSION(5, 10, 0) <= LINUX_VERSION_CODE
+	rockchip_monitor_volt_adjust_lock(rknpu_dev->mdev_info);
+#endif
+	ret = dev_pm_opp_set_rate(dev, *freq);
+	if (!ret) {
+		rknpu_dev->current_freq = *freq;
+		if (rknpu_dev->devfreq)
+			rknpu_dev->devfreq->last_status.current_frequency =
+				*freq;
+		rknpu_dev->current_volt = opp_volt;
+		LOG_DEV_INFO(dev, "set rknpu freq: %lu, volt: %lu\n",
+			     rknpu_dev->current_freq, rknpu_dev->current_volt);
+	}
+#if KERNEL_VERSION(5, 10, 0) <= LINUX_VERSION_CODE
+	rockchip_monitor_volt_adjust_unlock(rknpu_dev->mdev_info);
+#endif
+
+	return ret;
+}
+
+#else
+
+static int npu_devfreq_target(struct device *dev, unsigned long *target_freq,
+			      u32 flags)
+{
+	struct rknpu_device *rknpu_dev = dev_get_drvdata(dev);
+	struct dev_pm_opp *opp = NULL;
+	unsigned long freq = *target_freq;
+	unsigned long old_freq = rknpu_dev->current_freq;
+	unsigned long volt, old_volt = rknpu_dev->current_volt;
+	int ret = -EINVAL;
+
+#if KERNEL_VERSION(4, 11, 0) > LINUX_VERSION_CODE
+	rcu_read_lock();
+#endif
+
+	opp = devfreq_recommended_opp(dev, &freq, flags);
+	if (IS_ERR(opp)) {
+#if KERNEL_VERSION(4, 11, 0) > LINUX_VERSION_CODE
+		rcu_read_unlock();
+#endif
+		LOG_DEV_ERROR(dev, "failed to get opp (%ld)\n", PTR_ERR(opp));
+		return PTR_ERR(opp);
+	}
+	volt = dev_pm_opp_get_voltage(opp);
+#if KERNEL_VERSION(4, 11, 0) > LINUX_VERSION_CODE
+	rcu_read_unlock();
+#endif
+
+	/*
+	 * Only update if there is a change of frequency
+	 */
+	if (old_freq == freq) {
+		*target_freq = freq;
+		if (old_volt == volt)
+			return 0;
+		ret = regulator_set_voltage(rknpu_dev->vdd, volt, INT_MAX);
+		if (ret) {
+			LOG_DEV_ERROR(dev, "failed to set volt %lu\n", volt);
+			return ret;
+		}
+		rknpu_dev->current_volt = volt;
+		return 0;
+	}
+
+	if (rknpu_dev->vdd && old_volt != volt && old_freq < freq) {
+		ret = regulator_set_voltage(rknpu_dev->vdd, volt, INT_MAX);
+		if (ret) {
+			LOG_DEV_ERROR(dev, "failed to increase volt %lu\n",
+				      volt);
+			return ret;
+		}
+	}
+	LOG_DEV_DEBUG(dev, "%luHz %luuV -> %luHz %luuV\n", old_freq, old_volt,
+		      freq, volt);
+	ret = clk_set_rate(rknpu_dev->clks[0].clk, freq);
+	if (ret) {
+		LOG_DEV_ERROR(dev, "failed to set clock %lu\n", freq);
+		return ret;
+	}
+	*target_freq = freq;
+	rknpu_dev->current_freq = freq;
+
+	if (rknpu_dev->devfreq)
+		rknpu_dev->devfreq->last_status.current_frequency = freq;
+
+	if (rknpu_dev->vdd && old_volt != volt && old_freq > freq) {
+		ret = regulator_set_voltage(rknpu_dev->vdd, volt, INT_MAX);
+		if (ret) {
+			LOG_DEV_ERROR(dev, "failed to decrease volt %lu\n",
+				      volt);
+			return ret;
+		}
+	}
+	rknpu_dev->current_volt = volt;
+
+	LOG_DEV_INFO(dev, "set rknpu freq: %lu, volt: %lu\n",
+		     rknpu_dev->current_freq, rknpu_dev->current_volt);
+
+	return ret;
+}
+#endif
+
+static int npu_devfreq_get_dev_status(struct device *dev,
+				      struct devfreq_dev_status *stat)
+{
+	return 0;
+}
+
+static int npu_devfreq_get_cur_freq(struct device *dev, unsigned long *freq)
+{
+	struct rknpu_device *rknpu_dev = dev_get_drvdata(dev);
+
+	*freq = rknpu_dev->current_freq;
+
+	return 0;
+}
+
+static struct devfreq_dev_profile npu_devfreq_profile = {
+	.polling_ms = 50,
+	.target = npu_devfreq_target,
+	.get_dev_status = npu_devfreq_get_dev_status,
+	.get_cur_freq = npu_devfreq_get_cur_freq,
+};
+
+static int devfreq_rknpu_ondemand_func(struct devfreq *df, unsigned long *freq)
+{
+	struct rknpu_device *rknpu_dev = df->data;
+
+	if (rknpu_dev)
+		*freq = rknpu_dev->ondemand_freq;
+	else
+		*freq = df->previous_freq;
+
+	return 0;
+}
+
+static int devfreq_rknpu_ondemand_handler(struct devfreq *devfreq,
+					  unsigned int event, void *data)
+{
+	return 0;
+}
+
+static struct devfreq_governor devfreq_rknpu_ondemand = {
+	.name = "rknpu_ondemand",
+	.get_target_freq = devfreq_rknpu_ondemand_func,
+	.event_handler = devfreq_rknpu_ondemand_handler,
+};
+
+// static unsigned long npu_get_static_power(struct devfreq *devfreq,
+// 					  unsigned long voltage)
+// {
+// 	struct device *dev = devfreq->dev.parent;
+// 	struct rknpu_device *rknpu_dev = dev_get_drvdata(dev);
+
+// 	if (!rknpu_dev->model_data)
+// 		return 0;
+
+// 	return rockchip_ipa_get_static_power(rknpu_dev->model_data, voltage);
+// }
+
+static int npu_get_real_power(struct devfreq *df, u32 *power,
+			      unsigned long freq, unsigned long voltage)
+{
+	// TODO: dummy function, need to implement
+	return 0;
+}
+
+static struct devfreq_cooling_power npu_cooling_power = {
+	.get_real_power = &npu_get_real_power,
+};
+
+#if KERNEL_VERSION(5, 10, 0) <= LINUX_VERSION_CODE
+static int rk3588_npu_set_read_margin(struct device *dev,
+				      struct rockchip_opp_info *opp_info,
+				      u32 rm)
+{
+	u32 offset = 0, val = 0;
+	int i, ret = 0;
+
+	if (!opp_info->grf || !opp_info->volt_rm_tbl)
+		return 0;
+
+	if (rm == opp_info->current_rm || rm == UINT_MAX)
+		return 0;
+
+	LOG_DEV_DEBUG(dev, "set rm to %d\n", rm);
+
+	for (i = 0; i < 3; i++) {
+		ret = regmap_read(opp_info->grf, offset, &val);
+		if (ret < 0) {
+			LOG_DEV_ERROR(dev, "failed to get rm from 0x%x\n",
+				      offset);
+			return ret;
+		}
+		val &= ~0x1c;
+		regmap_write(opp_info->grf, offset, val | (rm << 2));
+		offset += 4;
+	}
+	opp_info->current_rm = rm;
+
+	return 0;
+}
+
+static const struct rockchip_opp_data rk3588_npu_opp_data = {
+	.set_read_margin = rk3588_npu_set_read_margin,
+};
+
+static const struct of_device_id rockchip_npu_of_match[] = {
+	{
+		.compatible = "rockchip,rk3588",
+		.data = (void *)&rk3588_npu_opp_data,
+	},
+	{},
+};
+
+static int rknpu_devfreq_init(struct rknpu_device *rknpu_dev)
+{
+	struct device *dev = rknpu_dev->dev;
+	struct devfreq_dev_profile *dp = &npu_devfreq_profile;
+	struct dev_pm_opp *opp;
+	int reg_result = 0;
+	int opp_result = 0;
+	const char *const reg_names[] = { "rknpu", "mem", NULL };
+	const char *const single_reg_names[] = { "rknpu", NULL };
+	int ret = -EINVAL;
+
+	if (of_find_property(dev->of_node, "rknpu-supply", NULL) &&
+	    of_find_property(dev->of_node, "mem-supply", NULL)) {
+		// reg_result = dev_pm_opp_set_regulators(dev, reg_names, 2);
+		reg_result = dev_pm_opp_set_regulators(dev, reg_names);
+		if (reg_result < 0)
+			return reg_result;
+		// opp_result =
+		// 	dev_pm_opp_register_set_opp_helper(dev, npu_opp_helper);
+		if (opp_result < 0) {
+			dev_pm_opp_put_regulators(reg_result);
+			return opp_result;
+		}
+	} else {
+		// reg_result = dev_pm_opp_set_regulators(dev, reg_names, 1);
+		reg_result = dev_pm_opp_set_regulators(dev, single_reg_names);
+		if (reg_result < 0)
+			return reg_result;
+	}
+
+	rockchip_get_opp_data(rockchip_npu_of_match, &rknpu_dev->opp_info);
+	ret = rockchip_init_opp_table(dev, &rknpu_dev->opp_info, "npu_leakage",
+				      "rknpu");
+	if (ret) {
+		LOG_DEV_ERROR(dev, "failed to init_opp_table\n");
+		return ret;
+	}
+
+	rknpu_dev->current_freq = clk_get_rate(rknpu_dev->clks[0].clk);
+
+	opp = devfreq_recommended_opp(dev, &rknpu_dev->current_freq, 0);
+	if (IS_ERR(opp)) {
+		ret = PTR_ERR(opp);
+		goto err_remove_table;
+	}
+	dev_pm_opp_put(opp);
+	dp->initial_freq = rknpu_dev->current_freq;
+
+	ret = devfreq_add_governor(&devfreq_rknpu_ondemand);
+	if (ret) {
+		LOG_DEV_ERROR(dev, "failed to add rknpu_ondemand governor\n");
+		goto err_remove_table;
+	}
+
+	rknpu_dev->devfreq = devm_devfreq_add_device(dev, dp, "rknpu_ondemand",
+						     (void *)rknpu_dev);
+	if (IS_ERR(rknpu_dev->devfreq)) {
+		LOG_DEV_ERROR(dev, "failed to add devfreq\n");
+		ret = PTR_ERR(rknpu_dev->devfreq);
+		goto err_remove_governor;
+	}
+	devm_devfreq_register_opp_notifier(dev, rknpu_dev->devfreq);
+
+	rknpu_dev->devfreq->last_status.current_frequency = dp->initial_freq;
+	rknpu_dev->devfreq->last_status.total_time = 1;
+	rknpu_dev->devfreq->last_status.busy_time = 1;
+
+	npu_mdevp.data = rknpu_dev->devfreq;
+	npu_mdevp.opp_info = &rknpu_dev->opp_info;
+	rknpu_dev->mdev_info =
+		rockchip_system_monitor_register(dev, &npu_mdevp);
+	if (IS_ERR(rknpu_dev->mdev_info)) {
+		LOG_DEV_DEBUG(dev, "without system monitor\n");
+		rknpu_dev->mdev_info = NULL;
+		npu_mdevp.is_checked = true;
+	}
+	rknpu_dev->current_freq = clk_get_rate(rknpu_dev->clks[0].clk);
+	rknpu_dev->current_volt = regulator_get_voltage(rknpu_dev->vdd);
+
+	
+	u32 dyn_power_coeff = 0;
+	of_property_read_u32(dev->of_node, "dynamic-power-coefficient",
+			    		&dyn_power_coeff);
+	rknpu_dev->model_data =
+		rockchip_ipa_power_model_init(dev, "npu_leakage");
+	if (IS_ERR_OR_NULL(rknpu_dev->model_data)) {
+		rknpu_dev->model_data = NULL;
+		LOG_DEV_ERROR(dev, "failed to initialize power model\n");
+	} else if (rknpu_dev->model_data->dynamic_coefficient) {
+		// TODO: implement power model
+		dyn_power_coeff =
+			rknpu_dev->model_data->dynamic_coefficient;
+	}
+	if (!dyn_power_coeff) {
+		LOG_DEV_ERROR(dev, "failed to get dynamic-coefficient\n");
+		goto out;
+	}
+
+	rknpu_dev->devfreq_cooling = of_devfreq_cooling_register_power(
+		dev->of_node, rknpu_dev->devfreq, &npu_cooling_power);
+	if (IS_ERR_OR_NULL(rknpu_dev->devfreq_cooling))
+		LOG_DEV_ERROR(dev, "failed to register cooling device\n");
+
+out:
+	return 0;
+
+err_remove_governor:
+	devfreq_remove_governor(&devfreq_rknpu_ondemand);
+err_remove_table:
+	dev_pm_opp_of_remove_table(dev);
+
+	rknpu_dev->devfreq = NULL;
+
+	return ret;
+}
+
+#else
+
+static int npu_devfreq_adjust_current_freq_volt(struct device *dev,
+						struct rknpu_device *rknpu_dev)
+{
+	unsigned long volt, old_freq, freq;
+	struct dev_pm_opp *opp = NULL;
+	int ret = -EINVAL;
+
+	old_freq = clk_get_rate(rknpu_dev->clks[0].clk);
+	freq = old_freq;
+
+#if KERNEL_VERSION(4, 11, 0) > LINUX_VERSION_CODE
+	rcu_read_lock();
+#endif
+
+	opp = devfreq_recommended_opp(dev, &freq, 0);
+	volt = dev_pm_opp_get_voltage(opp);
+
+#if KERNEL_VERSION(4, 11, 0) > LINUX_VERSION_CODE
+	rcu_read_unlock();
+#endif
+
+	if (freq >= old_freq && rknpu_dev->vdd) {
+		ret = regulator_set_voltage(rknpu_dev->vdd, volt, INT_MAX);
+		if (ret) {
+			LOG_DEV_ERROR(dev, "failed to set volt %lu\n", volt);
+			return ret;
+		}
+	}
+	LOG_DEV_DEBUG(dev, "adjust current freq=%luHz, volt=%luuV\n", freq,
+		      volt);
+	ret = clk_set_rate(rknpu_dev->clks[0].clk, freq);
+	if (ret) {
+		LOG_DEV_ERROR(dev, "failed to set clock %lu\n", freq);
+		return ret;
+	}
+	if (freq < old_freq && rknpu_dev->vdd) {
+		ret = regulator_set_voltage(rknpu_dev->vdd, volt, INT_MAX);
+		if (ret) {
+			LOG_DEV_ERROR(dev, "failed to set volt %lu\n", volt);
+			return ret;
+		}
+	}
+	rknpu_dev->current_freq = freq;
+	rknpu_dev->current_volt = volt;
+
+	return 0;
+}
+
+static int rknpu_devfreq_init(struct rknpu_device *rknpu_dev)
+{
+	struct device *dev = rknpu_dev->dev;
+	struct devfreq_dev_profile *dp = &npu_devfreq_profile;
+	int ret = -EINVAL;
+
+	ret = rockchip_init_opp_table(dev, NULL, "npu_leakage", "rknpu");
+	if (ret) {
+		LOG_DEV_ERROR(dev, "failed to init_opp_table\n");
+		return ret;
+	}
+
+	ret = npu_devfreq_adjust_current_freq_volt(dev, rknpu_dev);
+	if (ret) {
+		LOG_DEV_ERROR(dev, "failed to adjust current freq volt\n");
+		goto err_remove_table;
+	}
+	dp->initial_freq = rknpu_dev->current_freq;
+
+	ret = devfreq_add_governor(&devfreq_rknpu_ondemand);
+	if (ret) {
+		LOG_DEV_ERROR(dev, "failed to add rknpu_ondemand governor\n");
+		goto err_remove_table;
+	}
+
+	rknpu_dev->devfreq = devm_devfreq_add_device(dev, dp, "rknpu_ondemand",
+						     (void *)rknpu_dev);
+	if (IS_ERR(rknpu_dev->devfreq)) {
+		LOG_DEV_ERROR(dev, "failed to add devfreq\n");
+		ret = PTR_ERR(rknpu_dev->devfreq);
+		goto err_remove_governor;
+	}
+	devm_devfreq_register_opp_notifier(dev, rknpu_dev->devfreq);
+
+	rknpu_dev->devfreq->last_status.current_frequency = dp->initial_freq;
+	rknpu_dev->devfreq->last_status.total_time = 1;
+	rknpu_dev->devfreq->last_status.busy_time = 1;
+
+	npu_mdevp.data = rknpu_dev->devfreq;
+	rknpu_dev->mdev_info =
+		rockchip_system_monitor_register(dev, &npu_mdevp);
+	if (IS_ERR(rknpu_dev->mdev_info)) {
+		LOG_DEV_DEBUG(dev, "without system monitor\n");
+		rknpu_dev->mdev_info = NULL;
+	}
+	rknpu_dev->current_freq = clk_get_rate(rknpu_dev->clks[0].clk);
+	rknpu_dev->current_volt = regulator_get_voltage(rknpu_dev->vdd);
+
+	u32 dyn_power_coeff = 0;
+	of_property_read_u32(dev->of_node, "dynamic-power-coefficient",
+			     (u32 *)&dyn_power_coeff);
+	rknpu_dev->model_data =
+		rockchip_ipa_power_model_init(dev, "npu_leakage");
+	if (IS_ERR_OR_NULL(rknpu_dev->model_data)) {
+		rknpu_dev->model_data = NULL;
+		LOG_DEV_ERROR(dev, "failed to initialize power model\n");
+	} else if (rknpu_dev->model_data->dynamic_coefficient) {
+		dyn_power_coeff =
+			rknpu_dev->model_data->dynamic_coefficient;
+	}
+
+	if (!dyn_power_coeff) {
+		LOG_DEV_ERROR(dev, "failed to get dynamic-coefficient\n");
+		goto out;
+	}
+
+	rknpu_dev->devfreq_cooling = of_devfreq_cooling_register_power(
+		dev->of_node, rknpu_dev->devfreq, &npu_cooling_power);
+	if (IS_ERR_OR_NULL(rknpu_dev->devfreq_cooling))
+		LOG_DEV_ERROR(dev, "failed to register cooling device\n");
+
+out:
+	return 0;
+
+err_remove_governor:
+	devfreq_remove_governor(&devfreq_rknpu_ondemand);
+err_remove_table:
+	dev_pm_opp_of_remove_table(dev);
+
+	rknpu_dev->devfreq = NULL;
+
+	return ret;
+}
+#endif
+
+static int rknpu_devfreq_remove(struct rknpu_device *rknpu_dev)
+{
+	if (rknpu_dev->devfreq) {
+		devfreq_unregister_opp_notifier(rknpu_dev->dev,
+						rknpu_dev->devfreq);
+		dev_pm_opp_of_remove_table(rknpu_dev->dev);
+		devfreq_remove_governor(&devfreq_rknpu_ondemand);
+	}
+
+	return 0;
+}
+
+#endif
+
+static int rknpu_register_irq(struct platform_device *pdev,
+			      struct rknpu_device *rknpu_dev)
+{
+	const struct rknpu_config *config = rknpu_dev->config;
+	struct device *dev = &pdev->dev;
+	struct resource *res;
+	int i, ret, irq;
+
+	res = platform_get_resource_byname(pdev, IORESOURCE_IRQ,
+					   config->irqs[0].name);
+	if (res) {
+		/* there are irq names in dts */
+		for (i = 0; i < config->num_irqs; i++) {
+			irq = platform_get_irq_byname(pdev,
+						      config->irqs[i].name);
+			if (irq < 0) {
+				LOG_DEV_ERROR(dev, "no npu %s in dts\n",
+					      config->irqs[i].name);
+				return irq;
+			}
+
+			ret = devm_request_irq(dev, irq,
+					       config->irqs[i].irq_hdl,
+					       IRQF_SHARED, dev_name(dev),
+					       rknpu_dev);
+			if (ret < 0) {
+				LOG_DEV_ERROR(dev, "request %s failed: %d\n",
+					      config->irqs[i].name, ret);
+				return ret;
+			}
+		}
+	} else {
+		/* no irq names in dts */
+		irq = platform_get_irq(pdev, 0);
+		if (irq < 0) {
+			LOG_DEV_ERROR(dev, "no npu irq in dts\n");
+			return irq;
+		}
+
+		ret = devm_request_irq(dev, irq, rknpu_core0_irq_handler,
+				       IRQF_SHARED, dev_name(dev), rknpu_dev);
+		if (ret < 0) {
+			LOG_DEV_ERROR(dev, "request irq failed: %d\n", ret);
+			return ret;
+		}
+	}
+
+	return 0;
+}
+
+static int rknpu_find_sram_resource(struct rknpu_device *rknpu_dev)
+{
+	struct device *dev = rknpu_dev->dev;
+	struct device_node *sram_node = NULL;
+	struct resource sram_res;
+	uint32_t sram_size = 0;
+	int ret = -EINVAL;
+
+	/* get sram device node */
+	sram_node = of_parse_phandle(dev->of_node, "rockchip,sram", 0);
+	rknpu_dev->sram_size = 0;
+	if (!sram_node)
+		return -EINVAL;
+
+	/* get sram start and size */
+	ret = of_address_to_resource(sram_node, 0, &sram_res);
+	of_node_put(sram_node);
+	if (ret)
+		return ret;
+
+	/* check sram start and size is PAGE_SIZE align */
+	rknpu_dev->sram_start = round_up(sram_res.start, PAGE_SIZE);
+	rknpu_dev->sram_end = round_down(
+		sram_res.start + resource_size(&sram_res), PAGE_SIZE);
+	if (rknpu_dev->sram_end <= rknpu_dev->sram_start) {
+		LOG_DEV_WARN(
+			dev,
+			"invalid sram resource, sram start %pa, sram end %pa\n",
+			&rknpu_dev->sram_start, &rknpu_dev->sram_end);
+		return -EINVAL;
+	}
+
+	sram_size = rknpu_dev->sram_end - rknpu_dev->sram_start;
+
+	rknpu_dev->sram_base_io =
+		devm_ioremap(dev, rknpu_dev->sram_start, sram_size);
+	if (IS_ERR(rknpu_dev->sram_base_io)) {
+		LOG_DEV_ERROR(dev, "failed to remap sram base io!\n");
+		rknpu_dev->sram_base_io = NULL;
+	}
+
+	rknpu_dev->sram_size = sram_size;
+
+	LOG_DEV_INFO(dev, "sram region: [%pa, %pa), sram size: %#x\n",
+		     &rknpu_dev->sram_start, &rknpu_dev->sram_end,
+		     rknpu_dev->sram_size);
+
+	return 0;
+}
+
+static int rknpu_probe(struct platform_device *pdev)
+{
+	struct resource *res = NULL;
+	struct rknpu_device *rknpu_dev = NULL;
+	struct device *dev = &pdev->dev;
+	struct device *virt_dev = NULL;
+	const struct of_device_id *match = NULL;
+	const struct rknpu_config *config = NULL;
+	int ret = -EINVAL, i = 0;
+
+	if (!pdev->dev.of_node) {
+		LOG_DEV_ERROR(dev, "rknpu device-tree data is missing!\n");
+		return -ENODEV;
+	}
+
+	match = of_match_device(rknpu_of_match, dev);
+	if (!match) {
+		LOG_DEV_ERROR(dev, "rknpu device-tree entry is missing!\n");
+		return -ENODEV;
+	}
+
+	rknpu_dev = devm_kzalloc(dev, sizeof(*rknpu_dev), GFP_KERNEL);
+	if (!rknpu_dev) {
+		LOG_DEV_ERROR(dev, "failed to allocate rknpu device!\n");
+		return -ENOMEM;
+	}
+
+	config = of_device_get_match_data(dev);
+	if (!config)
+		return -EINVAL;
+
+	rknpu_dev->config = config;
+	rknpu_dev->dev = dev;
+
+	rknpu_dev->iommu_en = rknpu_is_iommu_enable(dev);
+	if (!rknpu_dev->iommu_en) {
+		/* Initialize reserved memory resources */
+		ret = of_reserved_mem_device_init(dev);
+		if (!ret) {
+			LOG_DEV_INFO(
+				dev,
+				"initialize reserved memory for rknpu device!\n");
+		}
+	}
+
+	rknpu_dev->bypass_irq_handler = bypass_irq_handler;
+	rknpu_dev->bypass_soft_reset = bypass_soft_reset;
+
+	rknpu_reset_get(rknpu_dev);
+
+	rknpu_dev->num_clks = devm_clk_bulk_get_all(dev, &rknpu_dev->clks);
+	if (rknpu_dev->num_clks < 1) {
+		LOG_DEV_ERROR(dev, "failed to get clk source for rknpu\n");
+#ifndef FPGA_PLATFORM
+		return -ENODEV;
+#endif
+	}
+
+#ifndef FPGA_PLATFORM
+#if KERNEL_VERSION(5, 10, 0) <= LINUX_VERSION_CODE
+	if (strstr(__clk_get_name(rknpu_dev->clks[0].clk), "scmi"))
+		rknpu_dev->opp_info.scmi_clk = rknpu_dev->clks[0].clk;
+#endif
+
+	rknpu_dev->vdd = devm_regulator_get_optional(dev, "rknpu");
+	if (IS_ERR(rknpu_dev->vdd)) {
+		if (PTR_ERR(rknpu_dev->vdd) != -ENODEV) {
+			ret = PTR_ERR(rknpu_dev->vdd);
+			LOG_DEV_ERROR(
+				dev,
+				"failed to get vdd regulator for rknpu: %d\n",
+				ret);
+			return ret;
+		}
+		rknpu_dev->vdd = NULL;
+	}
+
+	rknpu_dev->mem = devm_regulator_get_optional(dev, "mem");
+	if (IS_ERR(rknpu_dev->mem)) {
+		if (PTR_ERR(rknpu_dev->mem) != -ENODEV) {
+			ret = PTR_ERR(rknpu_dev->mem);
+			LOG_DEV_ERROR(
+				dev,
+				"failed to get mem regulator for rknpu: %d\n",
+				ret);
+			return ret;
+		}
+		rknpu_dev->mem = NULL;
+	}
+#endif
+
+	spin_lock_init(&rknpu_dev->lock);
+	spin_lock_init(&rknpu_dev->irq_lock);
+	mutex_init(&rknpu_dev->power_lock);
+	mutex_init(&rknpu_dev->reset_lock);
+	for (i = 0; i < config->num_irqs; i++) {
+		INIT_LIST_HEAD(&rknpu_dev->subcore_datas[i].todo_list);
+		init_waitqueue_head(&rknpu_dev->subcore_datas[i].job_done_wq);
+		rknpu_dev->subcore_datas[i].task_num = 0;
+		res = platform_get_resource(pdev, IORESOURCE_MEM, i);
+		if (!res) {
+			LOG_DEV_ERROR(
+				dev,
+				"failed to get memory resource for rknpu\n");
+			return -ENXIO;
+		}
+
+		rknpu_dev->base[i] = devm_ioremap_resource(dev, res);
+		if (PTR_ERR(rknpu_dev->base[i]) == -EBUSY) {
+			rknpu_dev->base[i] = devm_ioremap(dev, res->start,
+							  resource_size(res));
+		}
+
+		if (IS_ERR(rknpu_dev->base[i])) {
+			LOG_DEV_ERROR(dev,
+				      "failed to remap register for rknpu\n");
+			return PTR_ERR(rknpu_dev->base[i]);
+		}
+	}
+
+	if (config->bw_priority_length > 0) {
+		rknpu_dev->bw_priority_base =
+			devm_ioremap(dev, config->bw_priority_addr,
+				     config->bw_priority_length);
+		if (IS_ERR(rknpu_dev->bw_priority_base)) {
+			LOG_DEV_ERROR(
+				rknpu_dev->dev,
+				"failed to remap bw priority register for rknpu\n");
+			rknpu_dev->bw_priority_base = NULL;
+		}
+	}
+
+	if (!rknpu_dev->bypass_irq_handler) {
+		ret = rknpu_register_irq(pdev, rknpu_dev);
+		if (ret)
+			return ret;
+	} else {
+		LOG_DEV_WARN(dev, "bypass irq handler!\n");
+	}
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_DRM_GEM
+	ret = rknpu_drm_probe(rknpu_dev);
+	if (ret) {
+		LOG_DEV_ERROR(dev, "failed to probe device for rknpu\n");
+		return ret;
+	}
+#endif
+#ifdef CONFIG_ROCKCHIP_RKNPU_DMA_HEAP
+	rknpu_dev->miscdev.minor = MISC_DYNAMIC_MINOR;
+	rknpu_dev->miscdev.name = "rknpu";
+	rknpu_dev->miscdev.fops = &rknpu_fops;
+
+	ret = misc_register(&rknpu_dev->miscdev);
+	if (ret) {
+		LOG_DEV_ERROR(dev, "cannot register miscdev (%d)\n", ret);
+		return ret;
+	}
+
+	rknpu_dev->heap = rk_dma_heap_find("rk-dma-heap-cma");
+	if (!rknpu_dev->heap) {
+		LOG_DEV_ERROR(dev, "failed to find cma heap\n");
+		return -ENOMEM;
+	}
+	rk_dma_heap_set_dev(dev);
+	LOG_DEV_INFO(dev, "Initialized %s: v%d.%d.%d for %s\n", DRIVER_DESC,
+		     DRIVER_MAJOR, DRIVER_MINOR, DRIVER_PATCHLEVEL,
+		     DRIVER_DATE);
+#endif
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_FENCE
+	ret = rknpu_fence_context_alloc(rknpu_dev);
+	if (ret) {
+		LOG_DEV_ERROR(dev,
+			      "failed to allocate fence context for rknpu\n");
+		goto err_remove_drv;
+	}
+#endif
+
+	platform_set_drvdata(pdev, rknpu_dev);
+
+	pm_runtime_enable(dev);
+
+	if (of_count_phandle_with_args(dev->of_node, "power-domains",
+				       "#power-domain-cells") > 1) {
+		virt_dev = dev_pm_domain_attach_by_name(dev, "npu0");
+		if (!IS_ERR(virt_dev))
+			rknpu_dev->genpd_dev_npu0 = virt_dev;
+		virt_dev = dev_pm_domain_attach_by_name(dev, "npu1");
+		if (!IS_ERR(virt_dev))
+			rknpu_dev->genpd_dev_npu1 = virt_dev;
+		virt_dev = dev_pm_domain_attach_by_name(dev, "npu2");
+		if (!IS_ERR(virt_dev))
+			rknpu_dev->genpd_dev_npu2 = virt_dev;
+		rknpu_dev->multiple_domains = true;
+	}
+
+	ret = rknpu_power_on(rknpu_dev);
+	if (ret)
+		goto err_remove_drv;
+
+#ifndef FPGA_PLATFORM
+	rknpu_devfreq_init(rknpu_dev);
+#endif
+
+	// set default power put delay to 3s
+	rknpu_dev->power_put_delay = 3000;
+	rknpu_dev->power_off_wq =
+		create_freezable_workqueue("rknpu_power_off_wq");
+	if (!rknpu_dev->power_off_wq) {
+		LOG_DEV_ERROR(dev, "rknpu couldn't create power_off workqueue");
+		ret = -ENOMEM;
+		goto err_devfreq_remove;
+	}
+	INIT_DEFERRABLE_WORK(&rknpu_dev->power_off_work,
+			     rknpu_power_off_delay_work);
+
+	if (IS_ENABLED(CONFIG_ROCKCHIP_RKNPU_SRAM) && rknpu_dev->iommu_en) {
+		if (!rknpu_find_sram_resource(rknpu_dev)) {
+			ret = rknpu_mm_create(rknpu_dev->sram_size, PAGE_SIZE,
+					      &rknpu_dev->sram_mm);
+			if (ret != 0)
+				goto err_remove_wq;
+		} else {
+			LOG_DEV_WARN(dev, "could not find sram resource!\n");
+		}
+	}
+
+	rknpu_power_off(rknpu_dev);
+	atomic_set(&rknpu_dev->power_refcount, 0);
+	atomic_set(&rknpu_dev->cmdline_power_refcount, 0);
+
+	rknpu_debugger_init(rknpu_dev);
+	rknpu_init_timer(rknpu_dev);
+
+	return 0;
+
+err_remove_wq:
+	destroy_workqueue(rknpu_dev->power_off_wq);
+
+err_devfreq_remove:
+#ifndef FPGA_PLATFORM
+	rknpu_devfreq_remove(rknpu_dev);
+#endif
+
+err_remove_drv:
+#ifdef CONFIG_ROCKCHIP_RKNPU_DRM_GEM
+	rknpu_drm_remove(rknpu_dev);
+#endif
+#ifdef CONFIG_ROCKCHIP_RKNPU_DMA_HEAP
+	misc_deregister(&(rknpu_dev->miscdev));
+#endif
+
+	return ret;
+}
+
+static int rknpu_remove(struct platform_device *pdev)
+{
+	struct rknpu_device *rknpu_dev = platform_get_drvdata(pdev);
+	int i = 0;
+
+	cancel_delayed_work_sync(&rknpu_dev->power_off_work);
+	destroy_workqueue(rknpu_dev->power_off_wq);
+
+	if (IS_ENABLED(CONFIG_ROCKCHIP_RKNPU_SRAM) && rknpu_dev->sram_mm)
+		rknpu_mm_destroy(rknpu_dev->sram_mm);
+
+	rknpu_debugger_remove(rknpu_dev);
+	rknpu_cancel_timer(rknpu_dev);
+
+	for (i = 0; i < rknpu_dev->config->num_irqs; i++) {
+		WARN_ON(rknpu_dev->subcore_datas[i].job);
+		WARN_ON(!list_empty(&rknpu_dev->subcore_datas[i].todo_list));
+	}
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_DRM_GEM
+	rknpu_drm_remove(rknpu_dev);
+#endif
+#ifdef CONFIG_ROCKCHIP_RKNPU_DMA_HEAP
+	misc_deregister(&(rknpu_dev->miscdev));
+#endif
+
+#ifndef FPGA_PLATFORM
+	rknpu_devfreq_remove(rknpu_dev);
+#endif
+
+	mutex_lock(&rknpu_dev->power_lock);
+	if (atomic_read(&rknpu_dev->power_refcount) > 0)
+		rknpu_power_off(rknpu_dev);
+	mutex_unlock(&rknpu_dev->power_lock);
+
+	if (rknpu_dev->multiple_domains) {
+		if (rknpu_dev->genpd_dev_npu0)
+			dev_pm_domain_detach(rknpu_dev->genpd_dev_npu0, true);
+		if (rknpu_dev->genpd_dev_npu1)
+			dev_pm_domain_detach(rknpu_dev->genpd_dev_npu1, true);
+		if (rknpu_dev->genpd_dev_npu2)
+			dev_pm_domain_detach(rknpu_dev->genpd_dev_npu2, true);
+	}
+
+	pm_runtime_disable(&pdev->dev);
+
+	return 0;
+}
+
+#ifndef FPGA_PLATFORM
+#if KERNEL_VERSION(5, 10, 0) <= LINUX_VERSION_CODE
+static int rknpu_runtime_suspend(struct device *dev)
+{
+	struct rknpu_device *rknpu_dev = dev_get_drvdata(dev);
+	struct rockchip_opp_info *opp_info = &rknpu_dev->opp_info;
+
+	if (opp_info->scmi_clk) {
+		if (clk_set_rate(opp_info->scmi_clk, POWER_DOWN_FREQ))
+			LOG_DEV_ERROR(dev, "failed to restore clk rate\n");
+	}
+	opp_info->current_rm = UINT_MAX;
+
+	return 0;
+}
+
+static int rknpu_runtime_resume(struct device *dev)
+{
+	struct rknpu_device *rknpu_dev = dev_get_drvdata(dev);
+	struct rockchip_opp_info *opp_info = &rknpu_dev->opp_info;
+	int ret = 0;
+
+	if (!rknpu_dev->current_freq || !rknpu_dev->current_volt)
+		return 0;
+
+	ret = clk_bulk_prepare_enable(opp_info->num_clks, opp_info->clks);
+	if (ret) {
+		LOG_DEV_ERROR(dev, "failed to enable opp clks\n");
+		return ret;
+	}
+
+	if (opp_info->data && opp_info->data->set_read_margin)
+		opp_info->data->set_read_margin(dev, opp_info,
+						opp_info->target_rm);
+	if (opp_info->scmi_clk) {
+		if (clk_set_rate(opp_info->scmi_clk, rknpu_dev->current_freq))
+			LOG_DEV_ERROR(dev, "failed to set power down rate\n");
+	}
+
+	clk_bulk_disable_unprepare(opp_info->num_clks, opp_info->clks);
+
+	return ret;
+}
+
+static const struct dev_pm_ops rknpu_pm_ops = {
+	SET_SYSTEM_SLEEP_PM_OPS(pm_runtime_force_suspend,
+				pm_runtime_force_resume)
+		SET_RUNTIME_PM_OPS(rknpu_runtime_suspend, rknpu_runtime_resume,
+				   NULL)
+};
+#endif
+#endif
+
+static struct platform_driver rknpu_driver = {
+	.probe = rknpu_probe,
+	.remove = rknpu_remove,
+	.driver = {
+		.owner = THIS_MODULE,
+		.name = "RKNPU",
+#ifndef FPGA_PLATFORM
+#if KERNEL_VERSION(5, 5, 0) < LINUX_VERSION_CODE
+		.pm = &rknpu_pm_ops,
+#endif
+#endif
+		.of_match_table = of_match_ptr(rknpu_of_match),
+	},
+};
+
+static int rknpu_init(void)
+{
+	return platform_driver_register(&rknpu_driver);
+}
+
+static void rknpu_exit(void)
+{
+	platform_driver_unregister(&rknpu_driver);
+}
+
+late_initcall(rknpu_init);
+module_exit(rknpu_exit);
+
+MODULE_DESCRIPTION("RKNPU driver");
+MODULE_AUTHOR("Felix Zeng <felix.zeng@rock-chips.com>");
+MODULE_ALIAS("rockchip-rknpu");
+MODULE_LICENSE("GPL v2");
+MODULE_VERSION(RKNPU_GET_DRV_VERSION_STRING(DRIVER_MAJOR, DRIVER_MINOR,
+					    DRIVER_PATCHLEVEL));
diff --git a/drivers/rknpu/rknpu_fence.c b/drivers/rknpu/rknpu_fence.c
new file mode 100644
index 000000000..dc22ea1c4
--- /dev/null
+++ b/drivers/rknpu/rknpu_fence.c
@@ -0,0 +1,80 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#include <linux/slab.h>
+#include <linux/file.h>
+#include <linux/dma-fence.h>
+#include <linux/sync_file.h>
+
+#include "rknpu_drv.h"
+#include "rknpu_job.h"
+
+#include "rknpu_fence.h"
+
+static const char *rknpu_fence_get_name(struct dma_fence *fence)
+{
+	return DRIVER_NAME;
+}
+
+static const struct dma_fence_ops rknpu_fence_ops = {
+	.get_driver_name = rknpu_fence_get_name,
+	.get_timeline_name = rknpu_fence_get_name,
+};
+
+int rknpu_fence_context_alloc(struct rknpu_device *rknpu_dev)
+{
+	struct rknpu_fence_context *fence_ctx = NULL;
+
+	fence_ctx =
+		devm_kzalloc(rknpu_dev->dev, sizeof(*fence_ctx), GFP_KERNEL);
+	if (!fence_ctx)
+		return -ENOMEM;
+
+	fence_ctx->context = dma_fence_context_alloc(1);
+	spin_lock_init(&fence_ctx->spinlock);
+
+	rknpu_dev->fence_ctx = fence_ctx;
+
+	return 0;
+}
+
+int rknpu_fence_alloc(struct rknpu_job *job)
+{
+	struct rknpu_fence_context *fence_ctx = job->rknpu_dev->fence_ctx;
+	struct dma_fence *fence = NULL;
+
+	fence = kzalloc(sizeof(*fence), GFP_KERNEL);
+	if (!fence)
+		return -ENOMEM;
+
+	dma_fence_init(fence, &rknpu_fence_ops, &fence_ctx->spinlock,
+		       fence_ctx->context, ++fence_ctx->seqno);
+
+	job->fence = fence;
+
+	return 0;
+}
+
+int rknpu_fence_get_fd(struct rknpu_job *job)
+{
+	struct sync_file *sync_file = NULL;
+	int fence_fd = -1;
+
+	if (!job->fence)
+		return -EINVAL;
+
+	fence_fd = get_unused_fd_flags(O_CLOEXEC);
+	if (fence_fd < 0)
+		return fence_fd;
+
+	sync_file = sync_file_create(job->fence);
+	if (!sync_file)
+		return -ENOMEM;
+
+	fd_install(fence_fd, sync_file->file);
+
+	return fence_fd;
+}
diff --git a/drivers/rknpu/rknpu_gem.c b/drivers/rknpu/rknpu_gem.c
new file mode 100644
index 000000000..01fa4f859
--- /dev/null
+++ b/drivers/rknpu/rknpu_gem.c
@@ -0,0 +1,1321 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#include "linux/mm.h"
+#include <drm/drm_device.h>
+#include <drm/drm_vma_manager.h>
+#include <drm/drm_prime.h>
+#include <drm/drm_file.h>
+#include <drm/drm_drv.h>
+
+#include <linux/shmem_fs.h>
+#include <linux/dma-buf.h>
+#include <linux/iommu.h>
+#include <linux/dma-iommu.h>
+#include <linux/pfn_t.h>
+#include <linux/version.h>
+#include <asm/cacheflush.h>
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_SRAM
+#include "../../arch/arm/mm/dma.h"
+#endif
+
+#if KERNEL_VERSION(5, 10, 0) <= LINUX_VERSION_CODE
+#include <linux/dma-map-ops.h>
+#endif
+
+#include "rknpu_drv.h"
+#include "rknpu_ioctl.h"
+#include "rknpu_gem.h"
+
+#define RKNPU_GEM_ALLOC_FROM_PAGES 1
+
+#if RKNPU_GEM_ALLOC_FROM_PAGES
+static int rknpu_gem_get_pages(struct rknpu_gem_object *rknpu_obj)
+{
+	struct drm_device *drm = rknpu_obj->base.dev;
+	struct scatterlist *s = NULL;
+	dma_addr_t dma_addr = 0;
+	dma_addr_t phys = 0;
+	int ret = -EINVAL, i = 0;
+
+	rknpu_obj->pages = drm_gem_get_pages(&rknpu_obj->base);
+	if (IS_ERR(rknpu_obj->pages)) {
+		ret = PTR_ERR(rknpu_obj->pages);
+		LOG_ERROR("failed to get pages: %d\n", ret);
+		return ret;
+	}
+
+	rknpu_obj->num_pages = rknpu_obj->size >> PAGE_SHIFT;
+
+#if KERNEL_VERSION(5, 10, 0) <= LINUX_VERSION_CODE
+	rknpu_obj->sgt = drm_prime_pages_to_sg(drm, rknpu_obj->pages,
+					       rknpu_obj->num_pages);
+#else
+	rknpu_obj->sgt =
+		drm_prime_pages_to_sg(rknpu_obj->pages, rknpu_obj->num_pages);
+#endif
+	if (IS_ERR(rknpu_obj->sgt)) {
+		ret = PTR_ERR(rknpu_obj->sgt);
+		LOG_ERROR("failed to allocate sgt: %d\n", ret);
+		goto put_pages;
+	}
+
+	ret = dma_map_sg(drm->dev, rknpu_obj->sgt->sgl, rknpu_obj->sgt->nents,
+			 DMA_BIDIRECTIONAL);
+	if (ret == 0) {
+		ret = -EFAULT;
+		LOG_DEV_ERROR(drm->dev, "%s: dma map %zu fail\n", __func__,
+			      rknpu_obj->size);
+		goto free_sgt;
+	}
+
+	if (rknpu_obj->flags & RKNPU_MEM_KERNEL_MAPPING) {
+		rknpu_obj->cookie = vmap(rknpu_obj->pages, rknpu_obj->num_pages,
+					 VM_MAP, PAGE_KERNEL);
+		if (!rknpu_obj->cookie) {
+			ret = -ENOMEM;
+			LOG_ERROR("failed to vmap: %d\n", ret);
+			goto unmap_sg;
+		}
+		rknpu_obj->kv_addr = rknpu_obj->cookie;
+	}
+
+	dma_addr = sg_dma_address(rknpu_obj->sgt->sgl);
+	rknpu_obj->dma_addr = dma_addr;
+
+	for_each_sg(rknpu_obj->sgt->sgl, s, rknpu_obj->sgt->nents, i) {
+		dma_addr += s->length;
+		phys = sg_phys(s);
+		LOG_DEBUG(
+			"gem pages alloc sgt[%d], dma_address: %pad, length: %#x, phys: %pad, virt: %p\n",
+			i, &dma_addr, s->length, &phys, sg_virt(s));
+	}
+
+	return 0;
+
+unmap_sg:
+	dma_unmap_sg(drm->dev, rknpu_obj->sgt->sgl, rknpu_obj->sgt->nents,
+		     DMA_BIDIRECTIONAL);
+
+free_sgt:
+	sg_free_table(rknpu_obj->sgt);
+	kfree(rknpu_obj->sgt);
+
+put_pages:
+	drm_gem_put_pages(&rknpu_obj->base, rknpu_obj->pages, false, false);
+
+	return ret;
+}
+
+static void rknpu_gem_put_pages(struct rknpu_gem_object *rknpu_obj)
+{
+	struct drm_device *drm = rknpu_obj->base.dev;
+
+	if (rknpu_obj->flags & RKNPU_MEM_KERNEL_MAPPING) {
+		vunmap(rknpu_obj->kv_addr);
+		rknpu_obj->kv_addr = NULL;
+	}
+
+	dma_unmap_sg(drm->dev, rknpu_obj->sgt->sgl, rknpu_obj->sgt->nents,
+		     DMA_BIDIRECTIONAL);
+
+	drm_gem_put_pages(&rknpu_obj->base, rknpu_obj->pages, true, true);
+
+	if (rknpu_obj->sgt != NULL) {
+		sg_free_table(rknpu_obj->sgt);
+		kfree(rknpu_obj->sgt);
+	}
+}
+#endif
+
+static int rknpu_gem_alloc_buf(struct rknpu_gem_object *rknpu_obj)
+{
+	struct drm_device *drm = rknpu_obj->base.dev;
+	struct rknpu_device *rknpu_dev = drm->dev_private;
+	unsigned int nr_pages = 0;
+	struct sg_table *sgt = NULL;
+	struct scatterlist *s = NULL;
+	gfp_t gfp_mask = GFP_KERNEL;
+	int ret = -EINVAL, i = 0;
+
+	if (rknpu_obj->dma_addr) {
+		LOG_DEBUG("buffer already allocated.\n");
+		return 0;
+	}
+
+	rknpu_obj->dma_attrs = 0;
+
+	/*
+	 * if RKNPU_MEM_CONTIGUOUS, fully physically contiguous memory
+	 * region will be allocated else physically contiguous
+	 * as possible.
+	 */
+	if (!(rknpu_obj->flags & RKNPU_MEM_NON_CONTIGUOUS))
+		rknpu_obj->dma_attrs |= DMA_ATTR_FORCE_CONTIGUOUS;
+
+	// cacheable mapping or writecombine mapping
+	if (rknpu_obj->flags & RKNPU_MEM_CACHEABLE) {
+#ifdef DMA_ATTR_NON_CONSISTENT
+		rknpu_obj->dma_attrs |= DMA_ATTR_NON_CONSISTENT;
+#endif
+#ifdef DMA_ATTR_SYS_CACHE_ONLY
+		rknpu_obj->dma_attrs |= DMA_ATTR_SYS_CACHE_ONLY;
+#endif
+	} else if (rknpu_obj->flags & RKNPU_MEM_WRITE_COMBINE) {
+		rknpu_obj->dma_attrs |= DMA_ATTR_WRITE_COMBINE;
+	}
+
+	if (!(rknpu_obj->flags & RKNPU_MEM_KERNEL_MAPPING))
+		rknpu_obj->dma_attrs |= DMA_ATTR_NO_KERNEL_MAPPING;
+
+#ifdef DMA_ATTR_SKIP_ZEROING
+	if (!(rknpu_obj->flags & RKNPU_MEM_ZEROING))
+		rknpu_obj->dma_attrs |= DMA_ATTR_SKIP_ZEROING;
+#endif
+
+#if RKNPU_GEM_ALLOC_FROM_PAGES
+	if ((rknpu_obj->flags & RKNPU_MEM_NON_CONTIGUOUS) &&
+	    rknpu_dev->iommu_en) {
+		return rknpu_gem_get_pages(rknpu_obj);
+	}
+#endif
+
+	if (rknpu_obj->flags & RKNPU_MEM_ZEROING)
+		gfp_mask |= __GFP_ZERO;
+
+	if (!(rknpu_obj->flags & RKNPU_MEM_NON_DMA32)) {
+		gfp_mask &= ~__GFP_HIGHMEM;
+		gfp_mask |= __GFP_DMA32;
+	}
+
+	nr_pages = rknpu_obj->size >> PAGE_SHIFT;
+
+	rknpu_obj->pages = rknpu_gem_alloc_page(nr_pages);
+	if (!rknpu_obj->pages) {
+		LOG_ERROR("failed to allocate pages.\n");
+		return -ENOMEM;
+	}
+
+	rknpu_obj->cookie =
+		dma_alloc_attrs(drm->dev, rknpu_obj->size, &rknpu_obj->dma_addr,
+				gfp_mask, rknpu_obj->dma_attrs);
+	if (!rknpu_obj->cookie) {
+		/*
+		 * when RKNPU_MEM_CONTIGUOUS and IOMMU is available
+		 * try to fallback to allocate non-contiguous buffer
+		 */
+		if (!(rknpu_obj->flags & RKNPU_MEM_NON_CONTIGUOUS) &&
+		    rknpu_dev->iommu_en) {
+			LOG_DEV_WARN(
+				drm->dev,
+				"try to fallback to allocate non-contiguous %lu buffer.\n",
+				rknpu_obj->size);
+			rknpu_obj->dma_attrs &= ~DMA_ATTR_FORCE_CONTIGUOUS;
+			rknpu_obj->flags |= RKNPU_MEM_NON_CONTIGUOUS;
+			rknpu_obj->cookie =
+				dma_alloc_attrs(drm->dev, rknpu_obj->size,
+						&rknpu_obj->dma_addr, gfp_mask,
+						rknpu_obj->dma_attrs);
+			if (!rknpu_obj->cookie) {
+				LOG_DEV_ERROR(
+					drm->dev,
+					"failed to allocate non-contiguous %lu buffer.\n",
+					rknpu_obj->size);
+				goto err_free;
+			}
+		} else {
+			LOG_DEV_ERROR(drm->dev,
+				      "failed to allocate %lu buffer.\n",
+				      rknpu_obj->size);
+			goto err_free;
+		}
+	}
+
+	if (rknpu_obj->flags & RKNPU_MEM_KERNEL_MAPPING)
+		rknpu_obj->kv_addr = rknpu_obj->cookie;
+
+	sgt = kzalloc(sizeof(*sgt), GFP_KERNEL);
+	if (!sgt) {
+		ret = -ENOMEM;
+		goto err_free_dma;
+	}
+
+	ret = dma_get_sgtable_attrs(drm->dev, sgt, rknpu_obj->cookie,
+				    rknpu_obj->dma_addr, rknpu_obj->size,
+				    rknpu_obj->dma_attrs);
+	if (ret < 0) {
+		LOG_DEV_ERROR(drm->dev, "failed to get sgtable.\n");
+		goto err_free_sgt;
+	}
+
+	for_each_sg(sgt->sgl, s, sgt->nents, i) {
+		sg_dma_address(s) = sg_phys(s);
+		LOG_DEBUG("dma alloc sgt[%d], phys_address: %pad, length: %u\n",
+			  i, &s->dma_address, s->length);
+	}
+
+	if (drm_prime_sg_to_page_array(sgt, rknpu_obj->pages, nr_pages)) {
+		LOG_DEV_ERROR(drm->dev, "invalid sgtable.\n");
+		ret = -EINVAL;
+		goto err_free_sg_table;
+	}
+
+	rknpu_obj->sgt = sgt;
+
+	return ret;
+
+err_free_sg_table:
+	sg_free_table(sgt);
+err_free_sgt:
+	kfree(sgt);
+err_free_dma:
+	dma_free_attrs(drm->dev, rknpu_obj->size, rknpu_obj->cookie,
+		       rknpu_obj->dma_addr, rknpu_obj->dma_attrs);
+err_free:
+	rknpu_gem_free_page(rknpu_obj->pages);
+
+	return ret;
+}
+
+static void rknpu_gem_free_buf(struct rknpu_gem_object *rknpu_obj)
+{
+	struct drm_device *drm = rknpu_obj->base.dev;
+#if RKNPU_GEM_ALLOC_FROM_PAGES
+	struct rknpu_device *rknpu_dev = drm->dev_private;
+#endif
+
+	if (!rknpu_obj->dma_addr) {
+		LOG_DEBUG("dma handle is invalid.\n");
+		return;
+	}
+
+#if RKNPU_GEM_ALLOC_FROM_PAGES
+	if ((rknpu_obj->flags & RKNPU_MEM_NON_CONTIGUOUS) &&
+	    rknpu_dev->iommu_en) {
+		rknpu_gem_put_pages(rknpu_obj);
+		return;
+	}
+#endif
+
+	sg_free_table(rknpu_obj->sgt);
+	kfree(rknpu_obj->sgt);
+
+	dma_free_attrs(drm->dev, rknpu_obj->size, rknpu_obj->cookie,
+		       rknpu_obj->dma_addr, rknpu_obj->dma_attrs);
+
+	rknpu_gem_free_page(rknpu_obj->pages);
+
+	rknpu_obj->dma_addr = 0;
+}
+
+static int rknpu_gem_handle_create(struct drm_gem_object *obj,
+				   struct drm_file *file_priv,
+				   unsigned int *handle)
+{
+	int ret = -EINVAL;
+	/*
+	 * allocate a id of idr table where the obj is registered
+	 * and handle has the id what user can see.
+	 */
+	ret = drm_gem_handle_create(file_priv, obj, handle);
+	if (ret)
+		return ret;
+
+	LOG_DEBUG("gem handle: %#x\n", *handle);
+
+	/* drop reference from allocate - handle holds it now. */
+	rknpu_gem_object_put(obj);
+
+	return 0;
+}
+
+static int rknpu_gem_handle_destroy(struct drm_file *file_priv,
+				    unsigned int handle)
+{
+	return drm_gem_handle_delete(file_priv, handle);
+}
+
+static const struct vm_operations_struct rknpu_gem_vm_ops = {
+	.fault = rknpu_gem_fault,
+	.open = drm_gem_vm_open,
+	.close = drm_gem_vm_close,
+};
+
+static struct drm_gem_object_funcs rknpu_gem_object_funcs = {
+	.free = rknpu_gem_free_object,
+	.vm_ops = &rknpu_gem_vm_ops,
+	.get_sg_table = rknpu_gem_prime_get_sg_table,
+	.vmap = rknpu_gem_prime_vmap,
+	.vunmap = rknpu_gem_prime_vunmap,
+	// will cause recursive call
+	// .mmap = rknpu_gem_prime_mmap,
+};
+
+
+static struct rknpu_gem_object *rknpu_gem_init(struct drm_device *drm,
+					       unsigned long size)
+{
+	struct rknpu_gem_object *rknpu_obj = NULL;
+	struct drm_gem_object *obj = NULL;
+	gfp_t gfp_mask;
+	int ret = -EINVAL;
+
+	rknpu_obj = kzalloc(sizeof(*rknpu_obj), GFP_KERNEL);
+	if (!rknpu_obj)
+		return ERR_PTR(-ENOMEM);
+
+	obj = &rknpu_obj->base;
+
+	ret = drm_gem_object_init(drm, obj, size);
+	if (ret < 0) {
+		LOG_DEV_ERROR(drm->dev, "failed to initialize gem object\n");
+		kfree(rknpu_obj);
+		return ERR_PTR(ret);
+	}
+
+	obj->funcs = &rknpu_gem_object_funcs;
+
+	rknpu_obj->size = rknpu_obj->base.size;
+
+	gfp_mask = mapping_gfp_mask(obj->filp->f_mapping);
+
+	if (rknpu_obj->flags & RKNPU_MEM_ZEROING)
+		gfp_mask |= __GFP_ZERO;
+
+	if (!(rknpu_obj->flags & RKNPU_MEM_NON_DMA32)) {
+		gfp_mask &= ~__GFP_HIGHMEM;
+		gfp_mask |= __GFP_DMA32;
+	}
+
+	mapping_set_gfp_mask(obj->filp->f_mapping, gfp_mask);
+
+	return rknpu_obj;
+}
+
+static void rknpu_gem_release(struct rknpu_gem_object *rknpu_obj)
+{
+	/* release file pointer to gem object. */
+	drm_gem_object_release(&rknpu_obj->base);
+	kfree(rknpu_obj);
+}
+
+static int rknpu_gem_alloc_buf_with_sram(struct rknpu_gem_object *rknpu_obj)
+{
+	struct drm_device *drm = rknpu_obj->base.dev;
+	struct rknpu_device *rknpu_dev = drm->dev_private;
+	struct iommu_domain *domain = NULL;
+	struct rknpu_iommu_dma_cookie *cookie = NULL;
+	struct iova_domain *iovad = NULL;
+	struct scatterlist *s = NULL;
+	unsigned long length = 0;
+	unsigned long size = 0;
+	unsigned long offset = 0;
+	int i = 0;
+	int ret = -EINVAL;
+
+	/* iova map to sram */
+	domain = iommu_get_domain_for_dev(rknpu_dev->dev);
+	if (!domain) {
+		LOG_ERROR("failed to get iommu domain!");
+		return -EINVAL;
+	}
+
+	cookie = (struct rknpu_iommu_dma_cookie *)domain->iova_cookie;
+	iovad = &cookie->iovad;
+	rknpu_obj->iova_size =
+		iova_align(iovad, rknpu_obj->sram_size + rknpu_obj->size);
+	rknpu_obj->iova_start = rknpu_iommu_dma_alloc_iova(
+		domain, rknpu_obj->iova_size, dma_get_mask(drm->dev), drm->dev);
+	if (!rknpu_obj->iova_start) {
+		LOG_ERROR("iommu_dma_alloc_iova failed\n");
+		return -ENOMEM;
+	}
+
+	LOG_INFO("allocate iova start: %pad, size: %lu\n",
+		 &rknpu_obj->iova_start, rknpu_obj->iova_size);
+
+	/*
+	 * Overview SRAM + DDR map to IOVA
+	 * --------
+	 * sram_size: rknpu_obj->sram_size
+	 *   - allocate from SRAM, this size value has been page-aligned
+	 * size: rknpu_obj->size
+	 *   - allocate from DDR pages, this size value has been page-aligned
+	 * iova_size: rknpu_obj->iova_size
+	 *   - from iova_align(sram_size + size)
+	 *   - it may be larger than the (sram_size + size), and the larger part is not mapped
+	 * --------
+	 *
+	 * |<- sram_size ->|      |<- - - - size - - - ->|
+	 * +---------------+      +----------------------+
+	 * |     SRAM      |      |         DDR          |
+	 * +---------------+      +----------------------+
+	 *         |                    |
+	 * |       V       |            V          |
+	 * +---------------------------------------+
+	 * |             IOVA range                |
+	 * +---------------------------------------+
+	 * |<- - - - - - - iova_size - - - - - - ->|
+	 *
+	 */
+	offset = rknpu_obj->sram_obj->range_start *
+		 rknpu_dev->sram_mm->chunk_size;
+	ret = iommu_map(domain, rknpu_obj->iova_start,
+			rknpu_dev->sram_start + offset, rknpu_obj->sram_size,
+			IOMMU_READ | IOMMU_WRITE, GFP_KERNEL);
+	if (ret) {
+		LOG_ERROR("sram iommu_map error: %d\n", ret);
+		goto free_iova;
+	}
+
+	rknpu_obj->dma_addr = rknpu_obj->iova_start;
+
+	if (rknpu_obj->size == 0) {
+		LOG_INFO("allocate sram size: %lu\n", rknpu_obj->sram_size);
+		return 0;
+	}
+
+	rknpu_obj->pages = drm_gem_get_pages(&rknpu_obj->base);
+	if (IS_ERR(rknpu_obj->pages)) {
+		ret = PTR_ERR(rknpu_obj->pages);
+		LOG_ERROR("failed to get pages: %d\n", ret);
+		goto sram_unmap;
+	}
+
+	rknpu_obj->num_pages = rknpu_obj->size >> PAGE_SHIFT;
+
+#if KERNEL_VERSION(5, 10, 0) <= LINUX_VERSION_CODE
+	rknpu_obj->sgt = drm_prime_pages_to_sg(drm, rknpu_obj->pages,
+					       rknpu_obj->num_pages);
+#else
+	rknpu_obj->sgt =
+		drm_prime_pages_to_sg(rknpu_obj->pages, rknpu_obj->num_pages);
+#endif
+	if (IS_ERR(rknpu_obj->sgt)) {
+		ret = PTR_ERR(rknpu_obj->sgt);
+		LOG_ERROR("failed to allocate sgt: %d\n", ret);
+		goto put_pages;
+	}
+
+	length = rknpu_obj->size;
+	offset = rknpu_obj->iova_start + rknpu_obj->sram_size;
+
+	for_each_sg(rknpu_obj->sgt->sgl, s, rknpu_obj->sgt->nents, i) {
+		size = (length < s->length) ? length : s->length;
+
+		ret = iommu_map(domain, offset, sg_phys(s), size,
+				IOMMU_READ | IOMMU_WRITE, GFP_KERNEL);
+		if (ret) {
+			LOG_ERROR("ddr iommu_map error: %d\n", ret);
+			goto sgl_unmap;
+		}
+
+		length -= size;
+		offset += size;
+
+		if (length == 0)
+			break;
+	}
+
+	LOG_INFO("allocate size: %lu with sram size: %lu\n", rknpu_obj->size,
+		 rknpu_obj->sram_size);
+
+	return 0;
+
+sgl_unmap:
+	iommu_unmap(domain, rknpu_obj->iova_start + rknpu_obj->sram_size,
+		    rknpu_obj->size - length);
+	sg_free_table(rknpu_obj->sgt);
+	kfree(rknpu_obj->sgt);
+
+put_pages:
+	drm_gem_put_pages(&rknpu_obj->base, rknpu_obj->pages, false, false);
+
+sram_unmap:
+	iommu_unmap(domain, rknpu_obj->iova_start, rknpu_obj->sram_size);
+
+free_iova:
+	rknpu_iommu_dma_free_iova((struct rknpu_iommu_dma_cookie *)domain->iova_cookie, rknpu_obj->iova_start,
+				  rknpu_obj->iova_size);
+
+	return ret;
+}
+
+static void rknpu_gem_free_buf_with_sram(struct rknpu_gem_object *rknpu_obj)
+{
+	struct drm_device *drm = rknpu_obj->base.dev;
+	struct rknpu_device *rknpu_dev = drm->dev_private;
+	struct iommu_domain *domain = NULL;
+
+	domain = iommu_get_domain_for_dev(rknpu_dev->dev);
+	if (domain) {
+		iommu_unmap(domain, rknpu_obj->iova_start,
+			    rknpu_obj->sram_size);
+		if (rknpu_obj->size > 0)
+			iommu_unmap(domain,
+				    rknpu_obj->iova_start +
+					    rknpu_obj->sram_size,
+				    rknpu_obj->size);
+		rknpu_iommu_dma_free_iova((struct rknpu_iommu_dma_cookie *)domain->iova_cookie,
+					  rknpu_obj->iova_start,
+					  rknpu_obj->iova_size);
+	}
+
+	if (rknpu_obj->pages)
+		drm_gem_put_pages(&rknpu_obj->base, rknpu_obj->pages, true,
+				  true);
+
+	if (rknpu_obj->sgt != NULL) {
+		sg_free_table(rknpu_obj->sgt);
+		kfree(rknpu_obj->sgt);
+	}
+}
+
+struct rknpu_gem_object *rknpu_gem_object_create(struct drm_device *drm,
+						 unsigned int flags,
+						 unsigned long size,
+						 unsigned long sram_size)
+{
+	struct rknpu_device *rknpu_dev = drm->dev_private;
+	struct rknpu_gem_object *rknpu_obj = NULL;
+	size_t remain_ddr_size = 0;
+	int ret = -EINVAL;
+
+	if (!size) {
+		LOG_DEV_ERROR(drm->dev, "invalid buffer size: %lu\n", size);
+		return ERR_PTR(-EINVAL);
+	}
+
+	remain_ddr_size = round_up(size, PAGE_SIZE);
+
+	if (!rknpu_dev->iommu_en && (flags & RKNPU_MEM_NON_CONTIGUOUS)) {
+		/*
+		 * when no IOMMU is available, all allocated buffers are
+		 * contiguous anyway, so drop RKNPU_MEM_NON_CONTIGUOUS flag
+		 */
+		flags &= ~RKNPU_MEM_NON_CONTIGUOUS;
+		LOG_WARN(
+			"non-contiguous allocation is not supported without IOMMU, falling back to contiguous buffer\n");
+	}
+
+	if (IS_ENABLED(CONFIG_ROCKCHIP_RKNPU_SRAM) &&
+	    (flags & RKNPU_MEM_TRY_ALLOC_SRAM) && rknpu_dev->sram_size > 0) {
+		size_t sram_free_size = 0;
+		size_t real_sram_size = 0;
+
+		if (sram_size != 0)
+			sram_size = round_up(sram_size, PAGE_SIZE);
+
+		rknpu_obj = rknpu_gem_init(drm, remain_ddr_size);
+		if (IS_ERR(rknpu_obj))
+			return rknpu_obj;
+
+		/* set memory type and cache attribute from user side. */
+		rknpu_obj->flags = flags;
+
+		sram_free_size = rknpu_dev->sram_mm->free_chunks *
+				 rknpu_dev->sram_mm->chunk_size;
+		if (sram_free_size > 0) {
+			real_sram_size = remain_ddr_size;
+			if (sram_size != 0 && remain_ddr_size > sram_size)
+				real_sram_size = sram_size;
+			if (real_sram_size > sram_free_size)
+				real_sram_size = sram_free_size;
+			ret = rknpu_mm_alloc(rknpu_dev->sram_mm, real_sram_size,
+					     &rknpu_obj->sram_obj);
+			if (ret != 0) {
+				sram_free_size =
+					rknpu_dev->sram_mm->free_chunks *
+					rknpu_dev->sram_mm->chunk_size;
+				LOG_WARN(
+					"mm allocate %zu failed, ret: %d, free size: %zu\n",
+					real_sram_size, ret, sram_free_size);
+				real_sram_size = 0;
+			}
+		}
+
+		if (real_sram_size > 0) {
+			rknpu_obj->sram_size = real_sram_size;
+
+			ret = rknpu_gem_alloc_buf_with_sram(rknpu_obj);
+			if (ret < 0)
+				goto mm_free;
+			remain_ddr_size = 0;
+		}
+	}
+
+	if (remain_ddr_size > 0) {
+		rknpu_obj = rknpu_gem_init(drm, remain_ddr_size);
+		if (IS_ERR(rknpu_obj))
+			return rknpu_obj;
+
+		/* set memory type and cache attribute from user side. */
+		rknpu_obj->flags = flags;
+
+		ret = rknpu_gem_alloc_buf(rknpu_obj);
+		if (ret < 0)
+			goto gem_release;
+	}
+
+	if (rknpu_obj)
+		LOG_DEBUG(
+			"created dma addr: %pad, cookie: %p, ddr size: %lu, sram size: %lu, attrs: %#lx, flags: %#x\n",
+			&rknpu_obj->dma_addr, rknpu_obj->cookie, rknpu_obj->size,
+			rknpu_obj->sram_size, rknpu_obj->dma_attrs, rknpu_obj->flags);
+
+	return rknpu_obj;
+
+mm_free:
+	if (IS_ENABLED(CONFIG_ROCKCHIP_RKNPU_SRAM) &&
+	    rknpu_obj->sram_obj != NULL)
+		rknpu_mm_free(rknpu_dev->sram_mm, rknpu_obj->sram_obj);
+
+gem_release:
+	rknpu_gem_release(rknpu_obj);
+
+	return ERR_PTR(ret);
+}
+
+void rknpu_gem_object_destroy(struct rknpu_gem_object *rknpu_obj)
+{
+	struct drm_gem_object *obj = &rknpu_obj->base;
+
+	LOG_DEBUG(
+		"destroy dma addr: %pad, cookie: %p, size: %lu, attrs: %#lx, flags: %#x, handle count: %d\n",
+		&rknpu_obj->dma_addr, rknpu_obj->cookie, rknpu_obj->size,
+		rknpu_obj->dma_attrs, rknpu_obj->flags, obj->handle_count);
+
+	/*
+	 * do not release memory region from exporter.
+	 *
+	 * the region will be released by exporter
+	 * once dmabuf's refcount becomes 0.
+	 */
+	if (obj->import_attach) {
+		drm_prime_gem_destroy(obj, rknpu_obj->sgt);
+		rknpu_gem_free_page(rknpu_obj->pages);
+	} else {
+		if (IS_ENABLED(CONFIG_ROCKCHIP_RKNPU_SRAM) &&
+		    rknpu_obj->sram_size > 0) {
+			struct rknpu_device *rknpu_dev = obj->dev->dev_private;
+
+			if (rknpu_obj->sram_obj != NULL)
+				rknpu_mm_free(rknpu_dev->sram_mm,
+					      rknpu_obj->sram_obj);
+			rknpu_gem_free_buf_with_sram(rknpu_obj);
+		} else {
+			rknpu_gem_free_buf(rknpu_obj);
+		}
+	}
+
+	rknpu_gem_release(rknpu_obj);
+}
+
+int rknpu_gem_create_ioctl(struct drm_device *dev, void *data,
+			   struct drm_file *file_priv)
+{
+	struct rknpu_mem_create *args = data;
+	struct rknpu_gem_object *rknpu_obj = NULL;
+	int ret = -EINVAL;
+
+	rknpu_obj = rknpu_gem_object_find(file_priv, args->handle);
+	if (!rknpu_obj) {
+		rknpu_obj = rknpu_gem_object_create(
+			dev, args->flags, args->size, args->sram_size);
+		if (IS_ERR(rknpu_obj))
+			return PTR_ERR(rknpu_obj);
+
+		ret = rknpu_gem_handle_create(&rknpu_obj->base, file_priv,
+					      &args->handle);
+		if (ret) {
+			rknpu_gem_object_destroy(rknpu_obj);
+			return ret;
+		}
+	}
+
+	// rknpu_gem_object_get(&rknpu_obj->base);
+
+	args->size = rknpu_obj->size;
+	args->sram_size = rknpu_obj->sram_size;
+	args->obj_addr = (__u64)(uintptr_t)rknpu_obj;
+	args->dma_addr = rknpu_obj->dma_addr;
+
+	return 0;
+}
+
+int rknpu_gem_map_ioctl(struct drm_device *dev, void *data,
+			struct drm_file *file_priv)
+{
+	struct rknpu_mem_map *args = data;
+
+#if KERNEL_VERSION(4, 19, 0) > LINUX_VERSION_CODE
+	return rknpu_gem_dumb_map_offset(file_priv, dev, args->handle,
+					 &args->offset);
+#else
+	return drm_gem_dumb_map_offset(file_priv, dev, args->handle,
+				       &args->offset);
+#endif
+}
+
+int rknpu_gem_destroy_ioctl(struct drm_device *dev, void *data,
+			    struct drm_file *file_priv)
+{
+	struct rknpu_gem_object *rknpu_obj = NULL;
+	struct rknpu_mem_destroy *args = data;
+
+	rknpu_obj = rknpu_gem_object_find(file_priv, args->handle);
+	if (!rknpu_obj)
+		return -EINVAL;
+
+	// rknpu_gem_object_put(&rknpu_obj->base);
+
+	return rknpu_gem_handle_destroy(file_priv, args->handle);
+}
+
+#if RKNPU_GEM_ALLOC_FROM_PAGES
+/*
+ * __vm_map_pages - maps range of kernel pages into user vma
+ * @vma: user vma to map to
+ * @pages: pointer to array of source kernel pages
+ * @num: number of pages in page array
+ * @offset: user's requested vm_pgoff
+ *
+ * This allows drivers to map range of kernel pages into a user vma.
+ *
+ * Return: 0 on success and error code otherwise.
+ */
+static int __vm_map_pages(struct vm_area_struct *vma, struct page **pages,
+			  unsigned long num, unsigned long offset)
+{
+	unsigned long count = vma_pages(vma);
+	unsigned long uaddr = vma->vm_start;
+	int ret = -EINVAL, i = 0;
+
+	/* Fail if the user requested offset is beyond the end of the object */
+	if (offset >= num)
+		return -ENXIO;
+
+	/* Fail if the user requested size exceeds available object size */
+	if (count > num - offset)
+		return -ENXIO;
+
+	for (i = 0; i < count; i++) {
+		ret = vm_insert_page(vma, uaddr, pages[offset + i]);
+		if (ret < 0)
+			return ret;
+		uaddr += PAGE_SIZE;
+	}
+
+	return 0;
+}
+
+static int rknpu_gem_mmap_pages(struct rknpu_gem_object *rknpu_obj,
+				struct vm_area_struct *vma)
+{
+	struct drm_device *drm = rknpu_obj->base.dev;
+	int ret = -EINVAL;
+
+	vm_flags_set(vma, VM_MIXEDMAP);
+
+	ret = __vm_map_pages(vma, rknpu_obj->pages, rknpu_obj->num_pages,
+			     vma->vm_pgoff);
+	if (ret < 0)
+		LOG_DEV_ERROR(drm->dev, "failed to map pages into vma: %d\n",
+			      ret);
+
+	return ret;
+}
+#endif
+
+static int rknpu_gem_mmap_buffer(struct rknpu_gem_object *rknpu_obj,
+				 struct vm_area_struct *vma)
+{
+	struct drm_device *drm = rknpu_obj->base.dev;
+#if RKNPU_GEM_ALLOC_FROM_PAGES
+	struct rknpu_device *rknpu_dev = drm->dev_private;
+#endif
+	unsigned long vm_size = 0;
+	int ret = -EINVAL;
+
+	/*
+	 * clear the VM_PFNMAP flag that was set by drm_gem_mmap(), and set the
+	 * vm_pgoff (used as a fake buffer offset by DRM) to 0 as we want to map
+	 * the whole buffer.
+	 */
+	vm_flags_clear(vma, VM_MIXEDMAP);
+	vma->vm_pgoff = 0;
+
+	vm_size = vma->vm_end - vma->vm_start;
+
+	/* check if user-requested size is valid. */
+	if (vm_size > rknpu_obj->size)
+		return -EINVAL;
+
+	if (rknpu_obj->sram_size > 0) {
+		unsigned long offset = 0;
+		unsigned long num_pages = 0;
+		int i = 0;
+
+		vm_flags_set(vma, VM_MIXEDMAP);
+
+		offset = rknpu_obj->sram_obj->range_start *
+			 rknpu_dev->sram_mm->chunk_size;
+		vma->vm_pgoff = __phys_to_pfn(rknpu_dev->sram_start + offset);
+
+		ret = remap_pfn_range(vma, vma->vm_start, vma->vm_pgoff,
+				      rknpu_obj->sram_size, vma->vm_page_prot);
+		if (ret)
+			return -EAGAIN;
+
+		if (rknpu_obj->size == 0)
+			return 0;
+
+		offset = rknpu_obj->sram_size;
+
+		num_pages = (vm_size - rknpu_obj->sram_size) / PAGE_SIZE;
+		for (i = 0; i < num_pages; ++i) {
+			ret = vm_insert_page(vma, vma->vm_start + offset,
+					     rknpu_obj->pages[i]);
+			if (ret < 0)
+				return ret;
+			offset += PAGE_SIZE;
+		}
+
+		return 0;
+	}
+
+#if RKNPU_GEM_ALLOC_FROM_PAGES
+	if ((rknpu_obj->flags & RKNPU_MEM_NON_CONTIGUOUS) &&
+	    rknpu_dev->iommu_en) {
+		return rknpu_gem_mmap_pages(rknpu_obj, vma);
+	}
+#endif
+
+	ret = dma_mmap_attrs(drm->dev, vma, rknpu_obj->cookie,
+			     rknpu_obj->dma_addr, rknpu_obj->size,
+			     rknpu_obj->dma_attrs);
+	if (ret < 0) {
+		LOG_DEV_ERROR(drm->dev, "failed to mmap, ret: %d\n", ret);
+		return ret;
+	}
+
+	return 0;
+}
+
+void rknpu_gem_free_object(struct drm_gem_object *obj)
+{
+	rknpu_gem_object_destroy(to_rknpu_obj(obj));
+}
+
+int rknpu_gem_dumb_create(struct drm_file *file_priv, struct drm_device *drm,
+			  struct drm_mode_create_dumb *args)
+{
+	struct rknpu_device *rknpu_dev = drm->dev_private;
+	struct rknpu_gem_object *rknpu_obj = NULL;
+	unsigned int flags = 0;
+	int ret = -EINVAL;
+
+	/*
+	 * allocate memory to be used for framebuffer.
+	 * - this callback would be called by user application
+	 *	with DRM_IOCTL_MODE_CREATE_DUMB command.
+	 */
+	args->pitch = args->width * ((args->bpp + 7) / 8);
+	args->size = args->pitch * args->height;
+
+	if (rknpu_dev->iommu_en)
+		flags = RKNPU_MEM_NON_CONTIGUOUS | RKNPU_MEM_WRITE_COMBINE;
+	else
+		flags = RKNPU_MEM_CONTIGUOUS | RKNPU_MEM_WRITE_COMBINE;
+
+	rknpu_obj = rknpu_gem_object_create(drm, flags, args->size, 0);
+	if (IS_ERR(rknpu_obj)) {
+		LOG_DEV_ERROR(drm->dev, "gem object allocate failed.\n");
+		return PTR_ERR(rknpu_obj);
+	}
+
+	ret = rknpu_gem_handle_create(&rknpu_obj->base, file_priv,
+				      &args->handle);
+	if (ret) {
+		rknpu_gem_object_destroy(rknpu_obj);
+		return ret;
+	}
+
+	return 0;
+}
+
+#if KERNEL_VERSION(4, 19, 0) > LINUX_VERSION_CODE
+int rknpu_gem_dumb_map_offset(struct drm_file *file_priv,
+			      struct drm_device *drm, uint32_t handle,
+			      uint64_t *offset)
+{
+	struct rknpu_gem_object *rknpu_obj = NULL;
+	struct drm_gem_object *obj = NULL;
+	int ret = -EINVAL;
+
+	rknpu_obj = rknpu_gem_object_find(file_priv, handle);
+	if (!rknpu_obj)
+		return 0;
+
+	/* Don't allow imported objects to be mapped */
+	obj = &rknpu_obj->base;
+	if (obj->import_attach)
+		return -EINVAL;
+
+	ret = drm_gem_create_mmap_offset(obj);
+	if (ret)
+		return ret;
+
+	*offset = drm_vma_node_offset_addr(&obj->vma_node);
+
+	return 0;
+}
+#endif
+
+#if KERNEL_VERSION(4, 15, 0) <= LINUX_VERSION_CODE
+vm_fault_t rknpu_gem_fault(struct vm_fault *vmf)
+{
+	struct vm_area_struct *vma = vmf->vma;
+	struct drm_gem_object *obj = vma->vm_private_data;
+	struct rknpu_gem_object *rknpu_obj = to_rknpu_obj(obj);
+	struct drm_device *drm = rknpu_obj->base.dev;
+	unsigned long pfn = 0;
+	pgoff_t page_offset = 0;
+
+	page_offset = (vmf->address - vma->vm_start) >> PAGE_SHIFT;
+
+	if (page_offset >= (rknpu_obj->size >> PAGE_SHIFT)) {
+		LOG_DEV_ERROR(drm->dev, "invalid page offset\n");
+		return VM_FAULT_SIGBUS;
+	}
+
+	pfn = page_to_pfn(rknpu_obj->pages[page_offset]);
+	return vmf_insert_mixed(vma, vmf->address,
+				__pfn_to_pfn_t(pfn, PFN_DEV));
+}
+#elif KERNEL_VERSION(4, 14, 0) <= LINUX_VERSION_CODE
+int rknpu_gem_fault(struct vm_fault *vmf)
+{
+	struct vm_area_struct *vma = vmf->vma;
+	struct drm_gem_object *obj = vma->vm_private_data;
+	struct rknpu_gem_object *rknpu_obj = to_rknpu_obj(obj);
+	struct drm_device *drm = rknpu_obj->base.dev;
+	unsigned long pfn = 0;
+	pgoff_t page_offset = 0;
+	int ret = -EINVAL;
+
+	page_offset = (vmf->address - vma->vm_start) >> PAGE_SHIFT;
+
+	if (page_offset >= (rknpu_obj->size >> PAGE_SHIFT)) {
+		LOG_DEV_ERROR(drm->dev, "invalid page offset\n");
+		ret = -EINVAL;
+		goto out;
+	}
+
+	pfn = page_to_pfn(rknpu_obj->pages[page_offset]);
+	ret = vm_insert_mixed(vma, vmf->address, __pfn_to_pfn_t(pfn, PFN_DEV));
+
+out:
+	switch (ret) {
+	case 0:
+	case -ERESTARTSYS:
+	case -EINTR:
+		return VM_FAULT_NOPAGE;
+	case -ENOMEM:
+		return VM_FAULT_OOM;
+	default:
+		return VM_FAULT_SIGBUS;
+	}
+}
+#else
+int rknpu_gem_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
+{
+	struct drm_gem_object *obj = vma->vm_private_data;
+	struct rknpu_gem_object *rknpu_obj = to_rknpu_obj(obj);
+	struct drm_device *drm = rknpu_obj->base.dev;
+	unsigned long pfn = 0;
+	pgoff_t page_offset = 0;
+	int ret = -EINVAL;
+
+	page_offset = ((unsigned long)vmf->virtual_address - vma->vm_start) >>
+		      PAGE_SHIFT;
+
+	if (page_offset >= (rknpu_obj->size >> PAGE_SHIFT)) {
+		LOG_DEV_ERROR(drm->dev, "invalid page offset\n");
+		ret = -EINVAL;
+		goto out;
+	}
+
+	pfn = page_to_pfn(rknpu_obj->pages[page_offset]);
+	ret = vm_insert_mixed(vma, (unsigned long)vmf->virtual_address,
+			      __pfn_to_pfn_t(pfn, PFN_DEV));
+
+out:
+	switch (ret) {
+	case 0:
+	case -ERESTARTSYS:
+	case -EINTR:
+		return VM_FAULT_NOPAGE;
+	case -ENOMEM:
+		return VM_FAULT_OOM;
+	default:
+		return VM_FAULT_SIGBUS;
+	}
+}
+#endif
+
+static int rknpu_gem_mmap_obj(struct drm_gem_object *obj,
+			      struct vm_area_struct *vma)
+{
+	struct rknpu_gem_object *rknpu_obj = to_rknpu_obj(obj);
+	int ret = -EINVAL;
+
+	LOG_DEBUG("flags: %#x\n", rknpu_obj->flags);
+
+	/* non-cacheable as default. */
+	if (rknpu_obj->flags & RKNPU_MEM_CACHEABLE) {
+		vma->vm_page_prot = vm_get_page_prot(vma->vm_flags);
+	} else if (rknpu_obj->flags & RKNPU_MEM_WRITE_COMBINE) {
+		vma->vm_page_prot =
+			pgprot_writecombine(vm_get_page_prot(vma->vm_flags));
+	} else {
+		vma->vm_page_prot =
+			pgprot_noncached(vm_get_page_prot(vma->vm_flags));
+	}
+
+	ret = rknpu_gem_mmap_buffer(rknpu_obj, vma);
+	if (ret)
+		goto err_close_vm;
+
+	return 0;
+
+err_close_vm:
+	drm_gem_vm_close(vma);
+
+	return ret;
+}
+
+int rknpu_gem_mmap(struct file *filp, struct vm_area_struct *vma)
+{
+	struct drm_gem_object *obj = NULL;
+	int ret = -EINVAL;
+
+	/* set vm_area_struct. */
+	ret = drm_gem_mmap(filp, vma);
+	if (ret < 0) {
+		LOG_ERROR("failed to mmap, ret: %d\n", ret);
+		return ret;
+	}
+
+	obj = vma->vm_private_data;
+
+	if (obj->import_attach)
+		return dma_buf_mmap(obj->dma_buf, vma, 0);
+
+	return rknpu_gem_mmap_obj(obj, vma);
+}
+
+/* low-level interface prime helpers */
+#if KERNEL_VERSION(4, 13, 0) <= LINUX_VERSION_CODE
+struct drm_gem_object *rknpu_gem_prime_import(struct drm_device *dev,
+					      struct dma_buf *dma_buf)
+{
+	return drm_gem_prime_import_dev(dev, dma_buf, dev->dev);
+}
+#endif
+
+struct sg_table *rknpu_gem_prime_get_sg_table(struct drm_gem_object *obj)
+{
+	struct rknpu_gem_object *rknpu_obj = to_rknpu_obj(obj);
+	int npages = 0;
+
+	npages = rknpu_obj->size >> PAGE_SHIFT;
+
+#if KERNEL_VERSION(5, 10, 0) <= LINUX_VERSION_CODE
+	return drm_prime_pages_to_sg(obj->dev, rknpu_obj->pages, npages);
+#else
+	return drm_prime_pages_to_sg(rknpu_obj->pages, npages);
+#endif
+}
+
+struct drm_gem_object *
+rknpu_gem_prime_import_sg_table(struct drm_device *dev,
+				struct dma_buf_attachment *attach,
+				struct sg_table *sgt)
+{
+	struct rknpu_gem_object *rknpu_obj = NULL;
+	int npages = 0;
+	int ret = -EINVAL;
+
+	rknpu_obj = rknpu_gem_init(dev, attach->dmabuf->size);
+	if (IS_ERR(rknpu_obj)) {
+		ret = PTR_ERR(rknpu_obj);
+		return ERR_PTR(ret);
+	}
+
+	rknpu_obj->dma_addr = sg_dma_address(sgt->sgl);
+
+	npages = rknpu_obj->size >> PAGE_SHIFT;
+	rknpu_obj->pages = rknpu_gem_alloc_page(npages);
+	if (!rknpu_obj->pages) {
+		ret = -ENOMEM;
+		goto err;
+	}
+
+	ret = drm_prime_sg_to_page_array(sgt, rknpu_obj->pages, npages);
+	if (ret < 0)
+		goto err_free_large;
+
+	rknpu_obj->sgt = sgt;
+
+	if (sgt->nents == 1) {
+		/* always physically continuous memory if sgt->nents is 1. */
+		rknpu_obj->flags |= RKNPU_MEM_CONTIGUOUS;
+	} else {
+		/*
+		 * this case could be CONTIG or NONCONTIG type but for now
+		 * sets NONCONTIG.
+		 * TODO. we have to find a way that exporter can notify
+		 * the type of its own buffer to importer.
+		 */
+		rknpu_obj->flags |= RKNPU_MEM_NON_CONTIGUOUS;
+	}
+
+	return &rknpu_obj->base;
+
+err_free_large:
+	rknpu_gem_free_page(rknpu_obj->pages);
+err:
+	rknpu_gem_release(rknpu_obj);
+	return ERR_PTR(ret);
+}
+
+int rknpu_gem_prime_vmap(struct drm_gem_object *obj, struct iosys_map *map)
+{
+	struct rknpu_gem_object *rknpu_obj = to_rknpu_obj(obj);
+
+	if (!rknpu_obj->pages)
+		return -EINVAL;
+	
+	map->vaddr = vmap(rknpu_obj->pages, rknpu_obj->num_pages, VM_MAP,
+			  PAGE_KERNEL);
+	
+	return map->vaddr ? 0 : -ENOMEM;
+}
+
+void rknpu_gem_prime_vunmap(struct drm_gem_object *obj, struct iosys_map *map)
+{
+	vunmap(map->vaddr);
+}
+
+int rknpu_gem_prime_mmap(struct drm_gem_object *obj, struct vm_area_struct *vma)
+{
+	int ret = -EINVAL;
+
+	ret = drm_gem_mmap_obj(obj, obj->size, vma);
+	if (ret < 0)
+		return ret;
+
+	return rknpu_gem_mmap_obj(obj, vma);
+}
+
+int rknpu_gem_sync_ioctl(struct drm_device *dev, void *data,
+			 struct drm_file *file_priv)
+{
+	struct rknpu_gem_object *rknpu_obj = NULL;
+	struct rknpu_mem_sync *args = data;
+	struct scatterlist *sg;
+	unsigned long length, offset = 0;
+	unsigned long sg_left, size = 0;
+	unsigned long len = 0;
+	int i;
+
+	rknpu_obj = (struct rknpu_gem_object *)(uintptr_t)args->obj_addr;
+	if (!rknpu_obj)
+		return -EINVAL;
+
+	if (!(rknpu_obj->flags & RKNPU_MEM_CACHEABLE))
+		return -EINVAL;
+
+	if (!(rknpu_obj->flags & RKNPU_MEM_NON_CONTIGUOUS)) {
+		if (args->flags & RKNPU_MEM_SYNC_TO_DEVICE) {
+			dma_sync_single_range_for_device(
+				dev->dev, rknpu_obj->dma_addr, args->offset,
+				args->size, DMA_TO_DEVICE);
+		}
+		if (args->flags & RKNPU_MEM_SYNC_FROM_DEVICE) {
+			dma_sync_single_range_for_cpu(dev->dev,
+						      rknpu_obj->dma_addr,
+						      args->offset, args->size,
+						      DMA_FROM_DEVICE);
+		}
+	} else {
+		length = args->size;
+		offset = args->offset;
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_SRAM
+		if (rknpu_obj->sram_size > 0) {
+			struct drm_gem_object *obj = &rknpu_obj->base;
+			struct rknpu_device *rknpu_dev = obj->dev->dev_private;
+			unsigned long sram_offset =
+				rknpu_obj->sram_obj->range_start *
+				rknpu_dev->sram_mm->chunk_size;
+			if ((offset + length) <= rknpu_obj->sram_size) {
+				dmac_map_area(rknpu_dev->sram_base_io +
+						       offset + sram_offset,
+					       length, DMA_TO_DEVICE);
+				dmac_unmap_area(rknpu_dev->sram_base_io +
+							 offset + sram_offset,
+						 length, DMA_FROM_DEVICE);
+				length = 0;
+				offset = 0;
+			} else if (offset >= rknpu_obj->sram_size) {
+				offset -= rknpu_obj->sram_size;
+			} else {
+				unsigned long sram_length =
+					rknpu_obj->sram_size - offset;
+				dmac_map_area(rknpu_dev->sram_base_io +
+						       offset + sram_offset,
+					       sram_length, DMA_TO_DEVICE);
+				dmac_unmap_area(rknpu_dev->sram_base_io +
+							 offset + sram_offset,
+						 sram_length, DMA_FROM_DEVICE);
+				length -= sram_length;
+				offset = 0;
+			}
+		}
+#endif
+
+		for_each_sg(rknpu_obj->sgt->sgl, sg, rknpu_obj->sgt->nents,
+			     i) {
+			if (length == 0)
+				break;
+
+			len += sg->length;
+			if (len <= offset)
+				continue;
+
+			sg_left = len - offset;
+			size = (length < sg_left) ? length : sg_left;
+
+			if (args->flags & RKNPU_MEM_SYNC_TO_DEVICE) {
+				dma_sync_sg_for_device(dev->dev, sg, 1,
+						       DMA_TO_DEVICE);
+			}
+
+			if (args->flags & RKNPU_MEM_SYNC_FROM_DEVICE) {
+				dma_sync_sg_for_cpu(dev->dev, sg, 1,
+						    DMA_FROM_DEVICE);
+			}
+
+			offset += size;
+			length -= size;
+		}
+	}
+
+	return 0;
+}
diff --git a/drivers/rknpu/rknpu_job.c b/drivers/rknpu/rknpu_job.c
new file mode 100644
index 000000000..6f601ab5f
--- /dev/null
+++ b/drivers/rknpu/rknpu_job.c
@@ -0,0 +1,910 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#include <linux/slab.h>
+#include <linux/delay.h>
+#include <linux/sync_file.h>
+#include <linux/io.h>
+
+#include "rknpu_ioctl.h"
+#include "rknpu_drv.h"
+#include "rknpu_reset.h"
+#include "rknpu_gem.h"
+#include "rknpu_fence.h"
+#include "rknpu_job.h"
+#include "rknpu_mem.h"
+
+#define _REG_READ(base, offset) readl(base + (offset))
+#define _REG_WRITE(base, value, offset) writel(value, base + (offset))
+
+#define REG_READ(offset) _REG_READ(rknpu_core_base, offset)
+#define REG_WRITE(value, offset) _REG_WRITE(rknpu_core_base, value, offset)
+
+static int rknpu_core_index(int core_mask)
+{
+	int index = 0;
+
+	if (core_mask & RKNPU_CORE0_MASK)
+		index = 0;
+	else if (core_mask & RKNPU_CORE1_MASK)
+		index = 1;
+	else if (core_mask & RKNPU_CORE2_MASK)
+		index = 2;
+
+	return index;
+}
+
+static int rknpu_core_mask(int core_index)
+{
+	int core_mask = RKNPU_CORE_AUTO_MASK;
+
+	switch (core_index) {
+	case 0:
+		core_mask = RKNPU_CORE0_MASK;
+		break;
+	case 1:
+		core_mask = RKNPU_CORE1_MASK;
+		break;
+	case 2:
+		core_mask = RKNPU_CORE2_MASK;
+		break;
+	default:
+		break;
+	}
+
+	return core_mask;
+}
+
+static int rknn_get_task_number(struct rknpu_job *job, int core_index)
+{
+	int task_num = job->args->task_number;
+
+	if (job->use_core_num == 2)
+		task_num = job->args->subcore_task[core_index].task_number;
+	else if (job->use_core_num == 3)
+		task_num = job->args->subcore_task[core_index + 2].task_number;
+
+	return task_num;
+}
+
+static void rknpu_job_free(struct rknpu_job *job)
+{
+#ifdef CONFIG_ROCKCHIP_RKNPU_DRM_GEM
+	struct rknpu_gem_object *task_obj = NULL;
+
+	task_obj =
+		(struct rknpu_gem_object *)(uintptr_t)job->args->task_obj_addr;
+	if (task_obj)
+		rknpu_gem_object_put(&task_obj->base);
+#endif
+
+	if (job->fence)
+		dma_fence_put(job->fence);
+
+	if (job->args_owner)
+		kfree(job->args);
+
+	kfree(job);
+}
+
+static int rknpu_job_cleanup(struct rknpu_job *job)
+{
+	rknpu_job_free(job);
+
+	return 0;
+}
+
+static void rknpu_job_cleanup_work(struct work_struct *work)
+{
+	struct rknpu_job *job =
+		container_of(work, struct rknpu_job, cleanup_work);
+
+	rknpu_job_cleanup(job);
+}
+
+static inline struct rknpu_job *rknpu_job_alloc(struct rknpu_device *rknpu_dev,
+						struct rknpu_submit *args)
+{
+	struct rknpu_job *job = NULL;
+#ifdef CONFIG_ROCKCHIP_RKNPU_DRM_GEM
+	struct rknpu_gem_object *task_obj = NULL;
+#endif
+	if (rknpu_dev->config->num_irqs == 1)
+		args->core_mask = RKNPU_CORE0_MASK;
+
+	job = kzalloc(sizeof(*job), GFP_KERNEL);
+	if (!job)
+		return NULL;
+
+	job->timestamp = ktime_get();
+	job->rknpu_dev = rknpu_dev;
+	job->use_core_num = (args->core_mask & RKNPU_CORE0_MASK) +
+			    ((args->core_mask & RKNPU_CORE1_MASK) >> 1) +
+			    ((args->core_mask & RKNPU_CORE2_MASK) >> 2);
+	job->run_count = job->use_core_num;
+	job->interrupt_count = job->use_core_num;
+#ifdef CONFIG_ROCKCHIP_RKNPU_DRM_GEM
+	task_obj = (struct rknpu_gem_object *)(uintptr_t)args->task_obj_addr;
+	if (task_obj)
+		rknpu_gem_object_get(&task_obj->base);
+#endif
+
+	if (!(args->flags & RKNPU_JOB_NONBLOCK)) {
+		job->args = args;
+		job->args_owner = false;
+		return job;
+	}
+
+	job->args = kzalloc(sizeof(*args), GFP_KERNEL);
+	if (!job->args) {
+		kfree(job);
+		return NULL;
+	}
+	*job->args = *args;
+	job->args_owner = true;
+
+	INIT_WORK(&job->cleanup_work, rknpu_job_cleanup_work);
+
+	return job;
+}
+
+static inline int rknpu_job_wait(struct rknpu_job *job)
+{
+	struct rknpu_device *rknpu_dev = job->rknpu_dev;
+	struct rknpu_submit *args = job->args;
+	struct rknpu_task *last_task = NULL;
+	struct rknpu_subcore_data *subcore_data = NULL;
+	void __iomem *rknpu_core_base = NULL;
+	int core_index = rknpu_core_index(job->args->core_mask);
+	unsigned long flags;
+	int wait_count = 0;
+	int ret = -EINVAL;
+
+	subcore_data = &rknpu_dev->subcore_datas[core_index];
+
+	do {
+		ret = wait_event_interruptible_timeout(
+			subcore_data->job_done_wq,
+			job->flags & RKNPU_JOB_DONE || rknpu_dev->soft_reseting,
+			msecs_to_jiffies(args->timeout));
+		if (++wait_count >= 3)
+			break;
+	} while (ret == 0 && job->in_queue[core_index]);
+
+	if (job->in_queue[core_index]) {
+		spin_lock_irqsave(&rknpu_dev->lock, flags);
+		list_del_init(&job->head[core_index]);
+		subcore_data->task_num -= rknn_get_task_number(job, core_index);
+		job->in_queue[core_index] = false;
+		spin_unlock_irqrestore(&rknpu_dev->lock, flags);
+		return ret < 0 ? ret : -EINVAL;
+	}
+
+	last_task = job->last_task;
+	if (!last_task)
+		return ret < 0 ? ret : -EINVAL;
+
+	last_task->int_status = job->int_status[core_index];
+
+	if (ret <= 0) {
+		args->task_counter = 0;
+		rknpu_core_base = rknpu_dev->base[core_index];
+		if (args->flags & RKNPU_JOB_PC) {
+			uint32_t task_status =
+				REG_READ(RKNPU_OFFSET_PC_TASK_STATUS);
+			args->task_counter =
+				(task_status &
+				 rknpu_dev->config->pc_task_number_mask);
+		}
+
+		LOG_ERROR(
+			"failed to wait job, task counter: %d, flags: %#x, ret = %d, elapsed time: %lldus\n",
+			args->task_counter, args->flags, ret,
+			ktime_to_us(ktime_sub(ktime_get(), job->timestamp)));
+
+		return ret < 0 ? ret : -ETIMEDOUT;
+	}
+
+	if (!(job->flags & RKNPU_JOB_DONE))
+		return -EINVAL;
+
+	args->task_counter = args->task_number;
+
+	return 0;
+}
+
+static inline int rknpu_job_commit_pc(struct rknpu_job *job, int core_index)
+{
+	struct rknpu_device *rknpu_dev = job->rknpu_dev;
+	struct rknpu_submit *args = job->args;
+#ifdef CONFIG_ROCKCHIP_RKNPU_DRM_GEM
+	struct rknpu_gem_object *task_obj =
+		(struct rknpu_gem_object *)(uintptr_t)args->task_obj_addr;
+#endif
+#ifdef CONFIG_ROCKCHIP_RKNPU_DMA_HEAP
+	struct rknpu_mem_object *task_obj =
+		(struct rknpu_mem_object *)(uintptr_t)args->task_obj_addr;
+#endif
+	struct rknpu_task *task_base = NULL;
+	struct rknpu_task *first_task = NULL;
+	struct rknpu_task *last_task = NULL;
+	void __iomem *rknpu_core_base = rknpu_dev->base[core_index];
+	int task_start = args->task_start;
+	int task_end = args->task_start + args->task_number - 1;
+	int task_number = args->task_number;
+	int task_pp_en = args->flags & RKNPU_JOB_PINGPONG ? 1 : 0;
+	int pc_data_amount_scale = rknpu_dev->config->pc_data_amount_scale;
+	int pc_task_number_bits = rknpu_dev->config->pc_task_number_bits;
+	int i = 0;
+
+	if (!task_obj)
+		return -EINVAL;
+
+	if (rknpu_dev->config->num_irqs > 1) {
+		for (i = 0; i < rknpu_dev->config->num_irqs; i++) {
+			if (i == core_index) {
+				REG_WRITE((0xe + 0x10000000 * i), 0x1004);
+				REG_WRITE((0xe + 0x10000000 * i), 0x3004);
+			}
+		}
+
+		if (job->use_core_num == 1) {
+			task_start = args->subcore_task[core_index].task_start;
+			task_end = args->subcore_task[core_index].task_start +
+				   args->subcore_task[core_index].task_number -
+				   1;
+			task_number =
+				args->subcore_task[core_index].task_number;
+		} else if (job->use_core_num == 2) {
+			task_start = args->subcore_task[core_index].task_start;
+			task_end = args->subcore_task[core_index].task_start +
+				   args->subcore_task[core_index].task_number -
+				   1;
+			task_number =
+				args->subcore_task[core_index].task_number;
+		} else if (job->use_core_num == 3) {
+			task_start =
+				args->subcore_task[core_index + 2].task_start;
+			task_end =
+				args->subcore_task[core_index + 2].task_start +
+				args->subcore_task[core_index + 2].task_number -
+				1;
+			task_number =
+				args->subcore_task[core_index + 2].task_number;
+		}
+	}
+
+	task_base = task_obj->kv_addr;
+
+	first_task = &task_base[task_start];
+	last_task = &task_base[task_end];
+
+	REG_WRITE(first_task->regcmd_addr, RKNPU_OFFSET_PC_DATA_ADDR);
+
+	REG_WRITE((first_task->regcfg_amount + RKNPU_PC_DATA_EXTRA_AMOUNT +
+		   pc_data_amount_scale - 1) /
+				  pc_data_amount_scale -
+			  1,
+		  RKNPU_OFFSET_PC_DATA_AMOUNT);
+
+	REG_WRITE(last_task->int_mask, RKNPU_OFFSET_INT_MASK);
+
+	REG_WRITE(first_task->int_mask, RKNPU_OFFSET_INT_CLEAR);
+
+	REG_WRITE(((0x6 | task_pp_en) << pc_task_number_bits) | task_number,
+		  RKNPU_OFFSET_PC_TASK_CONTROL);
+
+	REG_WRITE(args->task_base_addr, RKNPU_OFFSET_PC_DMA_BASE_ADDR);
+
+	job->first_task = first_task;
+	job->last_task = last_task;
+	job->int_mask[core_index] = last_task->int_mask;
+
+	REG_WRITE(0x1, RKNPU_OFFSET_PC_OP_EN);
+	REG_WRITE(0x0, RKNPU_OFFSET_PC_OP_EN);
+
+	return 0;
+}
+
+static int rknpu_job_commit(struct rknpu_job *job, int core_index)
+{
+	struct rknpu_device *rknpu_dev = job->rknpu_dev;
+	struct rknpu_submit *args = job->args;
+	void __iomem *rknpu_core_base = rknpu_dev->base[core_index];
+
+	// switch to slave mode
+	REG_WRITE(0x1, RKNPU_OFFSET_PC_DATA_ADDR);
+
+	if (!(args->flags & RKNPU_JOB_PC))
+		return -EINVAL;
+
+	return rknpu_job_commit_pc(job, core_index);
+}
+
+static void rknpu_job_next(struct rknpu_device *rknpu_dev, int core_index)
+{
+	struct rknpu_job *job = NULL;
+	struct rknpu_subcore_data *subcore_data = NULL;
+	unsigned long flags;
+
+	if (rknpu_dev->soft_reseting)
+		return;
+
+	subcore_data = &rknpu_dev->subcore_datas[core_index];
+
+	spin_lock_irqsave(&rknpu_dev->irq_lock, flags);
+
+	if (subcore_data->job || list_empty(&subcore_data->todo_list)) {
+		spin_unlock_irqrestore(&rknpu_dev->irq_lock, flags);
+		return;
+	}
+
+	job = list_first_entry(&subcore_data->todo_list, struct rknpu_job,
+			       head[core_index]);
+
+	list_del_init(&job->head[core_index]);
+	job->in_queue[core_index] = false;
+	subcore_data->job = job;
+	job->run_count--;
+	job->hw_recoder_time = ktime_get();
+	spin_unlock_irqrestore(&rknpu_dev->irq_lock, flags);
+
+	if (job->run_count == 0) {
+		if (job->args->core_mask & RKNPU_CORE0_MASK)
+			job->ret = rknpu_job_commit(job, 0);
+		if (job->args->core_mask & RKNPU_CORE1_MASK)
+			job->ret = rknpu_job_commit(job, 1);
+		if (job->args->core_mask & RKNPU_CORE2_MASK)
+			job->ret = rknpu_job_commit(job, 2);
+	}
+}
+
+static void rknpu_job_done(struct rknpu_job *job, int ret, int core_index)
+{
+	struct rknpu_device *rknpu_dev = job->rknpu_dev;
+	struct rknpu_subcore_data *subcore_data = NULL;
+	unsigned long flags;
+	ktime_t now = ktime_get();
+
+	subcore_data = &rknpu_dev->subcore_datas[core_index];
+
+	spin_lock_irqsave(&rknpu_dev->irq_lock, flags);
+	subcore_data->job = NULL;
+	subcore_data->task_num -= rknn_get_task_number(job, core_index);
+	job->interrupt_count--;
+	subcore_data->timer.busy_time +=
+		ktime_us_delta(now, job->hw_recoder_time);
+	spin_unlock_irqrestore(&rknpu_dev->irq_lock, flags);
+
+	if (job->interrupt_count == 0) {
+		int use_core_num = job->use_core_num;
+
+		job->flags |= RKNPU_JOB_DONE;
+		job->ret = ret;
+
+		if (job->fence)
+			dma_fence_signal(job->fence);
+
+		if (job->flags & RKNPU_JOB_ASYNC)
+			schedule_work(&job->cleanup_work);
+
+		if (use_core_num > 1)
+			wake_up(&(&rknpu_dev->subcore_datas[0])->job_done_wq);
+		else
+			wake_up(&subcore_data->job_done_wq);
+	}
+
+	rknpu_job_next(rknpu_dev, core_index);
+}
+
+static void rknpu_job_schedule(struct rknpu_job *job)
+{
+	struct rknpu_device *rknpu_dev = job->rknpu_dev;
+	struct rknpu_subcore_data *subcore_data = NULL;
+	int i = 0, core_index = 0;
+	unsigned long flags;
+	int task_num_list[3] = { 0, 1, 2 };
+	int tmp = 0;
+
+	if ((job->args->core_mask & 0x07) == RKNPU_CORE_AUTO_MASK) {
+		if (rknpu_dev->subcore_datas[0].task_num >
+		    rknpu_dev->subcore_datas[1].task_num) {
+			tmp = task_num_list[1];
+			task_num_list[1] = task_num_list[0];
+			task_num_list[0] = tmp;
+		}
+		if (rknpu_dev->subcore_datas[task_num_list[0]].task_num >
+		    rknpu_dev->subcore_datas[2].task_num) {
+			tmp = task_num_list[2];
+			task_num_list[2] = task_num_list[1];
+			task_num_list[1] = task_num_list[0];
+			task_num_list[0] = tmp;
+		} else if (rknpu_dev->subcore_datas[task_num_list[1]].task_num >
+			   rknpu_dev->subcore_datas[2].task_num) {
+			tmp = task_num_list[2];
+			task_num_list[2] = task_num_list[1];
+			task_num_list[1] = tmp;
+		}
+		if (!rknpu_dev->subcore_datas[task_num_list[0]].job)
+			core_index = task_num_list[0];
+		else if (!rknpu_dev->subcore_datas[task_num_list[1]].job)
+			core_index = task_num_list[1];
+		else if (!rknpu_dev->subcore_datas[task_num_list[2]].job)
+			core_index = task_num_list[2];
+		else
+			core_index = task_num_list[0];
+
+		job->args->core_mask = rknpu_core_mask(core_index);
+		job->use_core_num = 1;
+		job->interrupt_count = 1;
+		job->run_count = 1;
+	}
+
+	for (i = 0; i < rknpu_dev->config->num_irqs; i++) {
+		if (job->args->core_mask & rknpu_core_mask(i)) {
+			subcore_data = &rknpu_dev->subcore_datas[i];
+			spin_lock_irqsave(&rknpu_dev->irq_lock, flags);
+			list_add_tail(&job->head[i], &subcore_data->todo_list);
+			subcore_data->task_num += rknn_get_task_number(job, i);
+			job->in_queue[i] = true;
+			spin_unlock_irqrestore(&rknpu_dev->irq_lock, flags);
+		}
+	}
+
+	for (i = 0; i < rknpu_dev->config->num_irqs; i++) {
+		if (job->args->core_mask & rknpu_core_mask(i))
+			rknpu_job_next(rknpu_dev, i);
+	}
+}
+
+static void rknpu_job_abort(struct rknpu_job *job)
+{
+	struct rknpu_device *rknpu_dev = job->rknpu_dev;
+	struct rknpu_subcore_data *subcore_data = NULL;
+	int core_index = rknpu_core_index(job->args->core_mask);
+	void __iomem *rknpu_core_base = rknpu_dev->base[core_index];
+	unsigned long flags;
+	int i = 0;
+
+	msleep(100);
+
+	for (i = 0; i < rknpu_dev->config->num_irqs; i++) {
+		if (job->args->core_mask & rknpu_core_mask(i)) {
+			subcore_data = &rknpu_dev->subcore_datas[i];
+			spin_lock_irqsave(&rknpu_dev->irq_lock, flags);
+			if (job == subcore_data->job && !job->irq_entry[i]) {
+				subcore_data->job = NULL;
+				subcore_data->task_num -=
+					rknn_get_task_number(job, i);
+			}
+			spin_unlock_irqrestore(&rknpu_dev->irq_lock, flags);
+		}
+	}
+
+	if (job->ret == -ETIMEDOUT) {
+		LOG_ERROR(
+			"job timeout, flags: %#x, irq status: %#x, raw status: %#x, require mask: %#x, task counter: %#x, elapsed time: %lldus\n",
+			job->flags, REG_READ(RKNPU_OFFSET_INT_STATUS),
+			REG_READ(RKNPU_OFFSET_INT_RAW_STATUS),
+			job->int_mask[core_index],
+			(REG_READ(RKNPU_OFFSET_PC_TASK_STATUS) &
+			 rknpu_dev->config->pc_task_number_mask),
+			ktime_to_us(ktime_sub(ktime_get(), job->timestamp)));
+		rknpu_soft_reset(rknpu_dev);
+	} else {
+		LOG_ERROR(
+			"job abort, flags: %#x, ret: %d, elapsed time: %lldus\n",
+			job->flags, job->ret,
+			ktime_to_us(ktime_sub(ktime_get(), job->timestamp)));
+	}
+
+	rknpu_job_cleanup(job);
+}
+
+static inline uint32_t rknpu_fuzz_status(uint32_t status)
+{
+	uint32_t fuzz_status = 0;
+
+	if ((status & 0x3) != 0)
+		fuzz_status |= 0x3;
+
+	if ((status & 0xc) != 0)
+		fuzz_status |= 0xc;
+
+	if ((status & 0x30) != 0)
+		fuzz_status |= 0x30;
+
+	if ((status & 0xc0) != 0)
+		fuzz_status |= 0xc0;
+
+	if ((status & 0x300) != 0)
+		fuzz_status |= 0x300;
+
+	if ((status & 0xc00) != 0)
+		fuzz_status |= 0xc00;
+
+	return fuzz_status;
+}
+
+static inline irqreturn_t rknpu_irq_handler(int irq, void *data, int core_index)
+{
+	struct rknpu_device *rknpu_dev = data;
+	void __iomem *rknpu_core_base = rknpu_dev->base[core_index];
+	struct rknpu_subcore_data *subcore_data = NULL;
+	struct rknpu_job *job = NULL;
+	uint32_t status = 0;
+	unsigned long flags;
+
+	subcore_data = &rknpu_dev->subcore_datas[core_index];
+
+	spin_lock_irqsave(&rknpu_dev->irq_lock, flags);
+	job = subcore_data->job;
+	if (!job) {
+		spin_unlock_irqrestore(&rknpu_dev->irq_lock, flags);
+		REG_WRITE(RKNPU_INT_CLEAR, RKNPU_OFFSET_INT_CLEAR);
+		rknpu_job_next(rknpu_dev, core_index);
+		return IRQ_HANDLED;
+	}
+	job->irq_entry[core_index] = true;
+	spin_unlock_irqrestore(&rknpu_dev->irq_lock, flags);
+
+	status = REG_READ(RKNPU_OFFSET_INT_STATUS);
+
+	job->int_status[core_index] = status;
+
+	if (rknpu_fuzz_status(status) != job->int_mask[core_index]) {
+		LOG_ERROR(
+			"invalid irq status: %#x, raw status: %#x, require mask: %#x, task counter: %#x\n",
+			status, REG_READ(RKNPU_OFFSET_INT_RAW_STATUS),
+			job->int_mask[core_index],
+			(REG_READ(RKNPU_OFFSET_PC_TASK_STATUS) &
+			 rknpu_dev->config->pc_task_number_mask));
+		REG_WRITE(RKNPU_INT_CLEAR, RKNPU_OFFSET_INT_CLEAR);
+		return IRQ_HANDLED;
+	}
+
+	REG_WRITE(RKNPU_INT_CLEAR, RKNPU_OFFSET_INT_CLEAR);
+
+	rknpu_job_done(job, 0, core_index);
+
+	return IRQ_HANDLED;
+}
+
+irqreturn_t rknpu_core0_irq_handler(int irq, void *data)
+{
+	return rknpu_irq_handler(irq, data, 0);
+}
+
+irqreturn_t rknpu_core1_irq_handler(int irq, void *data)
+{
+	return rknpu_irq_handler(irq, data, 1);
+}
+
+irqreturn_t rknpu_core2_irq_handler(int irq, void *data)
+{
+	return rknpu_irq_handler(irq, data, 2);
+}
+
+static void rknpu_job_timeout_clean(struct rknpu_device *rknpu_dev,
+				    int core_mask)
+{
+	struct rknpu_job *job = NULL;
+	unsigned long flags;
+	ktime_t now = ktime_get();
+	struct rknpu_subcore_data *subcore_data = NULL;
+	int i = 0;
+
+	for (i = 0; i < rknpu_dev->config->num_irqs; i++) {
+		if (core_mask & rknpu_core_mask(i)) {
+			subcore_data = &rknpu_dev->subcore_datas[i];
+			job = subcore_data->job;
+			if (job &&
+			    ktime_to_ms(ktime_sub(now, job->timestamp)) >=
+				    job->args->timeout) {
+				rknpu_soft_reset(rknpu_dev);
+
+				spin_lock_irqsave(&rknpu_dev->irq_lock, flags);
+				subcore_data->job = NULL;
+				spin_unlock_irqrestore(&rknpu_dev->irq_lock,
+						       flags);
+
+				do {
+					schedule_work(&job->cleanup_work);
+
+					spin_lock_irqsave(&rknpu_dev->irq_lock,
+							  flags);
+
+					if (!list_empty(
+						    &subcore_data->todo_list)) {
+						job = list_first_entry(
+							&subcore_data->todo_list,
+							struct rknpu_job,
+							head[i]);
+						list_del_init(&job->head[i]);
+						job->in_queue[i] = false;
+					} else {
+						job = NULL;
+					}
+
+					spin_unlock_irqrestore(
+						&rknpu_dev->irq_lock, flags);
+				} while (job);
+			}
+		}
+	}
+}
+
+static int rknpu_submit(struct rknpu_device *rknpu_dev,
+			struct rknpu_submit *args)
+{
+	struct rknpu_job *job = NULL;
+	int ret = -EINVAL;
+
+	if (args->task_number == 0) {
+		LOG_ERROR("invalid rknpu task number!\n");
+		return -EINVAL;
+	}
+
+	job = rknpu_job_alloc(rknpu_dev, args);
+	if (!job) {
+		LOG_ERROR("failed to allocate rknpu job!\n");
+		return -ENOMEM;
+	}
+
+	if (args->flags & RKNPU_JOB_FENCE_IN) {
+#ifdef CONFIG_ROCKCHIP_RKNPU_FENCE
+		struct dma_fence *in_fence;
+
+		in_fence = sync_file_get_fence(args->fence_fd);
+
+		if (!in_fence) {
+			LOG_ERROR("invalid fence in fd, fd: %d\n",
+				  args->fence_fd);
+			return -EINVAL;
+		}
+		args->fence_fd = -1;
+
+		/*
+		 * Wait if the fence is from a foreign context, or if the fence
+		 * array contains any fence from a foreign context.
+		 */
+		ret = 0;
+		if (!dma_fence_match_context(in_fence,
+					     rknpu_dev->fence_ctx->context))
+			ret = dma_fence_wait_timeout(in_fence, true,
+						     args->timeout);
+		dma_fence_put(in_fence);
+		if (ret < 0) {
+			if (ret != -ERESTARTSYS)
+				LOG_ERROR("Error (%d) waiting for fence!\n",
+					  ret);
+
+			return ret;
+		}
+#else
+		LOG_ERROR(
+			"failed to use rknpu fence, please enable rknpu fence config!\n");
+		rknpu_job_free(job);
+		return -EINVAL;
+#endif
+	}
+
+	if (args->flags & RKNPU_JOB_FENCE_OUT) {
+#ifdef CONFIG_ROCKCHIP_RKNPU_FENCE
+		ret = rknpu_fence_alloc(job);
+		if (ret) {
+			rknpu_job_free(job);
+			return ret;
+		}
+		job->args->fence_fd = rknpu_fence_get_fd(job);
+		args->fence_fd = job->args->fence_fd;
+#else
+		LOG_ERROR(
+			"failed to use rknpu fence, please enable rknpu fence config!\n");
+		rknpu_job_free(job);
+		return -EINVAL;
+#endif
+	}
+
+	if (args->flags & RKNPU_JOB_NONBLOCK) {
+		job->flags |= RKNPU_JOB_ASYNC;
+		rknpu_job_timeout_clean(rknpu_dev, job->args->core_mask);
+		rknpu_job_schedule(job);
+		ret = job->ret;
+		if (ret) {
+			rknpu_job_abort(job);
+			return ret;
+		}
+	} else {
+		rknpu_job_schedule(job);
+		if (args->flags & RKNPU_JOB_PC)
+			job->ret = rknpu_job_wait(job);
+
+		args->task_counter = job->args->task_counter;
+		ret = job->ret;
+		if (!ret)
+			rknpu_job_cleanup(job);
+		else
+			rknpu_job_abort(job);
+	}
+
+	return ret;
+}
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_DRM_GEM
+int rknpu_submit_ioctl(struct drm_device *dev, void *data,
+		       struct drm_file *file_priv)
+{
+	struct rknpu_device *rknpu_dev = dev_get_drvdata(dev->dev);
+	struct rknpu_submit *args = data;
+
+	return rknpu_submit(rknpu_dev, args);
+}
+#endif
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_DMA_HEAP
+int rknpu_submit_ioctl(struct rknpu_device *rknpu_dev, unsigned long data)
+{
+	struct rknpu_submit args;
+	int ret = -EINVAL;
+
+	if (unlikely(copy_from_user(&args, (struct rknpu_submit *)data,
+				    sizeof(struct rknpu_submit)))) {
+		LOG_ERROR("%s: copy_from_user failed\n", __func__);
+		ret = -EFAULT;
+		return ret;
+	}
+
+	ret = rknpu_submit(rknpu_dev, &args);
+
+	if (unlikely(copy_to_user((struct rknpu_submit *)data, &args,
+				  sizeof(struct rknpu_submit)))) {
+		LOG_ERROR("%s: copy_to_user failed\n", __func__);
+		ret = -EFAULT;
+		return ret;
+	}
+
+	return ret;
+}
+#endif
+
+int rknpu_get_hw_version(struct rknpu_device *rknpu_dev, uint32_t *version)
+{
+	void __iomem *rknpu_core_base = rknpu_dev->base[0];
+
+	if (version == NULL)
+		return -EINVAL;
+
+	*version = REG_READ(RKNPU_OFFSET_VERSION) +
+		   REG_READ(RKNPU_OFFSET_VERSION_NUM);
+
+	return 0;
+}
+
+int rknpu_get_bw_priority(struct rknpu_device *rknpu_dev, uint32_t *priority,
+			  uint32_t *expect, uint32_t *tw)
+{
+	void __iomem *base = rknpu_dev->bw_priority_base;
+
+	if (!rknpu_dev->config->bw_enable) {
+		LOG_WARN("Get bw_priority is not supported on this device!\n");
+		return 0;
+	}
+
+	if (!base)
+		return -EINVAL;
+
+	spin_lock(&rknpu_dev->lock);
+
+	if (priority != NULL)
+		*priority = _REG_READ(base, 0x0);
+
+	if (expect != NULL)
+		*expect = _REG_READ(base, 0x8);
+
+	if (tw != NULL)
+		*tw = _REG_READ(base, 0xc);
+
+	spin_unlock(&rknpu_dev->lock);
+
+	return 0;
+}
+
+int rknpu_set_bw_priority(struct rknpu_device *rknpu_dev, uint32_t priority,
+			  uint32_t expect, uint32_t tw)
+{
+	void __iomem *base = rknpu_dev->bw_priority_base;
+
+	if (!rknpu_dev->config->bw_enable) {
+		LOG_WARN("Set bw_priority is not supported on this device!\n");
+		return 0;
+	}
+
+	if (!base)
+		return -EINVAL;
+
+	spin_lock(&rknpu_dev->lock);
+
+	if (priority != 0)
+		_REG_WRITE(base, priority, 0x0);
+
+	if (expect != 0)
+		_REG_WRITE(base, expect, 0x8);
+
+	if (tw != 0)
+		_REG_WRITE(base, tw, 0xc);
+
+	spin_unlock(&rknpu_dev->lock);
+
+	return 0;
+}
+
+int rknpu_clear_rw_amount(struct rknpu_device *rknpu_dev)
+{
+	void __iomem *rknpu_core_base = rknpu_dev->base[0];
+
+	if (!rknpu_dev->config->bw_enable) {
+		LOG_WARN("Clear rw_amount is not supported on this device!\n");
+		return 0;
+	}
+
+	spin_lock(&rknpu_dev->lock);
+
+	REG_WRITE(0x80000101, RKNPU_OFFSET_CLR_ALL_RW_AMOUNT);
+	REG_WRITE(0x00000101, RKNPU_OFFSET_CLR_ALL_RW_AMOUNT);
+
+	spin_unlock(&rknpu_dev->lock);
+
+	return 0;
+}
+
+int rknpu_get_rw_amount(struct rknpu_device *rknpu_dev, uint32_t *dt_wr,
+			uint32_t *dt_rd, uint32_t *wd_rd)
+{
+	void __iomem *rknpu_core_base = rknpu_dev->base[0];
+	int amount_scale = rknpu_dev->config->pc_data_amount_scale;
+
+	if (!rknpu_dev->config->bw_enable) {
+		LOG_WARN("Get rw_amount is not supported on this device!\n");
+		return 0;
+	}
+
+	spin_lock(&rknpu_dev->lock);
+
+	if (dt_wr != NULL)
+		*dt_wr = REG_READ(RKNPU_OFFSET_DT_WR_AMOUNT) * amount_scale;
+
+	if (dt_rd != NULL)
+		*dt_rd = REG_READ(RKNPU_OFFSET_DT_RD_AMOUNT) * amount_scale;
+
+	if (wd_rd != NULL)
+		*wd_rd = REG_READ(RKNPU_OFFSET_WT_RD_AMOUNT) * amount_scale;
+
+	spin_unlock(&rknpu_dev->lock);
+
+	return 0;
+}
+
+int rknpu_get_total_rw_amount(struct rknpu_device *rknpu_dev, uint32_t *amount)
+{
+	uint32_t dt_wr = 0;
+	uint32_t dt_rd = 0;
+	uint32_t wd_rd = 0;
+	int ret = -EINVAL;
+
+	if (!rknpu_dev->config->bw_enable) {
+		LOG_WARN(
+			"Get total_rw_amount is not supported on this device!\n");
+		return 0;
+	}
+
+	ret = rknpu_get_rw_amount(rknpu_dev, &dt_wr, &dt_rd, &wd_rd);
+
+	if (amount != NULL)
+		*amount = dt_wr + dt_rd + wd_rd;
+
+	return ret;
+}
diff --git a/drivers/rknpu/rknpu_mem.c b/drivers/rknpu/rknpu_mem.c
new file mode 100644
index 000000000..a3bf98256
--- /dev/null
+++ b/drivers/rknpu/rknpu_mem.c
@@ -0,0 +1,242 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#include <linux/version.h>
+#include <linux/rk-dma-heap.h>
+
+#if KERNEL_VERSION(5, 10, 0) <= LINUX_VERSION_CODE
+#include <linux/dma-map-ops.h>
+#endif
+
+#include "rknpu_drv.h"
+#include "rknpu_ioctl.h"
+#include "rknpu_mem.h"
+
+int rknpu_mem_create_ioctl(struct rknpu_device *rknpu_dev, unsigned long data)
+{
+	struct rknpu_mem_create args;
+	int ret = -EINVAL;
+	struct dma_buf_attachment *attachment;
+	struct sg_table *table;
+	struct scatterlist *sgl;
+	dma_addr_t phys;
+	struct dma_buf *dmabuf;
+	struct page **pages;
+	struct page *page;
+	struct rknpu_mem_object *rknpu_obj = NULL;
+	int i, fd;
+	unsigned int length, page_count;
+
+	if (unlikely(copy_from_user(&args, (struct rknpu_mem_create *)data,
+				    sizeof(struct rknpu_mem_create)))) {
+		LOG_ERROR("%s: copy_from_user failed\n", __func__);
+		ret = -EFAULT;
+		return ret;
+	}
+
+	if (args.flags & RKNPU_MEM_NON_CONTIGUOUS) {
+		LOG_ERROR("%s: malloc iommu memory unsupported in current!\n",
+			  __func__);
+		ret = -EINVAL;
+		return ret;
+	}
+
+	rknpu_obj = kzalloc(sizeof(*rknpu_obj), GFP_KERNEL);
+	if (!rknpu_obj)
+		return -ENOMEM;
+
+	if (args.handle > 0) {
+		fd = args.handle;
+
+		dmabuf = dma_buf_get(fd);
+		if (IS_ERR(dmabuf)) {
+			ret = PTR_ERR(dmabuf);
+			goto err_free_obj;
+		}
+
+		rknpu_obj->dmabuf = dmabuf;
+		rknpu_obj->owner = 0;
+	} else {
+		/* Start test kernel alloc/free dma buf */
+		dmabuf = rk_dma_heap_buffer_alloc(rknpu_dev->heap, args.size,
+						  O_CLOEXEC | O_RDWR, 0x0,
+						  dev_name(rknpu_dev->dev));
+		if (IS_ERR(dmabuf)) {
+			ret = PTR_ERR(dmabuf);
+			goto err_free_obj;
+		}
+
+		rknpu_obj->dmabuf = dmabuf;
+		rknpu_obj->owner = 1;
+
+		fd = dma_buf_fd(dmabuf, O_CLOEXEC | O_RDWR);
+		if (fd < 0) {
+			ret = -EFAULT;
+			goto err_free_dma_buf;
+		}
+	}
+
+	attachment = dma_buf_attach(dmabuf, rknpu_dev->dev);
+	if (IS_ERR(attachment)) {
+		ret = PTR_ERR(attachment);
+		goto err_free_dma_buf;
+	}
+
+	table = dma_buf_map_attachment(attachment, DMA_BIDIRECTIONAL);
+	if (IS_ERR(table)) {
+		dma_buf_detach(dmabuf, attachment);
+		ret = PTR_ERR(table);
+		goto err_free_dma_buf;
+	}
+
+	for_each_sgtable_sg(table, sgl, i) {
+		phys = sg_dma_address(sgl);
+		page = sg_page(sgl);
+		length = sg_dma_len(sgl);
+		LOG_DEBUG("%s, %d, phys: %pad, length: %u\n", __func__,
+			  __LINE__, &phys, length);
+	}
+
+	page_count = length >> PAGE_SHIFT;
+	pages = kmalloc_array(page_count, sizeof(struct page), GFP_KERNEL);
+	if (!pages) {
+		ret = -ENOMEM;
+		goto err_detach_dma_buf;
+	}
+
+	for (i = 0; i < page_count; i++)
+		pages[i] = &page[i];
+
+	rknpu_obj->kv_addr = vmap(pages, page_count, VM_MAP, PAGE_KERNEL);
+	if (!rknpu_obj->kv_addr) {
+		ret = -ENOMEM;
+		goto err_free_pages;
+	}
+
+	rknpu_obj->size = PAGE_ALIGN(args.size);
+	rknpu_obj->dma_addr = phys;
+	rknpu_obj->sgt = table;
+
+	args.size = rknpu_obj->size;
+	args.obj_addr = (__u64)(uintptr_t)rknpu_obj;
+	args.dma_addr = rknpu_obj->dma_addr;
+	args.handle = fd;
+
+	LOG_DEBUG(
+		"args.handle: %d, args.size: %lld, rknpu_obj: %#llx, rknpu_obj->dma_addr: %#llx\n",
+		args.handle, args.size, (__u64)(uintptr_t)rknpu_obj,
+		(__u64)rknpu_obj->dma_addr);
+
+	if (unlikely(copy_to_user((struct rknpu_mem_create *)data, &args,
+				  sizeof(struct rknpu_mem_create)))) {
+		LOG_ERROR("%s: copy_to_user failed\n", __func__);
+		ret = -EFAULT;
+		goto err_unmap_kv_addr;
+	}
+
+	kfree(pages);
+	dma_buf_unmap_attachment(attachment, table, DMA_BIDIRECTIONAL);
+	dma_buf_detach(dmabuf, attachment);
+
+	return 0;
+
+err_unmap_kv_addr:
+	vunmap(rknpu_obj->kv_addr);
+	rknpu_obj->kv_addr = NULL;
+
+err_free_pages:
+	kfree(pages);
+
+err_detach_dma_buf:
+	dma_buf_unmap_attachment(attachment, table, DMA_BIDIRECTIONAL);
+	dma_buf_detach(dmabuf, attachment);
+
+err_free_dma_buf:
+	if (rknpu_obj->owner)
+		rk_dma_heap_buffer_free(dmabuf);
+	else
+		dma_buf_put(dmabuf);
+
+err_free_obj:
+	kfree(rknpu_obj);
+
+	return ret;
+}
+
+int rknpu_mem_destroy_ioctl(struct rknpu_device *rknpu_dev, unsigned long data)
+{
+	struct rknpu_mem_object *rknpu_obj = NULL;
+	struct rknpu_mem_destroy args;
+	struct dma_buf *dmabuf;
+	int ret = -EFAULT;
+
+	if (unlikely(copy_from_user(&args, (struct rknpu_mem_destroy *)data,
+				    sizeof(struct rknpu_mem_destroy)))) {
+		LOG_ERROR("%s: copy_from_user failed\n", __func__);
+		ret = -EFAULT;
+		return ret;
+	}
+
+	if (!kern_addr_valid(args.obj_addr)) {
+		LOG_ERROR("%s: invalid obj_addr: %#llx\n", __func__,
+			  (__u64)(uintptr_t)args.obj_addr);
+		ret = -EINVAL;
+		return ret;
+	}
+
+	rknpu_obj = (struct rknpu_mem_object *)(uintptr_t)args.obj_addr;
+	dmabuf = rknpu_obj->dmabuf;
+	LOG_DEBUG(
+		"free args.handle: %d, rknpu_obj: %#llx, rknpu_obj->dma_addr: %#llx\n",
+		args.handle, (__u64)(uintptr_t)rknpu_obj,
+		(__u64)rknpu_obj->dma_addr);
+
+	vunmap(rknpu_obj->kv_addr);
+	rknpu_obj->kv_addr = NULL;
+
+	if (!rknpu_obj->owner)
+		dma_buf_put(dmabuf);
+
+	kfree(rknpu_obj);
+
+	return 0;
+}
+
+int rknpu_mem_sync_ioctl(struct rknpu_device *rknpu_dev, unsigned long data)
+{
+	struct rknpu_mem_object *rknpu_obj = NULL;
+	struct rknpu_mem_sync args;
+	struct dma_buf *dmabuf;
+	int ret = -EFAULT;
+
+	if (unlikely(copy_from_user(&args, (struct rknpu_mem_sync *)data,
+				    sizeof(struct rknpu_mem_sync)))) {
+		LOG_ERROR("%s: copy_from_user failed\n", __func__);
+		ret = -EFAULT;
+		return ret;
+	}
+
+	if (!kern_addr_valid(args.obj_addr)) {
+		LOG_ERROR("%s: invalid obj_addr: %#llx\n", __func__,
+			  (__u64)(uintptr_t)args.obj_addr);
+		ret = -EINVAL;
+		return ret;
+	}
+
+	rknpu_obj = (struct rknpu_mem_object *)(uintptr_t)args.obj_addr;
+	dmabuf = rknpu_obj->dmabuf;
+
+	if (args.flags & RKNPU_MEM_SYNC_TO_DEVICE) {
+		dmabuf->ops->end_cpu_access_partial(dmabuf, DMA_TO_DEVICE,
+						    args.offset, args.size);
+	}
+	if (args.flags & RKNPU_MEM_SYNC_FROM_DEVICE) {
+		dmabuf->ops->begin_cpu_access_partial(dmabuf, DMA_FROM_DEVICE,
+						      args.offset, args.size);
+	}
+
+	return 0;
+}
diff --git a/drivers/rknpu/rknpu_mm.c b/drivers/rknpu/rknpu_mm.c
new file mode 100644
index 000000000..9a13c3e25
--- /dev/null
+++ b/drivers/rknpu/rknpu_mm.c
@@ -0,0 +1,289 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#include "rknpu_debugger.h"
+#include "rknpu_mm.h"
+
+int rknpu_mm_create(unsigned int mem_size, unsigned int chunk_size,
+		    struct rknpu_mm **mm)
+{
+	unsigned int num_of_longs;
+	int ret = -EINVAL;
+
+	if (WARN_ON(mem_size < chunk_size))
+		return -EINVAL;
+	if (WARN_ON(mem_size == 0))
+		return -EINVAL;
+	if (WARN_ON(chunk_size == 0))
+		return -EINVAL;
+
+	*mm = kzalloc(sizeof(struct rknpu_mm), GFP_KERNEL);
+	if (!(*mm))
+		return -ENOMEM;
+
+	(*mm)->chunk_size = chunk_size;
+	(*mm)->total_chunks = mem_size / chunk_size;
+	(*mm)->free_chunks = (*mm)->total_chunks;
+
+	num_of_longs =
+		((*mm)->total_chunks + BITS_PER_LONG - 1) / BITS_PER_LONG;
+
+	(*mm)->bitmap = kcalloc(num_of_longs, sizeof(long), GFP_KERNEL);
+	if (!(*mm)->bitmap) {
+		ret = -ENOMEM;
+		goto free_mm;
+	}
+
+	mutex_init(&(*mm)->lock);
+
+	LOG_DEBUG("total_chunks: %d, bitmap: %p\n", (*mm)->total_chunks,
+		  (*mm)->bitmap);
+
+	return 0;
+
+free_mm:
+	kfree(mm);
+	return ret;
+}
+
+void rknpu_mm_destroy(struct rknpu_mm *mm)
+{
+	if (mm != NULL) {
+		mutex_destroy(&mm->lock);
+		kfree(mm->bitmap);
+		kfree(mm);
+	}
+}
+
+int rknpu_mm_alloc(struct rknpu_mm *mm, unsigned int size,
+		   struct rknpu_mm_obj **mm_obj)
+{
+	unsigned int found, start_search, cur_size;
+
+	if (size == 0)
+		return -EINVAL;
+
+	if (size > mm->total_chunks * mm->chunk_size)
+		return -ENOMEM;
+
+	*mm_obj = kzalloc(sizeof(struct rknpu_mm_obj), GFP_KERNEL);
+	if (!(*mm_obj))
+		return -ENOMEM;
+
+	start_search = 0;
+
+	mutex_lock(&mm->lock);
+
+mm_restart_search:
+	/* Find the first chunk that is free */
+	found = find_next_zero_bit(mm->bitmap, mm->total_chunks, start_search);
+
+	/* If there wasn't any free chunk, bail out */
+	if (found == mm->total_chunks)
+		goto mm_no_free_chunk;
+
+	/* Update fields of mm_obj */
+	(*mm_obj)->range_start = found;
+	(*mm_obj)->range_end = found;
+
+	/* If we need only one chunk, mark it as allocated and get out */
+	if (size <= mm->chunk_size) {
+		set_bit(found, mm->bitmap);
+		goto mm_out;
+	}
+
+	/* Otherwise, try to see if we have enough contiguous chunks */
+	cur_size = size - mm->chunk_size;
+	do {
+		(*mm_obj)->range_end = find_next_zero_bit(
+			mm->bitmap, mm->total_chunks, ++found);
+		/*
+		 * If next free chunk is not contiguous than we need to
+		 * restart our search from the last free chunk we found (which
+		 * wasn't contiguous to the previous ones
+		 */
+		if ((*mm_obj)->range_end != found) {
+			start_search = found;
+			goto mm_restart_search;
+		}
+
+		/*
+		 * If we reached end of buffer, bail out with error
+		 */
+		if (found == mm->total_chunks)
+			goto mm_no_free_chunk;
+
+		/* Check if we don't need another chunk */
+		if (cur_size <= mm->chunk_size)
+			cur_size = 0;
+		else
+			cur_size -= mm->chunk_size;
+
+	} while (cur_size > 0);
+
+	/* Mark the chunks as allocated */
+	for (found = (*mm_obj)->range_start; found <= (*mm_obj)->range_end;
+	     found++)
+		set_bit(found, mm->bitmap);
+
+mm_out:
+	mm->free_chunks -= ((*mm_obj)->range_end - (*mm_obj)->range_start + 1);
+	mutex_unlock(&mm->lock);
+
+	LOG_DEBUG("mm allocate, mm_obj: %p, range_start: %d, range_end: %d\n",
+		  *mm_obj, (*mm_obj)->range_start, (*mm_obj)->range_end);
+
+	return 0;
+
+mm_no_free_chunk:
+	mutex_unlock(&mm->lock);
+	kfree(*mm_obj);
+
+	return -ENOMEM;
+}
+
+int rknpu_mm_free(struct rknpu_mm *mm, struct rknpu_mm_obj *mm_obj)
+{
+	unsigned int bit;
+
+	/* Act like kfree when trying to free a NULL object */
+	if (!mm_obj)
+		return 0;
+
+	LOG_DEBUG("mm free, mem_obj: %p, range_start: %d, range_end: %d\n",
+		  mm_obj, mm_obj->range_start, mm_obj->range_end);
+
+	mutex_lock(&mm->lock);
+
+	/* Mark the chunks as free */
+	for (bit = mm_obj->range_start; bit <= mm_obj->range_end; bit++)
+		clear_bit(bit, mm->bitmap);
+
+	mm->free_chunks += (mm_obj->range_end - mm_obj->range_start + 1);
+
+	mutex_unlock(&mm->lock);
+
+	kfree(mm_obj);
+
+	return 0;
+}
+
+int rknpu_mm_dump(struct seq_file *m, void *data)
+{
+	struct rknpu_debugger_node *node = m->private;
+	struct rknpu_debugger *debugger = node->debugger;
+	struct rknpu_device *rknpu_dev =
+		container_of(debugger, struct rknpu_device, debugger);
+	struct rknpu_mm *mm = NULL;
+	int cur = 0, rbot = 0, rtop = 0;
+	size_t ret = 0;
+	char buf[64];
+	size_t size = sizeof(buf);
+	int seg_chunks = 32, seg_id = 0;
+	int free_size = 0;
+	int i = 0;
+
+	mm = rknpu_dev->sram_mm;
+	if (mm == NULL)
+		return 0;
+
+	seq_printf(m, "SRAM bitmap: \"*\" - used, \".\" - free (1bit = %dKB)\n",
+		   mm->chunk_size / 1024);
+
+	rbot = cur = find_first_bit(mm->bitmap, mm->total_chunks);
+	for (i = 0; i < cur; ++i) {
+		ret += scnprintf(buf + ret, size - ret, ".");
+		if (ret >= seg_chunks) {
+			seq_printf(m, "[%03d] [%s]\n", seg_id++, buf);
+			ret = 0;
+		}
+	}
+	while (cur < mm->total_chunks) {
+		rtop = cur;
+		cur = find_next_bit(mm->bitmap, mm->total_chunks, cur + 1);
+		if (cur < mm->total_chunks && cur <= rtop + 1)
+			continue;
+
+		for (i = rbot; i <= rtop; ++i) {
+			ret += scnprintf(buf + ret, size - ret, "*");
+			if (ret >= seg_chunks) {
+				seq_printf(m, "[%03d] [%s]\n", seg_id++, buf);
+				ret = 0;
+			}
+		}
+
+		for (i = rtop + 1; i < cur; ++i) {
+			ret += scnprintf(buf + ret, size - ret, ".");
+			if (ret >= seg_chunks) {
+				seq_printf(m, "[%03d] [%s]\n", seg_id++, buf);
+				ret = 0;
+			}
+		}
+
+		rbot = cur;
+	}
+
+	if (ret > 0)
+		seq_printf(m, "[%03d] [%s]\n", seg_id++, buf);
+
+	free_size = mm->free_chunks * mm->chunk_size;
+	seq_printf(m, "SRAM total size: %d, used: %d, free: %d\n",
+		   rknpu_dev->sram_size, rknpu_dev->sram_size - free_size,
+		   free_size);
+
+	return 0;
+}
+
+dma_addr_t rknpu_iommu_dma_alloc_iova(struct iommu_domain *domain, size_t size,
+				      u64 dma_limit, struct device *dev)
+{
+	struct rknpu_iommu_dma_cookie *cookie = domain->iova_cookie;
+	struct iova_domain *iovad = &cookie->iovad;
+	unsigned long shift, iova_len, iova = 0;
+#if (KERNEL_VERSION(5, 4, 0) > LINUX_VERSION_CODE)
+	dma_addr_t limit;
+#endif
+
+	shift = iova_shift(iovad);
+	iova_len = size >> shift;
+	/*
+	 * Freeing non-power-of-two-sized allocations back into the IOVA caches
+	 * will come back to bite us badly, so we have to waste a bit of space
+	 * rounding up anything cacheable to make sure that can't happen. The
+	 * order of the unadjusted size will still match upon freeing.
+	 */
+	if (iova_len < (1 << (IOVA_RANGE_CACHE_MAX_SIZE - 1)))
+		iova_len = roundup_pow_of_two(iova_len);
+
+#if (KERNEL_VERSION(5, 10, 0) <= LINUX_VERSION_CODE)
+	dma_limit = min_not_zero(dma_limit, dev->bus_dma_limit);
+#else
+	if (dev->bus_dma_mask)
+		dma_limit &= dev->bus_dma_mask;
+#endif
+
+	if (domain->geometry.force_aperture)
+		dma_limit =
+			min_t(u64, dma_limit, domain->geometry.aperture_end);
+
+#if (KERNEL_VERSION(5, 4, 0) <= LINUX_VERSION_CODE)
+	iova = alloc_iova_fast(iovad, iova_len, dma_limit >> shift, true);
+#else
+	limit = min_t(dma_addr_t, dma_limit >> shift, iovad->end_pfn);
+
+	iova = alloc_iova_fast(iovad, iova_len, limit, true);
+#endif
+
+	return (dma_addr_t)iova << shift;
+}
+
+void rknpu_iommu_dma_free_iova(struct rknpu_iommu_dma_cookie *cookie,
+			       dma_addr_t iova, size_t size)
+{
+	struct iova_domain *iovad = &cookie->iovad;
+
+	free_iova_fast(iovad, iova_pfn(iovad, iova), size >> iova_shift(iovad));
+}
diff --git a/drivers/rknpu/rknpu_reset.c b/drivers/rknpu/rknpu_reset.c
new file mode 100644
index 000000000..91c9b75d6
--- /dev/null
+++ b/drivers/rknpu/rknpu_reset.c
@@ -0,0 +1,148 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#include <linux/delay.h>
+#include <linux/iommu.h>
+
+#include "rknpu_reset.h"
+
+#ifndef FPGA_PLATFORM
+static inline struct reset_control *rknpu_reset_control_get(struct device *dev,
+							    const char *name)
+{
+	struct reset_control *rst = NULL;
+
+	rst = devm_reset_control_get(dev, name);
+	if (IS_ERR(rst))
+		LOG_DEV_ERROR(dev,
+			      "failed to get rknpu reset control: %s, %ld\n",
+			      name, PTR_ERR(rst));
+
+	return rst;
+}
+#endif
+
+int rknpu_reset_get(struct rknpu_device *rknpu_dev)
+{
+#ifndef FPGA_PLATFORM
+	struct reset_control *srst_a = NULL;
+	struct reset_control *srst_h = NULL;
+	int i = 0;
+
+	for (i = 0; i < rknpu_dev->config->num_resets; i++) {
+		srst_a = rknpu_reset_control_get(
+			rknpu_dev->dev,
+			rknpu_dev->config->resets[i].srst_a_name);
+		if (IS_ERR(srst_a))
+			return PTR_ERR(srst_a);
+
+		rknpu_dev->srst_a[i] = srst_a;
+
+		srst_h = rknpu_reset_control_get(
+			rknpu_dev->dev,
+			rknpu_dev->config->resets[i].srst_h_name);
+		if (IS_ERR(srst_h))
+			return PTR_ERR(srst_h);
+
+		rknpu_dev->srst_h[i] = srst_h;
+	}
+#endif
+
+	return 0;
+}
+
+#ifndef FPGA_PLATFORM
+static int rknpu_reset_assert(struct reset_control *rst)
+{
+	int ret = -EINVAL;
+
+	if (!rst)
+		return -EINVAL;
+
+	ret = reset_control_assert(rst);
+	if (ret < 0) {
+		LOG_ERROR("failed to assert rknpu reset: %d\n", ret);
+		return ret;
+	}
+
+	return 0;
+}
+
+static int rknpu_reset_deassert(struct reset_control *rst)
+{
+	int ret = -EINVAL;
+
+	if (!rst)
+		return -EINVAL;
+
+	ret = reset_control_deassert(rst);
+	if (ret < 0) {
+		LOG_ERROR("failed to deassert rknpu reset: %d\n", ret);
+		return ret;
+	}
+
+	return 0;
+}
+#endif
+
+int rknpu_soft_reset(struct rknpu_device *rknpu_dev)
+{
+#ifndef FPGA_PLATFORM
+	struct iommu_domain *domain = NULL;
+	struct rknpu_subcore_data *subcore_data = NULL;
+	int ret = -EINVAL, i = 0;
+
+	if (rknpu_dev->bypass_soft_reset) {
+		LOG_WARN("bypass soft reset\n");
+		return 0;
+	}
+
+	if (!mutex_trylock(&rknpu_dev->reset_lock))
+		return 0;
+
+	rknpu_dev->soft_reseting = true;
+
+	msleep(100);
+
+	for (i = 0; i < rknpu_dev->config->num_irqs; ++i) {
+		subcore_data = &rknpu_dev->subcore_datas[i];
+		wake_up(&subcore_data->job_done_wq);
+	}
+
+	LOG_INFO("soft reset\n");
+
+	for (i = 0; i < rknpu_dev->config->num_resets; i++) {
+		ret = rknpu_reset_assert(rknpu_dev->srst_a[i]);
+		ret |= rknpu_reset_assert(rknpu_dev->srst_h[i]);
+
+		udelay(10);
+
+		ret |= rknpu_reset_deassert(rknpu_dev->srst_a[i]);
+		ret |= rknpu_reset_deassert(rknpu_dev->srst_h[i]);
+	}
+
+	if (ret) {
+		LOG_DEV_ERROR(rknpu_dev->dev,
+			      "failed to soft reset for rknpu: %d\n", ret);
+		mutex_unlock(&rknpu_dev->reset_lock);
+		return ret;
+	}
+
+	if (rknpu_dev->iommu_en)
+		domain = iommu_get_domain_for_dev(rknpu_dev->dev);
+
+	if (domain) {
+		iommu_detach_device(domain, rknpu_dev->dev);
+		iommu_attach_device(domain, rknpu_dev->dev);
+	}
+
+	rknpu_dev->soft_reseting = false;
+
+	mutex_unlock(&rknpu_dev->reset_lock);
+#endif
+
+	return 0;
+}
diff --git a/drivers/soc/rockchip/Kconfig b/drivers/soc/rockchip/Kconfig
index aff2f7e95..9871fe046 100644
--- a/drivers/soc/rockchip/Kconfig
+++ b/drivers/soc/rockchip/Kconfig
@@ -43,3 +43,9 @@ config ROCKCHIP_DTPM
 	  devices.
 
 endif
+
+config ROCKCHIP_OPP
+	tristate "Rockchip OPP select support"
+	depends on PM_DEVFREQ
+	help
+	  Say y here to enable rockchip OPP support.
\ No newline at end of file
diff --git a/drivers/soc/rockchip/Makefile b/drivers/soc/rockchip/Makefile
index 23d414433..8f3c243c1 100644
--- a/drivers/soc/rockchip/Makefile
+++ b/drivers/soc/rockchip/Makefile
@@ -5,3 +5,4 @@
 obj-$(CONFIG_ROCKCHIP_GRF) += grf.o
 obj-$(CONFIG_ROCKCHIP_IODOMAIN) += io-domain.o
 obj-$(CONFIG_ROCKCHIP_DTPM) += dtpm.o
+obj-$(CONFIG_ROCKCHIP_OPP) += rockchip_opp_select.o
\ No newline at end of file
diff --git a/drivers/soc/rockchip/rockchip_opp_select.c b/drivers/soc/rockchip/rockchip_opp_select.c
new file mode 100644
index 000000000..f0410afa7
--- /dev/null
+++ b/drivers/soc/rockchip/rockchip_opp_select.c
@@ -0,0 +1,1746 @@
+/*
+ * Copyright (c) 2017 Fuzhou Rockchip Electronics Co., Ltd
+ *
+ * SPDX-License-Identifier: GPL-2.0+
+ */
+//#define DEBUG
+#include <linux/clk.h>
+#include <linux/cpufreq.h>
+#include <linux/devfreq.h>
+#include <linux/mfd/syscon.h>
+#include <linux/module.h>
+#include <linux/nvmem-consumer.h>
+#include <linux/regmap.h>
+#include <linux/regulator/consumer.h>
+#include <linux/slab.h>
+#include <linux/soc/rockchip/pvtm.h>
+#include <linux/thermal.h>
+#include <linux/pm_opp.h>
+#include <linux/version.h>
+#include <soc/rockchip/rockchip_opp_select.h>
+
+#include "../../clk/rockchip/clk.h"
+#include "../../opp/opp.h"
+#include "../../devfreq/governor.h"
+
+#define MAX_PROP_NAME_LEN	6
+#define SEL_TABLE_END		~1
+#define AVS_DELETE_OPP		0
+#define AVS_SCALING_RATE	1
+
+#define LEAKAGE_V1		1
+#define LEAKAGE_V2		2
+#define LEAKAGE_V3		3
+
+#define to_thermal_opp_info(nb) container_of(nb, struct thermal_opp_info, \
+					     thermal_nb)
+
+struct sel_table {
+	int min;
+	int max;
+	int sel;
+};
+
+struct bin_sel_table {
+	int bin;
+	int sel;
+};
+
+struct pvtm_config {
+	unsigned int freq;
+	unsigned int volt;
+	unsigned int ch[2];
+	unsigned int sample_time;
+	unsigned int num;
+	unsigned int err;
+	unsigned int ref_temp;
+	unsigned int offset;
+	int temp_prop[2];
+	const char *tz_name;
+	struct thermal_zone_device *tz;
+	struct regmap *grf;
+};
+
+struct lkg_conversion_table {
+	int temp;
+	int conv;
+};
+
+#define PVTM_CH_MAX	8
+#define PVTM_SUB_CH_MAX	8
+
+#define FRAC_BITS 10
+#define int_to_frac(x) ((x) << FRAC_BITS)
+#define frac_to_int(x) ((x) >> FRAC_BITS)
+
+static int pvtm_value[PVTM_CH_MAX][PVTM_SUB_CH_MAX];
+static int lkg_version;
+
+/*
+ * temp = temp * 10
+ * conv = exp(-ln(1.2) / 5 * (temp - 23)) * 100
+ */
+static const struct lkg_conversion_table conv_table[] = {
+	{ 200, 111 },
+	{ 205, 109 },
+	{ 210, 107 },
+	{ 215, 105 },
+	{ 220, 103 },
+	{ 225, 101 },
+	{ 230, 100 },
+	{ 235, 98 },
+	{ 240, 96 },
+	{ 245, 94 },
+	{ 250, 92 },
+	{ 255, 91 },
+	{ 260, 89 },
+	{ 265, 88 },
+	{ 270, 86 },
+	{ 275, 84 },
+	{ 280, 83 },
+	{ 285, 81 },
+	{ 290, 80 },
+	{ 295, 78 },
+	{ 300, 77 },
+	{ 305, 76 },
+	{ 310, 74 },
+	{ 315, 73 },
+	{ 320, 72 },
+	{ 325, 70 },
+	{ 330, 69 },
+	{ 335, 68 },
+	{ 340, 66 },
+	{ 345, 65 },
+	{ 350, 64 },
+	{ 355, 63 },
+	{ 360, 62 },
+	{ 365, 61 },
+	{ 370, 60 },
+	{ 375, 58 },
+	{ 380, 57 },
+	{ 385, 56 },
+	{ 390, 55 },
+	{ 395, 54 },
+	{ 400, 53 },
+};
+
+static int rockchip_nvmem_cell_read_common(struct device_node *np,
+					   const char *cell_id,
+					   void *val, size_t count)
+{
+	struct nvmem_cell *cell;
+	void *buf;
+	size_t len;
+
+	cell = of_nvmem_cell_get(np, cell_id);
+	if (IS_ERR(cell))
+		return PTR_ERR(cell);
+
+	buf = nvmem_cell_read(cell, &len);
+	if (IS_ERR(buf)) {
+		nvmem_cell_put(cell);
+		return PTR_ERR(buf);
+	}
+	if (len != count) {
+		kfree(buf);
+		nvmem_cell_put(cell);
+		return -EINVAL;
+	}
+	memcpy(val, buf, count);
+	kfree(buf);
+	nvmem_cell_put(cell);
+
+	return 0;
+}
+
+int rockchip_nvmem_cell_read_u8(struct device_node *np, const char *cell_id,
+				u8 *val)
+{
+	return rockchip_nvmem_cell_read_common(np, cell_id, val, sizeof(*val));
+}
+EXPORT_SYMBOL(rockchip_nvmem_cell_read_u8);
+
+int rockchip_nvmem_cell_read_u16(struct device_node *np, const char *cell_id,
+				 u16 *val)
+{
+	return rockchip_nvmem_cell_read_common(np, cell_id, val, sizeof(*val));
+}
+EXPORT_SYMBOL(rockchip_nvmem_cell_read_u16);
+
+static int rockchip_get_sel_table(struct device_node *np, char *porp_name,
+				  struct sel_table **table)
+{
+	struct sel_table *sel_table;
+	const struct property *prop;
+	int count, i;
+
+	prop = of_find_property(np, porp_name, NULL);
+	if (!prop)
+		return -EINVAL;
+
+	if (!prop->value)
+		return -ENODATA;
+
+	count = of_property_count_u32_elems(np, porp_name);
+	if (count < 0)
+		return -EINVAL;
+
+	if (count % 3)
+		return -EINVAL;
+
+	sel_table = kzalloc(sizeof(*sel_table) * (count / 3 + 1), GFP_KERNEL);
+	if (!sel_table)
+		return -ENOMEM;
+
+	for (i = 0; i < count / 3; i++) {
+		of_property_read_u32_index(np, porp_name, 3 * i,
+					   &sel_table[i].min);
+		of_property_read_u32_index(np, porp_name, 3 * i + 1,
+					   &sel_table[i].max);
+		of_property_read_u32_index(np, porp_name, 3 * i + 2,
+					   &sel_table[i].sel);
+	}
+	sel_table[i].min = 0;
+	sel_table[i].max = 0;
+	sel_table[i].sel = SEL_TABLE_END;
+
+	*table = sel_table;
+
+	return 0;
+}
+
+static int rockchip_get_bin_sel_table(struct device_node *np, char *porp_name,
+				      struct bin_sel_table **table)
+{
+	struct bin_sel_table *sel_table;
+	const struct property *prop;
+	int count, i;
+
+	prop = of_find_property(np, porp_name, NULL);
+	if (!prop)
+		return -EINVAL;
+
+	if (!prop->value)
+		return -ENODATA;
+
+	count = of_property_count_u32_elems(np, porp_name);
+	if (count < 0)
+		return -EINVAL;
+
+	if (count % 2)
+		return -EINVAL;
+
+	sel_table = kzalloc(sizeof(*sel_table) * (count / 2 + 1), GFP_KERNEL);
+	if (!sel_table)
+		return -ENOMEM;
+
+	for (i = 0; i < count / 2; i++) {
+		of_property_read_u32_index(np, porp_name, 2 * i,
+					   &sel_table[i].bin);
+		of_property_read_u32_index(np, porp_name, 2 * i + 1,
+					   &sel_table[i].sel);
+	}
+
+	sel_table[i].bin = 0;
+	sel_table[i].sel = SEL_TABLE_END;
+
+	*table = sel_table;
+
+	return 0;
+}
+
+static int rockchip_get_sel(struct device_node *np, char *name,
+			    int value, int *sel)
+{
+	struct sel_table *table = NULL;
+	int i, ret = -EINVAL;
+
+	if (!sel)
+		return -EINVAL;
+
+	if (rockchip_get_sel_table(np, name, &table))
+		return -EINVAL;
+
+	for (i = 0; table[i].sel != SEL_TABLE_END; i++) {
+		if (value >= table[i].min) {
+			*sel = table[i].sel;
+			ret = 0;
+		}
+	}
+	kfree(table);
+
+	return ret;
+}
+
+static int rockchip_get_bin_sel(struct device_node *np, char *name,
+				int value, int *sel)
+{
+	struct bin_sel_table *table = NULL;
+	int i, ret = -EINVAL;
+
+	if (!sel)
+		return -EINVAL;
+
+	if (rockchip_get_bin_sel_table(np, name, &table))
+		return -EINVAL;
+
+	for (i = 0; table[i].sel != SEL_TABLE_END; i++) {
+		if (value == table[i].bin) {
+			*sel = table[i].sel;
+			ret = 0;
+			break;
+		}
+	}
+	kfree(table);
+
+	return ret;
+}
+
+static int rockchip_parse_pvtm_config(struct device_node *np,
+				      struct pvtm_config *pvtm)
+{
+	if (of_property_read_u32(np, "rockchip,pvtm-freq", &pvtm->freq))
+		return -EINVAL;
+	if (of_property_read_u32(np, "rockchip,pvtm-volt", &pvtm->volt))
+		return -EINVAL;
+	if (of_property_read_u32(np, "rockchip,pvtm-sample-time",
+				 &pvtm->sample_time))
+		return -EINVAL;
+	if (of_property_read_u32(np, "rockchip,pvtm-ref-temp", &pvtm->ref_temp))
+		return -EINVAL;
+	if (of_property_read_u32_array(np, "rockchip,pvtm-temp-prop",
+				       pvtm->temp_prop, 2))
+		return -EINVAL;
+	if (of_property_read_string(np, "rockchip,pvtm-thermal-zone",
+				    &pvtm->tz_name)) {
+		if (of_property_read_string(np, "rockchip,thermal-zone",
+					    &pvtm->tz_name))
+			return -EINVAL;
+	}
+	pvtm->tz = thermal_zone_get_zone_by_name(pvtm->tz_name);
+	if (IS_ERR(pvtm->tz))
+		return -EINVAL;
+	if (!pvtm->tz->ops->get_temp)
+		return -EINVAL;
+	if (of_property_read_bool(np, "rockchip,pvtm-pvtpll")) {
+		if (of_property_read_u32(np, "rockchip,pvtm-offset",
+					 &pvtm->offset))
+			return -EINVAL;
+		pvtm->grf = syscon_regmap_lookup_by_phandle(np, "rockchip,grf");
+		if (IS_ERR(pvtm->grf))
+			return -EINVAL;
+		return 0;
+	}
+	if (of_property_read_u32_array(np, "rockchip,pvtm-ch", pvtm->ch, 2))
+		return -EINVAL;
+	if (pvtm->ch[0] >= PVTM_CH_MAX || pvtm->ch[1] >= PVTM_SUB_CH_MAX)
+		return -EINVAL;
+	if (of_property_read_u32(np, "rockchip,pvtm-number", &pvtm->num))
+		return -EINVAL;
+	if (of_property_read_u32(np, "rockchip,pvtm-error", &pvtm->err))
+		return -EINVAL;
+
+	return 0;
+}
+
+static int rockchip_get_pvtm_specific_value(struct device *dev,
+					    struct device_node *np,
+					    struct clk *clk,
+					    struct regulator *reg,
+					    int *target_value)
+{
+	struct pvtm_config *pvtm;
+	unsigned long old_freq;
+	unsigned int old_volt;
+	int cur_temp, diff_temp;
+	int cur_value, total_value, avg_value, diff_value;
+	int min_value, max_value;
+	int ret = 0, i = 0, retry = 2;
+
+	pvtm = kzalloc(sizeof(*pvtm), GFP_KERNEL);
+	if (!pvtm)
+		return -ENOMEM;
+
+	ret = rockchip_parse_pvtm_config(np, pvtm);
+	if (ret)
+		goto pvtm_value_out;
+
+	old_freq = clk_get_rate(clk);
+	old_volt = regulator_get_voltage(reg);
+
+	/*
+	 * Set pvtm_freq to the lowest frequency in dts,
+	 * so change frequency first.
+	 */
+	ret = clk_set_rate(clk, pvtm->freq * 1000);
+	if (ret) {
+		dev_err(dev, "Failed to set pvtm freq\n");
+		goto pvtm_value_out;
+	}
+
+	ret = regulator_set_voltage(reg, pvtm->volt, pvtm->volt);
+	if (ret) {
+		dev_err(dev, "Failed to set pvtm_volt\n");
+		goto restore_clk;
+	}
+
+	/* The first few values may be fluctuant, if error is too big, retry*/
+	while (retry--) {
+		total_value = 0;
+		min_value = INT_MAX;
+		max_value = 0;
+
+		for (i = 0; i < pvtm->num; i++) {
+			cur_value = rockchip_get_pvtm_value(pvtm->ch[0],
+							    pvtm->ch[1],
+							    pvtm->sample_time);
+			if (cur_value <= 0) {
+				ret = -EINVAL;
+				goto resetore_volt;
+			}
+			if (cur_value < min_value)
+				min_value = cur_value;
+			if (cur_value > max_value)
+				max_value = cur_value;
+			total_value += cur_value;
+		}
+		if (max_value - min_value < pvtm->err)
+			break;
+	}
+	if (!total_value || !pvtm->num) {
+		ret = -EINVAL;
+		goto resetore_volt;
+	}
+	avg_value = total_value / pvtm->num;
+
+	/*
+	 * As pvtm is influenced by temperature, compute difference between
+	 * current temperature and reference temperature
+	 */
+	pvtm->tz->ops->get_temp(pvtm->tz, &cur_temp);
+	diff_temp = (cur_temp / 1000 - pvtm->ref_temp);
+	diff_value = diff_temp *
+		(diff_temp < 0 ? pvtm->temp_prop[0] : pvtm->temp_prop[1]);
+	*target_value = avg_value + diff_value;
+
+	pvtm_value[pvtm->ch[0]][pvtm->ch[1]] = *target_value;
+
+	dev_info(dev, "temp=%d, pvtm=%d (%d + %d)\n",
+		 cur_temp, *target_value, avg_value, diff_value);
+
+resetore_volt:
+	regulator_set_voltage(reg, old_volt, old_volt);
+restore_clk:
+	clk_set_rate(clk, old_freq);
+pvtm_value_out:
+	kfree(pvtm);
+
+	return ret;
+}
+
+/**
+ * mul_frac() - multiply two fixed-point numbers
+ * @x:	first multiplicand
+ * @y:	second multiplicand
+ *
+ * Return: the result of multiplying two fixed-point numbers.  The
+ * result is also a fixed-point number.
+ */
+static inline s64 mul_frac(s64 x, s64 y)
+{
+	return (x * y) >> FRAC_BITS;
+}
+
+static int temp_to_conversion_rate(int temp)
+{
+	int high, low, mid;
+
+	low = 0;
+	high = ARRAY_SIZE(conv_table) - 1;
+	mid = (high + low) / 2;
+
+	/* No temp available, return max conversion_rate */
+	if (temp <= conv_table[low].temp)
+		return conv_table[low].conv;
+	if (temp >= conv_table[high].temp)
+		return conv_table[high].conv;
+
+	while (low <= high) {
+		if (temp <= conv_table[mid].temp && temp >
+		    conv_table[mid - 1].temp) {
+			return conv_table[mid - 1].conv +
+			    (conv_table[mid].conv - conv_table[mid - 1].conv) *
+			    (temp - conv_table[mid - 1].temp) /
+			    (conv_table[mid].temp - conv_table[mid - 1].temp);
+		} else if (temp > conv_table[mid].temp) {
+			low = mid + 1;
+		} else {
+			high = mid - 1;
+		}
+		mid = (low + high) / 2;
+	}
+
+	return 100;
+}
+
+static int rockchip_adjust_leakage(struct device *dev, struct device_node *np,
+				   int *leakage)
+{
+	struct nvmem_cell *cell;
+	u8 value = 0;
+	u32 temp;
+	int conversion;
+	int ret;
+
+	cell = of_nvmem_cell_get(np, "leakage_temp");
+	if (IS_ERR(cell))
+		goto next;
+	nvmem_cell_put(cell);
+	ret = rockchip_nvmem_cell_read_u8(np, "leakage_temp", &value);
+	if (ret) {
+		dev_err(dev, "Failed to get leakage temp\n");
+		return -EINVAL;
+	}
+	/*
+	 * The ambient temperature range: 20C to 40C
+	 * In order to improve the precision, we do a conversion.
+	 * The temp in efuse : temp_efuse = (temp - 20) / (40 - 20) * 63
+	 * The ambient temp : temp = (temp_efuse / 63) * (40 - 20) + 20
+	 * Reserves a decimal point : temp = temp * 10
+	 */
+	temp = value;
+	temp = mul_frac((int_to_frac(temp) / 63 * 20 + int_to_frac(20)),
+			int_to_frac(10));
+	conversion = temp_to_conversion_rate(frac_to_int(temp));
+	*leakage = *leakage * conversion / 100;
+
+next:
+	cell = of_nvmem_cell_get(np, "leakage_volt");
+	if (IS_ERR(cell))
+		return 0;
+	nvmem_cell_put(cell);
+	ret = rockchip_nvmem_cell_read_u8(np, "leakage_volt", &value);
+	if (ret) {
+		dev_err(dev, "Failed to get leakage volt\n");
+		return -EINVAL;
+	}
+	/*
+	 * if ft write leakage use 1.35v, need convert to 1v.
+	 * leakage(1v) = leakage(1.35v) / 4
+	 */
+	if (value)
+		*leakage = *leakage / 4;
+
+	return 0;
+}
+
+static int rockchip_get_leakage_version(int *version)
+{
+	if (*version)
+		return 0;
+
+	if (of_machine_is_compatible("rockchip,rk3368"))
+		*version = LEAKAGE_V2;
+	else if (of_machine_is_compatible("rockchip,rv1126") ||
+		 of_machine_is_compatible("rockchip,rv1109"))
+		*version = LEAKAGE_V3;
+	else
+		*version = LEAKAGE_V1;
+
+	return 0;
+}
+
+static int rockchip_get_leakage_v1(struct device *dev, struct device_node *np,
+				   char *lkg_name, int *leakage)
+{
+	struct nvmem_cell *cell;
+	int ret = 0;
+	u8 value = 0;
+
+	cell = of_nvmem_cell_get(np, "leakage");
+	if (IS_ERR(cell)) {
+		ret = rockchip_nvmem_cell_read_u8(np, lkg_name, &value);
+	} else {
+		nvmem_cell_put(cell);
+		ret = rockchip_nvmem_cell_read_u8(np, "leakage", &value);
+	}
+	if (ret)
+		dev_err(dev, "Failed to get %s\n", lkg_name);
+	else
+		*leakage = value;
+
+	return ret;
+}
+
+static int rockchip_get_leakage_v2(struct device *dev, struct device_node *np,
+				   char *lkg_name, int *leakage)
+{
+	int lkg = 0, ret = 0;
+
+	if (rockchip_get_leakage_v1(dev, np, lkg_name, &lkg))
+		return -EINVAL;
+
+	ret = rockchip_adjust_leakage(dev, np, &lkg);
+	if (ret)
+		dev_err(dev, "Failed to adjust leakage, value=%d\n", lkg);
+	else
+		*leakage = lkg;
+
+	return ret;
+}
+
+static int rockchip_get_leakage_v3(struct device *dev, struct device_node *np,
+				   char *lkg_name, int *leakage)
+{
+	int lkg = 0;
+
+	if (rockchip_get_leakage_v1(dev, np, lkg_name, &lkg))
+		return -EINVAL;
+
+	*leakage = (((lkg & 0xf8) >> 3) * 1000) + ((lkg & 0x7) * 125);
+
+	return 0;
+}
+
+int rockchip_of_get_leakage(struct device *dev, char *lkg_name, int *leakage)
+{
+	struct device_node *np;
+	int ret = -EINVAL;
+
+	np = of_parse_phandle(dev->of_node, "operating-points-v2", 0);
+	if (!np) {
+		dev_warn(dev, "OPP-v2 not supported\n");
+		return -ENOENT;
+	}
+
+	rockchip_get_leakage_version(&lkg_version);
+
+	switch (lkg_version) {
+	case LEAKAGE_V1:
+		ret = rockchip_get_leakage_v1(dev, np, lkg_name, leakage);
+		break;
+	case LEAKAGE_V2:
+		ret = rockchip_get_leakage_v2(dev, np, lkg_name, leakage);
+		break;
+	case LEAKAGE_V3:
+		ret = rockchip_get_leakage_v3(dev, np, lkg_name, leakage);
+		if (!ret) {
+			/*
+			 * round up to the nearest whole number for calculating
+			 * static power,  it does not need to be precise.
+			 */
+			if (*leakage % 1000 > 500)
+				*leakage = *leakage / 1000 + 1;
+			else
+				*leakage = *leakage / 1000;
+		}
+		break;
+	default:
+		break;
+	}
+
+	of_node_put(np);
+
+	return ret;
+}
+EXPORT_SYMBOL(rockchip_of_get_leakage);
+
+void rockchip_of_get_lkg_sel(struct device *dev, struct device_node *np,
+			     char *lkg_name, int process,
+			     int *volt_sel, int *scale_sel)
+{
+	struct property *prop = NULL;
+	int leakage = -EINVAL, ret = 0;
+	char name[NAME_MAX];
+
+	rockchip_get_leakage_version(&lkg_version);
+
+	switch (lkg_version) {
+	case LEAKAGE_V1:
+		ret = rockchip_get_leakage_v1(dev, np, lkg_name, &leakage);
+		if (ret)
+			return;
+		dev_info(dev, "leakage=%d\n", leakage);
+		break;
+	case LEAKAGE_V2:
+		ret = rockchip_get_leakage_v2(dev, np, lkg_name, &leakage);
+		if (ret)
+			return;
+		dev_info(dev, "leakage=%d\n", leakage);
+		break;
+	case LEAKAGE_V3:
+		ret = rockchip_get_leakage_v3(dev, np, lkg_name, &leakage);
+		if (ret)
+			return;
+		dev_info(dev, "leakage=%d.%d\n", leakage / 1000,
+			 leakage % 1000);
+		break;
+	default:
+		return;
+	}
+
+	if (!volt_sel)
+		goto next;
+	if (process >= 0) {
+		snprintf(name, sizeof(name),
+			 "rockchip,p%d-leakage-voltage-sel", process);
+		prop = of_find_property(np, name, NULL);
+	}
+	if (!prop)
+		sprintf(name, "rockchip,leakage-voltage-sel");
+	ret = rockchip_get_sel(np, name, leakage, volt_sel);
+	if (!ret)
+		dev_info(dev, "leakage-volt-sel=%d\n", *volt_sel);
+
+next:
+	if (!scale_sel)
+		return;
+	if (process >= 0) {
+		snprintf(name, sizeof(name),
+			 "rockchip,p%d-leakage-scaling-sel", process);
+		prop = of_find_property(np, name, NULL);
+	}
+	if (!prop)
+		sprintf(name, "rockchip,leakage-scaling-sel");
+	ret = rockchip_get_sel(np, name, leakage, scale_sel);
+	if (!ret)
+		dev_info(dev, "leakage-scale=%d\n", *scale_sel);
+}
+EXPORT_SYMBOL(rockchip_of_get_lkg_sel);
+
+static unsigned long rockchip_pvtpll_get_rate(struct rockchip_opp_info *info)
+{
+	unsigned int rate0, rate1, delta;
+	int i;
+
+#define MIN_STABLE_DELTA 3
+	regmap_read(info->grf, info->pvtpll_avg_offset, &rate0);
+	/* max delay 2ms */
+	for (i = 0; i < 20; i++) {
+		udelay(100);
+		regmap_read(info->grf, info->pvtpll_avg_offset, &rate1);
+		delta = abs(rate1 - rate0);
+		rate0 = rate1;
+		if (delta <= MIN_STABLE_DELTA)
+			break;
+	}
+
+	if (delta > MIN_STABLE_DELTA) {
+		dev_err(info->dev, "%s: bad delta: %u\n", __func__, delta);
+		return 0;
+	}
+
+	return rate0 * 1000000;
+}
+
+static int rockchip_pvtpll_parse_dt(struct rockchip_opp_info *info)
+{
+	struct device_node *np;
+	int ret;
+
+	np = of_parse_phandle(info->dev->of_node, "operating-points-v2", 0);
+	if (!np) {
+		dev_warn(info->dev, "OPP-v2 not supported\n");
+		return -ENOENT;
+	}
+
+	ret = of_property_read_u32(np, "rockchip,pvtpll-avg-offset", &info->pvtpll_avg_offset);
+	if (ret)
+		goto out;
+
+	ret = of_property_read_u32(np, "rockchip,pvtpll-min-rate", &info->pvtpll_min_rate);
+	if (ret)
+		goto out;
+
+	ret = of_property_read_u32(np, "rockchip,pvtpll-volt-step", &info->pvtpll_volt_step);
+out:
+	of_node_put(np);
+
+	return ret;
+}
+
+static int rockchip_init_pvtpll_info(struct rockchip_opp_info *info)
+{
+	struct opp_table *opp_table;
+	struct dev_pm_opp *opp;
+	int i = 0, max_count, ret;
+
+	ret = rockchip_pvtpll_parse_dt(info);
+	if (ret)
+		return ret;
+
+	max_count = dev_pm_opp_get_opp_count(info->dev);
+	if (max_count <= 0)
+		return max_count ? max_count : -ENODATA;
+
+	info->opp_table = kcalloc(max_count, sizeof(*info->opp_table), GFP_KERNEL);
+	if (!info->opp_table)
+		return -ENOMEM;
+
+	opp_table = dev_pm_opp_get_opp_table(info->dev);
+	if (!opp_table) {
+		kfree(info->opp_table);
+		info->opp_table = NULL;
+		return -ENOMEM;
+	}
+
+	mutex_lock(&opp_table->lock);
+	list_for_each_entry(opp, &opp_table->opp_list, node) {
+		if (!opp->available)
+			continue;
+
+		info->opp_table[i].u_volt = opp->supplies[0].u_volt;
+		info->opp_table[i].u_volt_min = opp->supplies[0].u_volt_min;
+		info->opp_table[i].u_volt_max = opp->supplies[0].u_volt_max;
+		if (opp_table->regulator_count > 1) {
+			info->opp_table[i].u_volt_mem = opp->supplies[1].u_volt;
+			info->opp_table[i].u_volt_mem_min = opp->supplies[1].u_volt_min;
+			info->opp_table[i].u_volt_mem_max = opp->supplies[1].u_volt_max;
+		}
+		// TODO: why an opp can have multiple rates?
+		// info->opp_table[i++].rate = opp->rate;
+		info->opp_table[i++].rate = opp->rates[0];
+	}
+	mutex_unlock(&opp_table->lock);
+
+	dev_pm_opp_put_opp_table(opp_table);
+
+	return 0;
+}
+
+static int rockchip_pvtpll_set_volt(struct device *dev, struct regulator *reg,
+				    int target_uV, int max_uV, char *reg_name)
+{
+	int ret = 0;
+
+	ret = regulator_set_voltage(reg, target_uV, max_uV);
+	if (ret)
+		dev_err(dev, "%s: failed to set %s voltage (%d %d uV): %d\n",
+			__func__, reg_name, target_uV, max_uV, ret);
+
+	return ret;
+}
+
+static int rockchip_pvtpll_set_clk(struct device *dev, struct clk *clk,
+				   unsigned long rate)
+{
+	int ret = 0;
+
+	ret = clk_set_rate(clk, rate);
+	if (ret)
+		dev_err(dev, "%s: failed to set rate %lu Hz, ret:%d\n",
+			__func__, rate, ret);
+
+	return ret;
+}
+
+void rockchip_pvtpll_calibrate_opp(struct rockchip_opp_info *info)
+{
+	struct opp_table *opp_table;
+	struct dev_pm_opp *opp;
+	struct regulator *reg = NULL, *reg_mem = NULL;
+	unsigned long old_volt = 0, old_volt_mem = 0;
+	unsigned long volt = 0, volt_mem = 0;
+	unsigned long volt_min, volt_max, volt_mem_min, volt_mem_max;
+	unsigned long rate, pvtpll_rate, old_rate, cur_rate, delta0, delta1;
+	int i = 0, max_count, step, cur_step, ret;
+
+	if (!info || !info->grf)
+		return;
+
+	dev_dbg(info->dev, "calibrating opp ...\n");
+	ret = rockchip_init_pvtpll_info(info);
+	if (ret)
+		return;
+
+	max_count = dev_pm_opp_get_opp_count(info->dev);
+	if (max_count <= 0)
+		return;
+
+	opp_table = dev_pm_opp_get_opp_table(info->dev);
+	if (!opp_table)
+		return;
+
+	if ((!opp_table->regulators) || IS_ERR(opp_table->clk))
+		goto out_put;
+
+	reg = opp_table->regulators[0];
+	old_volt = regulator_get_voltage(reg);
+	if (opp_table->regulator_count > 1) {
+		reg_mem = opp_table->regulators[1];
+		old_volt_mem = regulator_get_voltage(reg_mem);
+		if (IS_ERR_VALUE(old_volt_mem))
+			goto out_put;
+	}
+	old_rate = clk_get_rate(opp_table->clk);
+	if (IS_ERR_VALUE(old_volt) || IS_ERR_VALUE(old_rate))
+		goto out_put;
+	cur_rate = old_rate;
+
+	step = regulator_get_linear_step(reg);
+	if (!step || info->pvtpll_volt_step > step)
+		step = info->pvtpll_volt_step;
+
+	if (old_rate > info->pvtpll_min_rate * 1000) {
+		if (rockchip_pvtpll_set_clk(info->dev, opp_table->clk,
+					    info->pvtpll_min_rate * 1000))
+			goto out_put;
+	}
+
+	for (i = 0; i < max_count; i++) {
+		rate = info->opp_table[i].rate;
+		if (rate < 1000 * info->pvtpll_min_rate)
+			continue;
+
+		volt = max(volt, info->opp_table[i].u_volt);
+		volt_min = info->opp_table[i].u_volt_min;
+		volt_max = info->opp_table[i].u_volt_max;
+
+		if (opp_table->regulator_count > 1) {
+			volt_mem = max(volt_mem, info->opp_table[i].u_volt_mem);
+			volt_mem_min = info->opp_table[i].u_volt_mem_min;
+			volt_mem_max = info->opp_table[i].u_volt_mem_max;
+			if (rockchip_pvtpll_set_volt(info->dev, reg_mem,
+						     volt_mem, volt_mem_max, "mem"))
+				goto out;
+		}
+		if (rockchip_pvtpll_set_volt(info->dev, reg, volt, volt_max, "vdd"))
+			goto out;
+
+		if (rockchip_pvtpll_set_clk(info->dev, opp_table->clk, rate))
+			goto out;
+		cur_rate = rate;
+		pvtpll_rate = rockchip_pvtpll_get_rate(info);
+		if (!pvtpll_rate)
+			goto out;
+		cur_step = (pvtpll_rate < rate) ? step : -step;
+		delta1 = abs(pvtpll_rate - rate);
+		do {
+			delta0 = delta1;
+			volt += cur_step;
+			if ((volt < volt_min) || (volt > volt_max))
+				break;
+			if (opp_table->regulator_count > 1) {
+				if (volt > volt_mem_max)
+					break;
+				else if (volt < volt_mem_min)
+					volt_mem = volt_mem_min;
+				else
+					volt_mem = volt;
+				if (rockchip_pvtpll_set_volt(info->dev, reg_mem,
+							     volt_mem, volt_mem_max,
+							     "mem"))
+					break;
+			}
+			if (rockchip_pvtpll_set_volt(info->dev, reg, volt,
+						     volt_max, "vdd"))
+				break;
+			pvtpll_rate = rockchip_pvtpll_get_rate(info);
+			if (!pvtpll_rate)
+				goto out;
+			delta1 = abs(pvtpll_rate - rate);
+		} while (delta1 < delta0);
+
+		volt -= cur_step;
+		info->opp_table[i].u_volt = volt;
+		if (opp_table->regulator_count > 1) {
+			if (volt < volt_mem_min)
+				volt_mem = volt_mem_min;
+			else
+				volt_mem = volt;
+			info->opp_table[i].u_volt_mem = volt_mem;
+		}
+	}
+
+	i = 0;
+	mutex_lock(&opp_table->lock);
+	list_for_each_entry(opp, &opp_table->opp_list, node) {
+		if (!opp->available)
+			continue;
+
+		opp->supplies[0].u_volt = info->opp_table[i].u_volt;
+		if (opp_table->regulator_count > 1)
+			opp->supplies[1].u_volt = info->opp_table[i].u_volt_mem;
+		i++;
+	}
+	mutex_unlock(&opp_table->lock);
+	dev_info(info->dev, "opp calibration done\n");
+out:
+	if (cur_rate > old_rate)
+		rockchip_pvtpll_set_clk(info->dev, opp_table->clk, old_rate);
+	if (opp_table->regulator_count > 1)
+		rockchip_pvtpll_set_volt(info->dev, reg_mem, old_volt_mem,
+					 INT_MAX, "mem");
+	rockchip_pvtpll_set_volt(info->dev, reg, old_volt, INT_MAX, "vdd");
+	if (cur_rate < old_rate)
+		rockchip_pvtpll_set_clk(info->dev, opp_table->clk, old_rate);
+out_put:
+	dev_pm_opp_put_opp_table(opp_table);
+}
+EXPORT_SYMBOL(rockchip_pvtpll_calibrate_opp);
+
+static int rockchip_get_pvtm_pvtpll(struct device *dev, struct device_node *np,
+				    char *reg_name)
+{
+	struct regulator *reg;
+	struct clk *clk;
+	struct pvtm_config *pvtm;
+	unsigned long old_freq;
+	unsigned int old_volt;
+	int cur_temp, diff_temp, prop_temp, diff_value;
+	int pvtm_value = 0;
+	int ret = 0;
+
+	pvtm = kzalloc(sizeof(*pvtm), GFP_KERNEL);
+	if (!pvtm)
+		return -ENOMEM;
+
+	ret = rockchip_parse_pvtm_config(np, pvtm);
+	if (ret)
+		goto out;
+
+	clk = clk_get(dev, NULL);
+	if (IS_ERR_OR_NULL(clk)) {
+		dev_warn(dev, "Failed to get clk\n");
+		goto out;
+	}
+
+	reg = regulator_get_optional(dev, reg_name);
+	if (IS_ERR_OR_NULL(reg)) {
+		dev_warn(dev, "Failed to get reg\n");
+		clk_put(clk);
+		goto out;
+	}
+	old_freq = clk_get_rate(clk);
+	old_volt = regulator_get_voltage(reg);
+
+	ret = clk_set_rate(clk, pvtm->freq * 1000);
+	if (ret) {
+		dev_err(dev, "Failed to set pvtm freq\n");
+		goto put_reg;
+	}
+	ret = regulator_set_voltage(reg, pvtm->volt, pvtm->volt);
+	if (ret) {
+		dev_err(dev, "Failed to set pvtm_volt\n");
+		goto restore_clk;
+	}
+	usleep_range(pvtm->sample_time, pvtm->sample_time + 100);
+
+	ret = regmap_read(pvtm->grf, pvtm->offset, &pvtm_value);
+	if (ret < 0) {
+		dev_err(dev, "failed to get pvtm from 0x%x\n", pvtm->offset);
+		goto resetore_volt;
+	}
+	pvtm->tz->ops->get_temp(pvtm->tz, &cur_temp);
+	diff_temp = (cur_temp / 1000 - pvtm->ref_temp);
+	if (diff_temp < 0)
+		prop_temp = pvtm->temp_prop[0];
+	else
+		prop_temp = pvtm->temp_prop[1];
+	diff_value = diff_temp * prop_temp / 1000;
+	pvtm_value += diff_value;
+
+	dev_info(dev, "pvtm=%d\n", pvtm_value);
+
+resetore_volt:
+	regulator_set_voltage(reg, old_volt, old_volt);
+restore_clk:
+	clk_set_rate(clk, old_freq);
+put_reg:
+	regulator_put(reg);
+	clk_put(clk);
+out:
+	kfree(pvtm);
+
+	return pvtm_value;
+}
+
+static int rockchip_get_pvtm(struct device *dev, struct device_node *np,
+			     char *reg_name)
+{
+	struct regulator *reg;
+	struct clk *clk;
+	unsigned int ch[2];
+	int pvtm = 0;
+	u16 tmp = 0;
+
+	if (!rockchip_nvmem_cell_read_u16(np, "pvtm", &tmp) && tmp) {
+		pvtm = 10 * tmp;
+		dev_info(dev, "pvtm = %d, from nvmem\n", pvtm);
+		return pvtm;
+	}
+
+	if (of_property_read_u32_array(np, "rockchip,pvtm-ch", ch, 2))
+		return -EINVAL;
+
+	if (ch[0] >= PVTM_CH_MAX || ch[1] >= PVTM_SUB_CH_MAX)
+		return -EINVAL;
+
+	if (pvtm_value[ch[0]][ch[1]]) {
+		dev_info(dev, "pvtm = %d, form pvtm_value\n", pvtm_value[ch[0]][ch[1]]);
+		return pvtm_value[ch[0]][ch[1]];
+	}
+
+	clk = clk_get(dev, NULL);
+	if (IS_ERR_OR_NULL(clk)) {
+		dev_warn(dev, "Failed to get clk\n");
+		return PTR_ERR_OR_ZERO(clk);
+	}
+
+	reg = regulator_get_optional(dev, reg_name);
+	if (IS_ERR_OR_NULL(reg)) {
+		dev_warn(dev, "Failed to get reg\n");
+		clk_put(clk);
+		return PTR_ERR_OR_ZERO(reg);
+	}
+
+	rockchip_get_pvtm_specific_value(dev, np, clk, reg, &pvtm);
+
+	regulator_put(reg);
+	clk_put(clk);
+
+	return pvtm;
+}
+
+void rockchip_of_get_pvtm_sel(struct device *dev, struct device_node *np,
+			      char *reg_name, int process,
+			      int *volt_sel, int *scale_sel)
+{
+	struct property *prop = NULL;
+	char name[NAME_MAX];
+	int pvtm, ret;
+
+	if (of_property_read_bool(np, "rockchip,pvtm-pvtpll"))
+		pvtm = rockchip_get_pvtm_pvtpll(dev, np, reg_name);
+	else
+		pvtm = rockchip_get_pvtm(dev, np, reg_name);
+	if (pvtm <= 0)
+		return;
+
+	if (!volt_sel)
+		goto next;
+	if (process >= 0) {
+		snprintf(name, sizeof(name),
+			 "rockchip,p%d-pvtm-voltage-sel", process);
+		prop = of_find_property(np, name, NULL);
+	}
+	if (!prop)
+		sprintf(name, "rockchip,pvtm-voltage-sel");
+	ret = rockchip_get_sel(np, name, pvtm, volt_sel);
+	if (!ret && volt_sel)
+		dev_info(dev, "pvtm-volt-sel=%d\n", *volt_sel);
+
+next:
+	if (!scale_sel)
+		return;
+	if (process >= 0) {
+		snprintf(name, sizeof(name),
+			 "rockchip,p%d-pvtm-scaling-sel", process);
+		prop = of_find_property(np, name, NULL);
+	}
+	if (!prop)
+		sprintf(name, "rockchip,pvtm-scaling-sel");
+	ret = rockchip_get_sel(np, name, pvtm, scale_sel);
+	if (!ret)
+		dev_info(dev, "pvtm-scale=%d\n", *scale_sel);
+}
+EXPORT_SYMBOL(rockchip_of_get_pvtm_sel);
+
+void rockchip_of_get_bin_sel(struct device *dev, struct device_node *np,
+			     int bin, int *scale_sel)
+{
+	int ret = 0;
+
+	if (!scale_sel || bin < 0)
+		return;
+
+	ret = rockchip_get_bin_sel(np, "rockchip,bin-scaling-sel",
+				   bin, scale_sel);
+	if (!ret)
+		dev_info(dev, "bin-scale=%d\n", *scale_sel);
+}
+EXPORT_SYMBOL(rockchip_of_get_bin_sel);
+
+void rockchip_of_get_bin_volt_sel(struct device *dev, struct device_node *np,
+				  int bin, int *bin_volt_sel)
+{
+	int ret = 0;
+
+	if (!bin_volt_sel || bin < 0)
+		return;
+
+	ret = rockchip_get_bin_sel(np, "rockchip,bin-voltage-sel",
+				   bin, bin_volt_sel);
+	if (!ret)
+		dev_info(dev, "bin-volt-sel=%d\n", *bin_volt_sel);
+}
+EXPORT_SYMBOL(rockchip_of_get_bin_volt_sel);
+
+void rockchip_get_opp_data(const struct of_device_id *matches,
+			   struct rockchip_opp_info *info)
+{
+	const struct of_device_id *match;
+	struct device_node *node;
+
+	node = of_find_node_by_path("/");
+	match = of_match_node(matches, node);
+	if (match && match->data)
+		info->data = match->data;
+	of_node_put(node);
+}
+EXPORT_SYMBOL(rockchip_get_opp_data);
+
+int rockchip_get_volt_rm_table(struct device *dev, struct device_node *np,
+			       char *porp_name, struct volt_rm_table **table)
+{
+	struct volt_rm_table *rm_table;
+	const struct property *prop;
+	int count, i;
+
+	prop = of_find_property(np, porp_name, NULL);
+	if (!prop)
+		return -EINVAL;
+
+	if (!prop->value)
+		return -ENODATA;
+
+	count = of_property_count_u32_elems(np, porp_name);
+	if (count < 0)
+		return -EINVAL;
+
+	if (count % 2)
+		return -EINVAL;
+
+	rm_table = devm_kzalloc(dev, sizeof(*rm_table) * (count / 2 + 1),
+				GFP_KERNEL);
+	if (!rm_table)
+		return -ENOMEM;
+
+	for (i = 0; i < count / 2; i++) {
+		of_property_read_u32_index(np, porp_name, 2 * i,
+					   &rm_table[i].volt);
+		of_property_read_u32_index(np, porp_name, 2 * i + 1,
+					   &rm_table[i].rm);
+	}
+
+	rm_table[i].volt = 0;
+	rm_table[i].rm = VOLT_RM_TABLE_END;
+
+	*table = rm_table;
+
+	return 0;
+}
+EXPORT_SYMBOL(rockchip_get_volt_rm_table);
+
+void rockchip_get_scale_volt_sel(struct device *dev, char *lkg_name,
+				 char *reg_name, int bin, int process,
+				 int *scale, int *volt_sel)
+{
+	struct device_node *np;
+	int lkg_scale = 0, pvtm_scale = 0, bin_scale = 0;
+	int lkg_volt_sel = -EINVAL, pvtm_volt_sel = -EINVAL;
+	int bin_volt_sel = -EINVAL;
+
+	np = of_parse_phandle(dev->of_node, "operating-points-v2", 0);
+	if (!np) {
+		dev_warn(dev, "OPP-v2 not supported\n");
+		return;
+	}
+
+	rockchip_of_get_lkg_sel(dev, np, lkg_name, process,
+				&lkg_volt_sel, &lkg_scale);
+	rockchip_of_get_pvtm_sel(dev, np, reg_name, process,
+				 &pvtm_volt_sel, &pvtm_scale);
+	rockchip_of_get_bin_sel(dev, np, bin, &bin_scale);
+	rockchip_of_get_bin_volt_sel(dev, np, bin, &bin_volt_sel);
+	if (scale)
+		*scale = max3(lkg_scale, pvtm_scale, bin_scale);
+	if (volt_sel) {
+		if (bin_volt_sel >= 0)
+			*volt_sel = bin_volt_sel;
+		else
+			*volt_sel = max(lkg_volt_sel, pvtm_volt_sel);
+	}
+
+	of_node_put(np);
+}
+EXPORT_SYMBOL(rockchip_get_scale_volt_sel);
+
+struct opp_table *rockchip_set_opp_prop_name(struct device *dev, int process,
+					     int volt_sel)
+{
+	char name[MAX_PROP_NAME_LEN];
+
+	if (process >= 0) {
+		if (volt_sel >= 0)
+			snprintf(name, MAX_PROP_NAME_LEN, "P%d-L%d",
+				 process, volt_sel);
+		else
+			snprintf(name, MAX_PROP_NAME_LEN, "P%d", process);
+	} else if (volt_sel >= 0) {
+		snprintf(name, MAX_PROP_NAME_LEN, "L%d", volt_sel);
+	} else {
+		return NULL;
+	}
+
+	dev_pm_opp_set_prop_name(dev, name);
+
+	return dev_pm_opp_get_opp_table(dev);
+}
+EXPORT_SYMBOL(rockchip_set_opp_prop_name);
+
+static int rockchip_adjust_opp_by_irdrop(struct device *dev,
+					 struct device_node *np,
+					 unsigned long *safe_rate,
+					 unsigned long *max_rate)
+{
+	struct sel_table *irdrop_table = NULL;
+	struct opp_table *opp_table;
+	struct dev_pm_opp *opp;
+	unsigned long tmp_safe_rate = 0;
+	int evb_irdrop = 0, board_irdrop, delta_irdrop;
+	int opp_rate, i, ret = 0;
+	u32 max_volt = UINT_MAX;
+	bool reach_max_volt = false;
+
+	of_property_read_u32_index(np, "rockchip,max-volt", 0, &max_volt);
+	of_property_read_u32_index(np, "rockchip,evb-irdrop", 0, &evb_irdrop);
+	rockchip_get_sel_table(np, "rockchip,board-irdrop", &irdrop_table);
+
+	opp_table = dev_pm_opp_get_opp_table(dev);
+	if (!opp_table) {
+		ret =  -ENOMEM;
+		goto out;
+	}
+
+	mutex_lock(&opp_table->lock);
+	list_for_each_entry(opp, &opp_table->opp_list, node) {
+		if (!opp->available)
+			continue;
+		if (!irdrop_table) {
+			delta_irdrop = 0;
+		} else {
+			opp_rate = opp->rates[0] / 1000000;
+			board_irdrop = -EINVAL;
+			for (i = 0; irdrop_table[i].sel != SEL_TABLE_END; i++) {
+				if (opp_rate >= irdrop_table[i].min)
+					board_irdrop = irdrop_table[i].sel;
+			}
+			if (board_irdrop == -EINVAL)
+				delta_irdrop = 0;
+			else
+				delta_irdrop = board_irdrop - evb_irdrop;
+		}
+		if ((opp->supplies[0].u_volt + delta_irdrop) <= max_volt) {
+			opp->supplies[0].u_volt += delta_irdrop;
+			opp->supplies[0].u_volt_min += delta_irdrop;
+			if (opp->supplies[0].u_volt_max + delta_irdrop <=
+			    max_volt)
+				opp->supplies[0].u_volt_max += delta_irdrop;
+			else
+				opp->supplies[0].u_volt_max = max_volt;
+			if (!reach_max_volt)
+				tmp_safe_rate = opp->rates[0];
+			if (opp->supplies[0].u_volt == max_volt)
+				reach_max_volt = true;
+		} else {
+			opp->supplies[0].u_volt = max_volt;
+			opp->supplies[0].u_volt_min = max_volt;
+			opp->supplies[0].u_volt_max = max_volt;
+		}
+		if (max_rate)
+			*max_rate = opp->rates[0];
+		if (safe_rate && tmp_safe_rate != opp->rates[0])
+			*safe_rate = tmp_safe_rate;
+	}
+	mutex_unlock(&opp_table->lock);
+
+	dev_pm_opp_put_opp_table(opp_table);
+out:
+	kfree(irdrop_table);
+
+	return ret;
+}
+
+static void rockchip_adjust_opp_by_mbist_vmin(struct device *dev,
+					      struct device_node *np)
+{
+	struct opp_table *opp_table;
+	struct dev_pm_opp *opp;
+	u32 vmin = 0;
+	u8 index = 0;
+
+	if (rockchip_nvmem_cell_read_u8(np, "mbist-vmin", &index))
+		return;
+
+	if (!index)
+		return;
+
+	if (of_property_read_u32_index(np, "mbist-vmin", index-1, &vmin))
+		return;
+
+	opp_table = dev_pm_opp_get_opp_table(dev);
+	if (!opp_table)
+		return;
+
+	mutex_lock(&opp_table->lock);
+	list_for_each_entry(opp, &opp_table->opp_list, node) {
+		if (!opp->available)
+			continue;
+		if (opp->supplies->u_volt < vmin) {
+			opp->supplies->u_volt = vmin;
+			opp->supplies->u_volt_min = vmin;
+		}
+	}
+	mutex_unlock(&opp_table->lock);
+}
+
+static int rockchip_adjust_opp_table(struct device *dev,
+				     unsigned long scale_rate)
+{
+	struct dev_pm_opp *opp;
+	unsigned long rate;
+	int i, count, ret = 0;
+
+	count = dev_pm_opp_get_opp_count(dev);
+	if (count <= 0) {
+		ret = count ? count : -ENODATA;
+		goto out;
+	}
+
+	for (i = 0, rate = 0; i < count; i++, rate++) {
+		/* find next rate */
+		opp = dev_pm_opp_find_freq_ceil(dev, &rate);
+		if (IS_ERR(opp)) {
+			ret = PTR_ERR(opp);
+			goto out;
+		}
+		if (opp->rates[0] > scale_rate)
+			dev_pm_opp_disable(dev, opp->rates[0]);
+		dev_pm_opp_put(opp);
+	}
+out:
+	return ret;
+}
+
+static int rockchip_pll_clk_adaptive_scaling(struct clk *clk, int sel)
+{
+	(void *)clk;
+	(void *)sel;
+	return 0;
+}
+int rockchip_pll_clk_rate_to_scale(struct clk *clk, unsigned long rate);
+int rockchip_pll_clk_scale_to_rate(struct clk *clk, unsigned int scale);
+
+int rockchip_adjust_power_scale(struct device *dev, int scale)
+{
+	struct device_node *np;
+	struct clk *clk;
+	unsigned long safe_rate = 0, max_rate = 0;
+	int irdrop_scale = 0, opp_scale = 0;
+	u32 target_scale, avs = 0, avs_scale = 0;
+	long scale_rate = 0;
+	int ret = 0;
+
+	np = of_parse_phandle(dev->of_node, "operating-points-v2", 0);
+	if (!np) {
+		dev_warn(dev, "OPP-v2 not supported\n");
+		return -ENOENT;
+	}
+	of_property_read_u32(np, "rockchip,avs-enable", &avs);
+	of_property_read_u32(np, "rockchip,avs", &avs);
+	of_property_read_u32(np, "rockchip,avs-scale", &avs_scale);
+	rockchip_adjust_opp_by_mbist_vmin(dev, np);
+	rockchip_adjust_opp_by_irdrop(dev, np, &safe_rate, &max_rate);
+
+	dev_info(dev, "avs=%d\n", avs);
+	clk = of_clk_get_by_name(np, NULL);
+	if (IS_ERR(clk)) {
+		if (!safe_rate)
+			goto out_np;
+		dev_dbg(dev, "Failed to get clk, safe_rate=%lu\n", safe_rate);
+		ret = rockchip_adjust_opp_table(dev, safe_rate);
+		if (ret)
+			dev_err(dev, "Failed to adjust opp table\n");
+		goto out_np;
+	}
+
+	if (safe_rate)
+		irdrop_scale = rockchip_pll_clk_rate_to_scale(clk, safe_rate);
+	if (max_rate)
+		opp_scale = rockchip_pll_clk_rate_to_scale(clk, max_rate);
+	target_scale = max(irdrop_scale, scale);
+	if (target_scale <= 0)
+		goto out_clk;
+	dev_dbg(dev, "target_scale=%d, irdrop_scale=%d, scale=%d\n",
+		target_scale, irdrop_scale, scale);
+
+	if (avs == AVS_SCALING_RATE) {
+		ret = rockchip_pll_clk_adaptive_scaling(clk, target_scale);
+		if (ret)
+			dev_err(dev, "Failed to adaptive scaling\n");
+		if (opp_scale >= avs_scale)
+			goto out_clk;
+		dev_info(dev, "avs-scale=%d, opp-scale=%d\n", avs_scale,
+			 opp_scale);
+		scale_rate = rockchip_pll_clk_scale_to_rate(clk, avs_scale);
+		if (scale_rate <= 0) {
+			dev_err(dev, "Failed to get avs scale rate, %d\n",
+				avs_scale);
+			goto out_clk;
+		}
+		dev_dbg(dev, "scale_rate=%lu\n", scale_rate);
+		ret = rockchip_adjust_opp_table(dev, scale_rate);
+		if (ret)
+			dev_err(dev, "Failed to adjust opp table\n");
+	} else if (avs == AVS_DELETE_OPP) {
+		if (opp_scale >= target_scale)
+			goto out_clk;
+		dev_info(dev, "target_scale=%d, opp-scale=%d\n", target_scale,
+			 opp_scale);
+		scale_rate = rockchip_pll_clk_scale_to_rate(clk, target_scale);
+		if (scale_rate <= 0) {
+			dev_err(dev, "Failed to get scale rate, %d\n",
+				target_scale);
+			goto out_clk;
+		}
+		dev_dbg(dev, "scale_rate=%lu\n", scale_rate);
+		ret = rockchip_adjust_opp_table(dev, scale_rate);
+		if (ret)
+			dev_err(dev, "Failed to adjust opp table\n");
+	}
+
+out_clk:
+	clk_put(clk);
+out_np:
+	of_node_put(np);
+
+	return ret;
+}
+EXPORT_SYMBOL(rockchip_adjust_power_scale);
+
+int rockchip_get_read_margin(struct device *dev,
+			     struct rockchip_opp_info *opp_info,
+			     unsigned long volt, u32 *target_rm)
+{
+	int i;
+
+	if (!opp_info || !opp_info->volt_rm_tbl)
+		return 0;
+
+	for (i = 0; opp_info->volt_rm_tbl[i].rm != VOLT_RM_TABLE_END; i++) {
+		if (volt >= opp_info->volt_rm_tbl[i].volt) {
+			opp_info->target_rm = opp_info->volt_rm_tbl[i].rm;
+			break;
+		}
+	}
+	*target_rm = opp_info->target_rm;
+
+	return 0;
+}
+EXPORT_SYMBOL(rockchip_get_read_margin);
+
+int rockchip_set_read_margin(struct device *dev,
+			     struct rockchip_opp_info *opp_info, u32 rm,
+			     bool is_set_rm)
+{
+	if (!is_set_rm || !opp_info)
+		return 0;
+	if (!opp_info || !opp_info->volt_rm_tbl)
+		return 0;
+	if (!opp_info->data || !opp_info->data->set_read_margin)
+		return 0;
+	if (rm == opp_info->current_rm)
+		return 0;
+
+	return opp_info->data->set_read_margin(dev, opp_info, rm);
+}
+EXPORT_SYMBOL(rockchip_set_read_margin);
+
+int rockchip_init_read_margin(struct device *dev,
+			      struct rockchip_opp_info *opp_info,
+			      char *reg_name)
+{
+	struct clk *clk;
+	struct regulator *reg;
+	unsigned long cur_rate;
+	int cur_volt, ret = 0;
+	u32 target_rm = UINT_MAX;
+
+	reg = regulator_get_optional(dev, reg_name);
+	if (IS_ERR(reg)) {
+		ret = PTR_ERR(reg);
+		if (ret != -EPROBE_DEFER)
+			dev_err(dev, "%s: no regulator (%s) found: %d\n",
+				__func__, reg_name, ret);
+		return ret;
+	}
+	cur_volt = regulator_get_voltage(reg);
+	if (cur_volt < 0) {
+		ret = cur_volt;
+		if (ret != -EPROBE_DEFER)
+			dev_err(dev, "%s: failed to get (%s) volt: %d\n",
+				__func__, reg_name, ret);
+		goto out;
+	}
+
+	clk = clk_get(dev, NULL);
+	if (IS_ERR(clk)) {
+		ret = PTR_ERR(clk);
+		dev_err(dev, "%s: failed to get clk: %d\n", __func__, ret);
+		goto out;
+	}
+	cur_rate = clk_get_rate(clk);
+
+	rockchip_get_read_margin(dev, opp_info, cur_volt, &target_rm);
+	dev_dbg(dev, "cur_rate=%lu, threshold=%lu, cur_volt=%d, target_rm=%d\n",
+		cur_rate, opp_info->intermediate_threshold_freq,
+		cur_volt, target_rm);
+	if (opp_info->intermediate_threshold_freq &&
+	    cur_rate > opp_info->intermediate_threshold_freq) {
+		clk_set_rate(clk, opp_info->intermediate_threshold_freq);
+		rockchip_set_read_margin(dev, opp_info, target_rm, true);
+		clk_set_rate(clk, cur_rate);
+	} else {
+		rockchip_set_read_margin(dev, opp_info, target_rm, true);
+	}
+
+	clk_put(clk);
+out:
+	regulator_put(reg);
+
+	return ret;
+}
+EXPORT_SYMBOL(rockchip_init_read_margin);
+
+int rockchip_set_intermediate_rate(struct device *dev,
+				   struct rockchip_opp_info *opp_info,
+				   struct clk *clk, unsigned long old_freq,
+				   unsigned long new_freq, bool is_scaling_up,
+				   bool is_set_clk)
+{
+	if (!is_set_clk)
+		return 0;
+	if (!opp_info || !opp_info->volt_rm_tbl)
+		return 0;
+	if (!opp_info->data || !opp_info->data->set_read_margin)
+		return 0;
+	if (opp_info->target_rm == opp_info->current_rm)
+		return 0;
+	/*
+	 * There is no need to set intermediate rate if the new voltage
+	 * and the current voltage are high voltage.
+	 */
+	if ((opp_info->target_rm < opp_info->low_rm) &&
+	    (opp_info->current_rm < opp_info->low_rm))
+		return 0;
+
+	if (is_scaling_up) {
+		/*
+		 * If scaling up and the current frequency is less than
+		 * or equal to intermediate threshold frequency, there is
+		 * no need to set intermediate rate.
+		 */
+		if (opp_info->intermediate_threshold_freq &&
+		    old_freq <= opp_info->intermediate_threshold_freq)
+			return 0;
+		return clk_set_rate(clk, new_freq | OPP_SCALING_UP_INTER);
+	}
+	/*
+	 * If scaling down and the new frequency is less than or equal to
+	 * intermediate threshold frequency , there is no need to set
+	 * intermediate rate and set the new frequency directly.
+	 */
+	if (opp_info->intermediate_threshold_freq &&
+	    new_freq <= opp_info->intermediate_threshold_freq)
+		return clk_set_rate(clk, new_freq);
+
+	return clk_set_rate(clk, new_freq | OPP_SCALING_DOWN_INTER);
+}
+EXPORT_SYMBOL(rockchip_set_intermediate_rate);
+
+int rockchip_init_opp_table(struct device *dev, struct rockchip_opp_info *info,
+			    char *lkg_name, char *reg_name)
+{
+	struct device_node *np;
+	int bin = -EINVAL, process = -EINVAL;
+	int scale = 0, volt_sel = -EINVAL;
+	int ret = 0, num_clks = 0, i;
+	u32 freq;
+
+	/* Get OPP descriptor node */
+	np = of_parse_phandle(dev->of_node, "operating-points-v2", 0);
+	if (!np) {
+		dev_dbg(dev, "Failed to find operating-points-v2\n");
+		return -ENOENT;
+	}
+	if (!info)
+		goto next;
+	info->dev = dev;
+
+	num_clks = of_clk_get_parent_count(np);
+	if (num_clks > 0) {
+		info->clks = devm_kcalloc(dev, num_clks, sizeof(*info->clks),
+					  GFP_KERNEL);
+		if (!info->clks) {
+			ret = -ENOMEM;
+			goto out;
+		}
+		for (i = 0; i < num_clks; i++) {
+			info->clks[i].clk = of_clk_get(np, i);
+			if (IS_ERR(info->clks[i].clk)) {
+				ret = PTR_ERR(info->clks[i].clk);
+				dev_err(dev, "%s: failed to get clk %d\n",
+					np->name, i);
+				goto out;
+			}
+		}
+		info->num_clks = num_clks;
+		ret = clk_bulk_prepare_enable(info->num_clks, info->clks);
+		if (ret) {
+			dev_err(dev, "failed to enable opp clks\n");
+			goto out;
+		}
+	}
+	if (info->data && info->data->set_read_margin) {
+		info->current_rm = UINT_MAX;
+		info->grf = syscon_regmap_lookup_by_phandle(np, "rockchip,grf");
+		if (IS_ERR(info->grf))
+			info->grf = NULL;
+		rockchip_get_volt_rm_table(dev, np, "volt-mem-read-margin",
+					   &info->volt_rm_tbl);
+		of_property_read_u32(np, "low-volt-mem-read-margin",
+				     &info->low_rm);
+		if (!of_property_read_u32(np, "intermediate-threshold-freq",
+					  &freq))
+			info->intermediate_threshold_freq = freq * 1000;
+		rockchip_init_read_margin(dev, info, reg_name);
+	}
+	if (info->data && info->data->get_soc_info)
+		info->data->get_soc_info(dev, np, &bin, &process);
+
+next:
+	rockchip_get_scale_volt_sel(dev, lkg_name, reg_name, bin, process,
+				    &scale, &volt_sel);
+	rockchip_set_opp_prop_name(dev, process, volt_sel);
+	ret = dev_pm_opp_of_add_table(dev);
+	if (ret) {
+		dev_err(dev, "Invalid operating-points in device tree.\n");
+		goto dis_opp_clk;
+	}
+	rockchip_adjust_power_scale(dev, scale);
+	rockchip_pvtpll_calibrate_opp(info);
+
+dis_opp_clk:
+	if (info && info->clks)
+		clk_bulk_disable_unprepare(info->num_clks, info->clks);
+out:
+	of_node_put(np);
+
+	return ret;
+}
+EXPORT_SYMBOL(rockchip_init_opp_table);
+
+MODULE_DESCRIPTION("ROCKCHIP OPP Select");
+MODULE_AUTHOR("Finley Xiao <finley.xiao@rock-chips.com>, Liang Chen <cl@rock-chips.com>");
+MODULE_LICENSE("GPL");
diff --git a/include/linux/dma-iommu.h b/include/linux/dma-iommu.h
new file mode 100644
index 000000000..f51561eda
--- /dev/null
+++ b/include/linux/dma-iommu.h
@@ -0,0 +1,98 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Copyright (C) 2014-2015 ARM Ltd.
+ */
+#ifndef __DMA_IOMMU_H
+#define __DMA_IOMMU_H
+
+#include <linux/errno.h>
+#include <linux/types.h>
+
+#ifdef CONFIG_IOMMU_DMA
+#include <linux/dma-mapping.h>
+#include <linux/iommu.h>
+#include <linux/msi.h>
+
+/* Domain management interface for IOMMU drivers */
+int iommu_get_dma_cookie(struct iommu_domain *domain);
+int iommu_get_msi_cookie(struct iommu_domain *domain, dma_addr_t base);
+void iommu_put_dma_cookie(struct iommu_domain *domain);
+
+/* Setup call for arch DMA mapping code */
+void iommu_setup_dma_ops(struct device *dev, u64 dma_base, u64 size);
+
+/* The DMA API isn't _quite_ the whole story, though... */
+/*
+ * iommu_dma_prepare_msi() - Map the MSI page in the IOMMU device
+ *
+ * The MSI page will be stored in @desc.
+ *
+ * Return: 0 on success otherwise an error describing the failure.
+ */
+int iommu_dma_prepare_msi(struct msi_desc *desc, phys_addr_t msi_addr);
+
+/* Update the MSI message if required. */
+void iommu_dma_compose_msi_msg(struct msi_desc *desc,
+			       struct msi_msg *msg);
+
+void iommu_dma_get_resv_regions(struct device *dev, struct list_head *list);
+
+int iommu_dma_reserve_iova(struct device *dev, dma_addr_t base,
+			   u64 size);
+
+int iommu_dma_enable_best_fit_algo(struct device *dev);
+
+#else /* CONFIG_IOMMU_DMA */
+
+struct iommu_domain;
+struct msi_desc;
+struct msi_msg;
+struct device;
+
+static inline void iommu_setup_dma_ops(struct device *dev, u64 dma_base,
+		u64 size)
+{
+}
+
+static inline int iommu_get_dma_cookie(struct iommu_domain *domain)
+{
+	return -ENODEV;
+}
+
+static inline int iommu_get_msi_cookie(struct iommu_domain *domain, dma_addr_t base)
+{
+	return -ENODEV;
+}
+
+static inline void iommu_put_dma_cookie(struct iommu_domain *domain)
+{
+}
+
+static inline int iommu_dma_prepare_msi(struct msi_desc *desc,
+					phys_addr_t msi_addr)
+{
+	return 0;
+}
+
+static inline void iommu_dma_compose_msi_msg(struct msi_desc *desc,
+					     struct msi_msg *msg)
+{
+}
+
+static inline void iommu_dma_get_resv_regions(struct device *dev, struct list_head *list)
+{
+}
+
+static inline int iommu_dma_reserve_iova(struct device *dev, dma_addr_t base,
+					 u64 size)
+{
+	return -ENODEV;
+}
+
+static inline int iommu_dma_enable_best_fit_algo(struct device *dev)
+{
+	return -ENODEV;
+}
+
+#endif	/* CONFIG_IOMMU_DMA */
+#endif	/* __DMA_IOMMU_H */
diff --git a/include/linux/rk-dma-heap.h b/include/linux/rk-dma-heap.h
new file mode 100644
index 000000000..cd0890cba
--- /dev/null
+++ b/include/linux/rk-dma-heap.h
@@ -0,0 +1,137 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * DMABUF Heaps Allocation Infrastructure
+ *
+ * Copyright (C) 2011 Google, Inc.
+ * Copyright (C) 2019 Linaro Ltd.
+ * Copyright (C) 2022 Rockchip Electronics Co. Ltd.
+ * Author: Simon Xue <xxm@rock-chips.com>
+ */
+
+#ifndef _RK_DMA_HEAPS_H_
+#define _RK_DMA_HEAPS_H_
+#include <linux/dma-buf.h>
+
+struct rk_dma_heap;
+
+#if defined(CONFIG_DMABUF_HEAPS_ROCKCHIP)
+int rk_dma_heap_cma_setup(void);
+
+/**
+ * rk_dma_heap_set_dev - set heap dev dma param
+ * @heap: DMA-Heap to retrieve private data for
+ *
+ * Returns:
+ * Zero on success, ERR_PTR(-errno) on error
+ */
+int rk_dma_heap_set_dev(struct device *heap_dev);
+
+/**
+ * rk_dma_heap_find - Returns the registered dma_heap with the specified name
+ * @name: Name of the heap to find
+ *
+ * NOTE: dma_heaps returned from this function MUST be released
+ * using rk_dma_heap_put() when the user is done.
+ */
+struct rk_dma_heap *rk_dma_heap_find(const char *name);
+
+/** rk_dma_heap_buffer_free - Free dma_buf allocated by rk_dma_heap_buffer_alloc
+ * @dma_buf:	dma_buf to free
+ *
+ * This is really only a simple wrapper to dma_buf_put()
+ */
+void rk_dma_heap_buffer_free(struct dma_buf *dmabuf);
+
+/**
+ * rk_dma_heap_buffer_alloc - Allocate dma-buf from a dma_heap
+ * @heap:	dma_heap to allocate from
+ * @len:	size to allocate
+ * @fd_flags:	flags to set on returned dma-buf fd
+ * @heap_flags:	flags to pass to the dma heap
+ *
+ * This is for internal dma-buf allocations only.
+ */
+struct dma_buf *rk_dma_heap_buffer_alloc(struct rk_dma_heap *heap, size_t len,
+					 unsigned int fd_flags,
+					 unsigned int heap_flags,
+					 const char *name);
+
+/**
+ * rk_dma_heap_bufferfd_alloc - Allocate dma-buf fd from a dma_heap
+ * @heap:	dma_heap to allocate from
+ * @len:	size to allocate
+ * @fd_flags:	flags to set on returned dma-buf fd
+ * @heap_flags:	flags to pass to the dma heap
+ */
+int rk_dma_heap_bufferfd_alloc(struct rk_dma_heap *heap, size_t len,
+			       unsigned int fd_flags,
+			       unsigned int heap_flags,
+			       const char *name);
+
+/**
+ * rk_dma_heap_alloc_contig_pages - Allocate contiguous pages from a dma_heap
+ * @heap:	dma_heap to allocate from
+ * @len:	size to allocate
+ * @name:	the name who allocate
+ */
+struct page *rk_dma_heap_alloc_contig_pages(struct rk_dma_heap *heap,
+					    size_t len, const char *name);
+
+/**
+ * rk_dma_heap_free_contig_pages - Free contiguous pages to a dma_heap
+ * @heap:	dma_heap to free to
+ * @pages:	pages to free to
+ * @len:	size to free
+ * @name:	the name who allocate
+ */
+void rk_dma_heap_free_contig_pages(struct rk_dma_heap *heap, struct page *pages,
+				   size_t len, const char *name);
+
+#else
+static inline int rk_dma_heap_cma_setup(void)
+{
+	return -ENODEV;
+}
+
+static inline int rk_dma_heap_set_dev(struct device *heap_dev)
+{
+	return -ENODEV;
+}
+
+static inline struct rk_dma_heap *rk_dma_heap_find(const char *name)
+{
+	return NULL;
+}
+
+static inline void rk_dma_heap_buffer_free(struct dma_buf *dmabuf)
+{
+}
+
+static inline struct dma_buf *rk_dma_heap_buffer_alloc(struct rk_dma_heap *heap, size_t len,
+						       unsigned int fd_flags,
+						       unsigned int heap_flags,
+						       const char *name)
+{
+	return NULL;
+}
+
+static inline int rk_dma_heap_bufferfd_alloc(struct rk_dma_heap *heap, size_t len,
+					     unsigned int fd_flags,
+					     unsigned int heap_flags,
+					     const char *name)
+{
+	return -ENODEV;
+}
+
+static inline struct page *rk_dma_heap_alloc_contig_pages(struct rk_dma_heap *heap,
+							  size_t len, const char *name)
+{
+	return NULL;
+}
+
+static inline void rk_dma_heap_free_contig_pages(struct rk_dma_heap *heap, struct page *pages,
+						 size_t len, const char *name)
+{
+}
+#endif
+#endif /* _DMA_HEAPS_H */
\ No newline at end of file
diff --git a/include/linux/soc/rockchip/pvtm.h b/include/linux/soc/rockchip/pvtm.h
new file mode 100644
index 000000000..3d2495cfd
--- /dev/null
+++ b/include/linux/soc/rockchip/pvtm.h
@@ -0,0 +1,17 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __SOC_ROCKCHIP_PVTM_H
+#define __SOC_ROCKCHIP_PVTM_H
+
+#if IS_ENABLED(CONFIG_ROCKCHIP_PVTM)
+u32 rockchip_get_pvtm_value(unsigned int id, unsigned int ring_sel,
+			    unsigned int time_us);
+#else
+static inline u32 rockchip_get_pvtm_value(unsigned int id,
+					  unsigned int ring_sel,
+					  unsigned int time_us)
+{
+	return 0;
+}
+#endif
+
+#endif /* __SOC_ROCKCHIP_PVTM_H */
diff --git a/include/linux/soc/rockchip/rk_fiq_debugger.h b/include/linux/soc/rockchip/rk_fiq_debugger.h
new file mode 100644
index 000000000..f5ec8d143
--- /dev/null
+++ b/include/linux/soc/rockchip/rk_fiq_debugger.h
@@ -0,0 +1,22 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __PLAT_RK_FIQ_DEBUGGER_H
+#define __PLAT_RK_FIQ_DEBUGGER_H
+
+#ifdef CONFIG_FIQ_DEBUGGER_TRUST_ZONE
+void fiq_debugger_fiq(void *regs, u32 cpu);
+
+#ifdef CONFIG_ARM_SDE_INTERFACE
+int sdei_fiq_debugger_is_enabled(void);
+int fiq_sdei_event_enable(u32 event_num);
+int fiq_sdei_event_routing_set(u32 event_num, unsigned long flags,
+			       unsigned long affinity);
+int fiq_sdei_event_disable(u32 event_num);
+#else
+static inline int sdei_fiq_debugger_is_enabled(void)
+{
+	return 0;
+}
+#endif
+#endif
+
+#endif
diff --git a/include/linux/soc/rockchip/rk_sdmmc.h b/include/linux/soc/rockchip/rk_sdmmc.h
new file mode 100644
index 000000000..14f70f72e
--- /dev/null
+++ b/include/linux/soc/rockchip/rk_sdmmc.h
@@ -0,0 +1,13 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __RK_SDMMC_H
+#define __RK_SDMMC_H
+
+#if IS_ENABLED(CONFIG_CPU_RV1106) && IS_REACHABLE(CONFIG_MMC_DW)
+void rv1106_sdmmc_get_lock(void);
+void rv1106_sdmmc_put_lock(void);
+#else
+static inline void rv1106_sdmmc_get_lock(void) {}
+static inline void rv1106_sdmmc_put_lock(void) {}
+#endif
+
+#endif
diff --git a/include/linux/soc/rockchip/rk_vendor_storage.h b/include/linux/soc/rockchip/rk_vendor_storage.h
new file mode 100644
index 000000000..29cee9bf6
--- /dev/null
+++ b/include/linux/soc/rockchip/rk_vendor_storage.h
@@ -0,0 +1,59 @@
+/*
+ * Copyright (c) 2016, Fuzhou Rockchip Electronics Co., Ltd
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or (at
+ * your option) any later version.
+ */
+
+#ifndef __PLAT_RK_VENDOR_STORAGE_H
+#define __PLAT_RK_VENDOR_STORAGE_H
+
+#define RSV_ID				0
+#define SN_ID				1
+#define WIFI_MAC_ID			2
+#define LAN_MAC_ID			3
+#define BT_MAC_ID			4
+#define HDCP_14_HDMI_ID			5
+#define HDCP_14_DP_ID			6
+#define HDCP_2X_ID			7
+#define DRM_KEY_ID			8
+#define PLAYREADY_CERT_ID		9
+#define ATTENTION_KEY_ID		10
+#define PLAYREADY_ROOT_KEY_0_ID		11
+#define PLAYREADY_ROOT_KEY_1_ID		12
+#define HDCP_14_HDMIRX_ID		13
+#define SENSOR_CALIBRATION_ID		14
+#define IMEI_ID				15
+#define LAN_RGMII_DL_ID			16
+#define EINK_VCOM_ID			17
+
+#if IS_REACHABLE(CONFIG_ROCKCHIP_VENDOR_STORAGE)
+int rk_vendor_read(u32 id, void *pbuf, u32 size);
+int rk_vendor_write(u32 id, void *pbuf, u32 size);
+int rk_vendor_register(void *read, void *write);
+bool is_rk_vendor_ready(void);
+#else
+static inline int rk_vendor_read(u32 id, void *pbuf, u32 size)
+{
+	return -1;
+}
+
+static inline int rk_vendor_write(u32 id, void *pbuf, u32 size)
+{
+	return -1;
+}
+
+static inline int rk_vendor_register(void *read, void *write)
+{
+	return -1;
+}
+
+static inline bool is_rk_vendor_ready(void)
+{
+	return false;
+}
+#endif
+
+#endif
diff --git a/include/linux/soc/rockchip/rockchip_decompress.h b/include/linux/soc/rockchip/rockchip_decompress.h
new file mode 100644
index 000000000..120ae907c
--- /dev/null
+++ b/include/linux/soc/rockchip/rockchip_decompress.h
@@ -0,0 +1,42 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/* Copyright (c) 2020 Rockchip Electronics Co., Ltd */
+
+#ifndef _ROCKCHIP_DECOMPRESS
+#define _ROCKCHIP_DECOMPRESS
+
+enum decom_mod {
+	LZ4_MOD,
+	GZIP_MOD,
+	ZLIB_MOD,
+};
+
+/* The high 16 bits indicate whether decompression is non-blocking */
+#define DECOM_NOBLOCKING		(0x00010000)
+
+static inline u32 rk_get_decom_mode(u32 mode)
+{
+	return mode & 0x0000ffff;
+}
+
+static inline bool rk_get_noblocking_flag(u32 mode)
+{
+	return !!(mode & DECOM_NOBLOCKING);
+}
+
+#ifdef CONFIG_ROCKCHIP_HW_DECOMPRESS
+int rk_decom_start(u32 mode, phys_addr_t src, phys_addr_t dst, u32 dst_max_size);
+/* timeout in seconds */
+int rk_decom_wait_done(u32 timeout, u64 *decom_len);
+#else
+static inline int rk_decom_start(u32 mode, phys_addr_t src, phys_addr_t dst, u32 dst_max_size)
+{
+	return -EINVAL;
+}
+
+static inline int rk_decom_wait_done(u32 timeout, u64 *decom_len)
+{
+	return -EINVAL;
+}
+#endif
+
+#endif
diff --git a/include/linux/soc/rockchip/rockchip_thunderboot_crypto.h b/include/linux/soc/rockchip/rockchip_thunderboot_crypto.h
new file mode 100644
index 000000000..2fe176649
--- /dev/null
+++ b/include/linux/soc/rockchip/rockchip_thunderboot_crypto.h
@@ -0,0 +1,9 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/* Copyright (c) 2021 Rockchip Electronics Co., Ltd */
+
+#ifndef _ROCKCHIP_THUNDERBOOT_CRYPTO_
+#define _ROCKCHIP_THUNDERBOOT_CRYPTO_
+
+int rk_tb_sha256(dma_addr_t data, size_t data_len, void *user_data);
+
+#endif
diff --git a/include/linux/soc/rockchip/rockchip_thunderboot_service.h b/include/linux/soc/rockchip/rockchip_thunderboot_service.h
new file mode 100644
index 000000000..5ab1cf490
--- /dev/null
+++ b/include/linux/soc/rockchip/rockchip_thunderboot_service.h
@@ -0,0 +1,29 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/* Copyright (c) 2022 Rockchip Electronics Co., Ltd */
+
+#ifndef _ROCKCHIP_THUNDERBOOT_SERVICE_H
+#define _ROCKCHIP_THUNDERBOOT_SERVICE_H
+
+struct rk_tb_client {
+	struct list_head node;
+	void *data;
+	void (*cb)(void *data);
+};
+
+#ifdef CONFIG_ROCKCHIP_THUNDER_BOOT_SERVICE
+bool rk_tb_mcu_is_done(void);
+int rk_tb_client_register_cb(struct rk_tb_client *client);
+#else
+static inline bool rk_tb_mcu_is_done(void)
+{
+	return true;
+}
+static inline int rk_tb_client_register_cb(struct rk_tb_client *client)
+{
+	if (client && client->cb)
+		client->cb(client->data);
+
+	return 0;
+}
+#endif
+#endif
diff --git a/include/soc/rockchip/rockchip_iommu.h b/include/soc/rockchip/rockchip_iommu.h
new file mode 100644
index 000000000..804f0fdc4
--- /dev/null
+++ b/include/soc/rockchip/rockchip_iommu.h
@@ -0,0 +1,39 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2021 Rockchip Electronics Co., Ltd
+ */
+#ifndef __SOC_ROCKCHIP_IOMMU_H
+#define __SOC_ROCKCHIP_IOMMU_H
+
+struct device;
+
+#if IS_ENABLED(CONFIG_ROCKCHIP_IOMMU)
+int rockchip_iommu_enable(struct device *dev);
+int rockchip_iommu_disable(struct device *dev);
+int rockchip_pagefault_done(struct device *master_dev);
+void __iomem *rockchip_get_iommu_base(struct device *master_dev, int idx);
+bool rockchip_iommu_is_enabled(struct device *dev);
+#else
+static inline int rockchip_iommu_enable(struct device *dev)
+{
+	return -ENODEV;
+}
+static inline int rockchip_iommu_disable(struct device *dev)
+{
+	return -ENODEV;
+}
+static inline int rockchip_pagefault_done(struct device *master_dev)
+{
+	return 0;
+}
+static inline void __iomem *rockchip_get_iommu_base(struct device *master_dev, int idx)
+{
+	return NULL;
+}
+static inline bool rockchip_iommu_is_enabled(struct device *dev)
+{
+	return false;
+}
+#endif
+
+#endif
diff --git a/include/soc/rockchip/rockchip_ipa.h b/include/soc/rockchip/rockchip_ipa.h
new file mode 100644
index 000000000..cb333f463
--- /dev/null
+++ b/include/soc/rockchip/rockchip_ipa.h
@@ -0,0 +1,40 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2018 Fuzhou Rockchip Electronics Co., Ltd
+ */
+#ifndef __SOC_ROCKCHIP_IPA_H
+#define __SOC_ROCKCHIP_IPA_H
+
+struct ipa_power_model_data {
+	u32 static_coefficient;
+	u32 dynamic_coefficient;
+	s32 ts[4];			/* temperature scaling factor */
+	struct thermal_zone_device *tz;
+	u32 leakage;
+	u32 ref_leakage;
+	u32 lkg_range[2];		/* min leakage and max leakage */
+	s32 ls[3];			/* leakage scaling factor */
+};
+
+#if IS_ENABLED(CONFIG_ROCKCHIP_IPA)
+struct ipa_power_model_data *rockchip_ipa_power_model_init(struct device *dev,
+							   char *lkg_name);
+unsigned long
+rockchip_ipa_get_static_power(struct ipa_power_model_data *model_data,
+			      unsigned long voltage_mv);
+#else
+static inline struct ipa_power_model_data *
+rockchip_ipa_power_model_init(struct device *dev, char *lkg_name)
+{
+	return ERR_PTR(-ENOTSUPP);
+};
+
+static inline unsigned long
+rockchip_ipa_get_static_power(struct ipa_power_model_data *data,
+			      unsigned long voltage_mv)
+{
+	return 0;
+}
+#endif /* CONFIG_ROCKCHIP_IPA */
+
+#endif
diff --git a/include/soc/rockchip/rockchip_opp_select.h b/include/soc/rockchip/rockchip_opp_select.h
new file mode 100644
index 000000000..da9cab568
--- /dev/null
+++ b/include/soc/rockchip/rockchip_opp_select.h
@@ -0,0 +1,231 @@
+/*
+ * Copyright (c) 2017 Fuzhou Rockchip Electronics Co., Ltd
+ *
+ * SPDX-License-Identifier: GPL-2.0+
+ */
+#ifndef __SOC_ROCKCHIP_OPP_SELECT_H
+#define __SOC_ROCKCHIP_OPP_SELECT_H
+
+#define VOLT_RM_TABLE_END	~1
+
+#define OPP_INTERMEDIATE_MASK	0x3f
+#define OPP_INTERMEDIATE_RATE	BIT(0)
+#define OPP_SCALING_UP_RATE	BIT(1)
+#define OPP_SCALING_UP_INTER	(OPP_INTERMEDIATE_RATE | OPP_SCALING_UP_RATE)
+#define OPP_SCALING_DOWN_INTER	OPP_INTERMEDIATE_RATE
+#define OPP_LENGTH_LOW		BIT(2)
+
+struct rockchip_opp_info;
+
+struct volt_rm_table {
+	int volt;
+	int rm;
+};
+
+struct rockchip_opp_data {
+	int (*get_soc_info)(struct device *dev, struct device_node *np,
+			    int *bin, int *process);
+	int (*set_soc_info)(struct device *dev, struct device_node *np,
+			    int bin, int process, int volt_sel);
+	int (*set_read_margin)(struct device *dev,
+			       struct rockchip_opp_info *opp_info,
+			       u32 rm);
+};
+
+struct pvtpll_opp_table {
+	unsigned long rate;
+	unsigned long u_volt;
+	unsigned long u_volt_min;
+	unsigned long u_volt_max;
+	unsigned long u_volt_mem;
+	unsigned long u_volt_mem_min;
+	unsigned long u_volt_mem_max;
+};
+
+struct rockchip_opp_info {
+	struct device *dev;
+	struct pvtpll_opp_table *opp_table;
+	const struct rockchip_opp_data *data;
+	struct volt_rm_table *volt_rm_tbl;
+	struct regmap *grf;
+	struct regmap *dsu_grf;
+	struct clk_bulk_data *clks;
+	struct clk *scmi_clk;
+	/* The threshold frequency for set intermediate rate */
+	unsigned long intermediate_threshold_freq;
+	unsigned int pvtpll_avg_offset;
+	unsigned int pvtpll_min_rate;
+	unsigned int pvtpll_volt_step;
+	int num_clks;
+	/* The read margin for low voltage */
+	u32 low_rm;
+	u32 current_rm;
+	u32 target_rm;
+};
+
+#if IS_ENABLED(CONFIG_ROCKCHIP_OPP)
+int rockchip_of_get_leakage(struct device *dev, char *lkg_name, int *leakage);
+void rockchip_of_get_lkg_sel(struct device *dev, struct device_node *np,
+			     char *lkg_name, int process,
+			     int *volt_sel, int *scale_sel);
+void rockchip_pvtpll_calibrate_opp(struct rockchip_opp_info *info);
+void rockchip_of_get_pvtm_sel(struct device *dev, struct device_node *np,
+			      char *reg_name, int process,
+			      int *volt_sel, int *scale_sel);
+void rockchip_of_get_bin_sel(struct device *dev, struct device_node *np,
+			     int bin, int *scale_sel);
+void rockchip_of_get_bin_volt_sel(struct device *dev, struct device_node *np,
+				  int bin, int *bin_volt_sel);
+int rockchip_nvmem_cell_read_u8(struct device_node *np, const char *cell_id,
+				u8 *val);
+int rockchip_nvmem_cell_read_u16(struct device_node *np, const char *cell_id,
+				 u16 *val);
+int rockchip_get_volt_rm_table(struct device *dev, struct device_node *np,
+			       char *porp_name, struct volt_rm_table **table);
+void rockchip_get_opp_data(const struct of_device_id *matches,
+			   struct rockchip_opp_info *info);
+void rockchip_get_scale_volt_sel(struct device *dev, char *lkg_name,
+				 char *reg_name, int bin, int process,
+				 int *scale, int *volt_sel);
+struct opp_table *rockchip_set_opp_prop_name(struct device *dev, int process,
+					     int volt_sel);
+int rockchip_adjust_power_scale(struct device *dev, int scale);
+int rockchip_get_read_margin(struct device *dev,
+			     struct rockchip_opp_info *opp_info,
+			     unsigned long volt, u32 *target_rm);
+int rockchip_set_read_margin(struct device *dev,
+			     struct rockchip_opp_info *opp_info, u32 rm,
+			     bool is_set_rm);
+int rockchip_init_read_margin(struct device *dev,
+			      struct rockchip_opp_info *opp_info,
+			      char *reg_name);
+int rockchip_set_intermediate_rate(struct device *dev,
+				   struct rockchip_opp_info *opp_info,
+				   struct clk *clk, unsigned long old_freq,
+				   unsigned long new_freq, bool is_scaling_up,
+				   bool is_set_clk);
+int rockchip_init_opp_table(struct device *dev,
+			    struct rockchip_opp_info *info,
+			    char *lkg_name, char *reg_name);
+#else
+static inline int rockchip_of_get_leakage(struct device *dev, char *lkg_name,
+					  int *leakage)
+{
+	return -EOPNOTSUPP;
+}
+
+static inline void rockchip_of_get_lkg_sel(struct device *dev,
+					   struct device_node *np,
+					   char *lkg_name, int process,
+					   int *volt_sel, int *scale_sel)
+{
+}
+
+static inline void rockchip_pvtpll_calibrate_opp(struct rockchip_opp_info *info)
+{
+}
+
+static inline void rockchip_of_get_pvtm_sel(struct device *dev,
+					    struct device_node *np,
+					    char *reg_name, int process,
+					    int *volt_sel, int *scale_sel)
+{
+}
+
+static inline void rockchip_of_get_bin_sel(struct device *dev,
+					   struct device_node *np, int bin,
+					   int *scale_sel)
+{
+}
+
+static inline void rockchip_of_get_bin_volt_sel(struct device *dev,
+						struct device_node *np,
+						int bin, int *bin_volt_sel)
+{
+}
+
+static inline int rockchip_nvmem_cell_read_u8(struct device_node *np,
+					      const char *cell_id, u8 *val)
+{
+	return -EOPNOTSUPP;
+}
+
+static inline int rockchip_nvmem_cell_read_u16(struct device_node *np,
+					       const char *cell_id, u16 *val)
+{
+	return -EOPNOTSUPP;
+}
+
+static inline int rockchip_get_volt_rm_table(struct device *dev,
+					     struct device_node *np,
+					     char *porp_name,
+					     struct volt_rm_table **table)
+{
+	return -EOPNOTSUPP;
+
+}
+
+static inline void rockchip_get_opp_data(const struct of_device_id *matches,
+					 struct rockchip_opp_info *info)
+{
+}
+
+static inline void rockchip_get_scale_volt_sel(struct device *dev,
+					       char *lkg_name, char *reg_name,
+					       int bin, int process, int *scale,
+					       int *volt_sel)
+{
+}
+
+static inline struct opp_table *rockchip_set_opp_prop_name(struct device *dev,
+							   int process,
+							   int volt_sel)
+{
+	return ERR_PTR(-EOPNOTSUPP);
+}
+
+static inline int rockchip_adjust_power_scale(struct device *dev, int scale)
+{
+	return -EOPNOTSUPP;
+}
+
+static inline int rockchip_get_read_margin(struct device *dev,
+					   struct rockchip_opp_info *opp_info,
+					   unsigned long volt, u32 *target_rm)
+{
+	return -EOPNOTSUPP;
+}
+static inline int rockchip_set_read_margin(struct device *dev,
+					   struct rockchip_opp_info *opp_info,
+					   u32 rm, bool is_set_rm)
+{
+	return -EOPNOTSUPP;
+}
+
+static inline int rockchip_init_read_margin(struct device *dev,
+					    struct rockchip_opp_info *opp_info,
+					    char *reg_name)
+{
+	return -EOPNOTSUPP;
+}
+
+static inline int
+rockchip_set_intermediate_rate(struct device *dev,
+			       struct rockchip_opp_info *opp_info,
+			       struct clk *clk, unsigned long old_freq,
+			       unsigned long new_freq, bool is_scaling_up,
+			       bool is_set_clk)
+{
+	return -EOPNOTSUPP;
+}
+
+static inline int rockchip_init_opp_table(struct device *dev,
+					  struct rockchip_opp_info *info,
+					  char *lkg_name, char *reg_name)
+{
+	return -EOPNOTSUPP;
+}
+
+#endif /* CONFIG_ROCKCHIP_OPP */
+
+#endif
diff --git a/include/soc/rockchip/rockchip_system_monitor.h b/include/soc/rockchip/rockchip_system_monitor.h
new file mode 100644
index 000000000..a8f500651
--- /dev/null
+++ b/include/soc/rockchip/rockchip_system_monitor.h
@@ -0,0 +1,213 @@
+/* SPDX-License-Identifier: (GPL-2.0+ OR MIT) */
+/*
+ * Copyright (C) 2019, Fuzhou Rockchip Electronics Co., Ltd
+ */
+
+#ifndef __SOC_ROCKCHIP_SYSTEM_MONITOR_H
+#define __SOC_ROCKCHIP_SYSTEM_MONITOR_H
+
+enum monitor_dev_type {
+	MONITOR_TPYE_CPU = 0,	/* CPU */
+	MONITOR_TPYE_DEV,	/* GPU, NPU, DMC, and so on */
+};
+
+struct volt_adjust_table {
+	unsigned int min;	/* Minimum frequency in MHz */
+	unsigned int max;	/* Maximum frequency in MHz */
+	int volt;		/* Voltage in microvolt */
+};
+
+struct temp_freq_table {
+	int temp;		/* millicelsius */
+	unsigned int freq;	/* KHz */
+};
+
+/**
+ * struct temp_opp_table - System monitor device OPP description structure
+ * @rate:		Frequency in hertz
+ * @volt:		Target voltage in microvolt
+ * @mem_volt:		Target voltage for memory in microvolt
+ * @low_temp_volt:	Target voltage when low temperature, in microvolt
+ * @low_temp_mem_volt:	Target voltage for memory when low temperature,
+ *			in microvolt
+ * @max_volt:		Maximum voltage in microvolt
+ * @max_mem_volt:	Maximum voltage for memory in microvolt
+ */
+struct temp_opp_table {
+	unsigned long rate;
+	unsigned long volt;
+	unsigned long mem_volt;
+	unsigned long low_temp_volt;
+	unsigned long low_temp_mem_volt;
+	unsigned long max_volt;
+	unsigned long max_mem_volt;
+};
+
+/**
+ * struct monitor_dev_info - structure for a system monitor device
+ * @dev:		Device registered by system monitor
+ * @low_temp_adjust_table:	Voltage margin for different OPPs when lowe
+ *				temperature
+ * @opp_table:		Frequency and voltage information of device
+ * @devp:		Device-specific system monitor profile
+ * @node:		Node in monitor_dev_list
+ * @high_limit_table:	Limit maximum frequency at different temperature,
+ *			but the frequency is also changed by thermal framework.
+ * @volt_adjust_mutex:	A mutex to protect changing voltage.
+ * @max_temp_freq_req:	CPU maximum frequency constraint changed according
+ *			to temperature.
+ * @min_sta_freq_req:   CPU minimum frequency constraint changed according
+ *			to system status.
+ * @max_sta_freq_req:   CPU maximum frequency constraint changed according
+ *			to system status.
+ * @dev_max_freq_req:	Devices maximum frequency constraint changed according
+ *			to temperature.
+ * @low_limit:		Limit maximum frequency when low temperature, in Hz
+ * @high_limit:		Limit maximum frequency when high temperature, in Hz
+ * @max_volt:		Maximum voltage in microvolt
+ * @low_temp_min_volt:	Minimum voltage of OPPs when low temperature, in
+ *			microvolt
+ * @high_temp_max_volt:	Maximum voltage when high temperature, in microvolt
+ * @wide_temp_limit:	Target maximum frequency when low or high temperature,
+ *			in Hz
+ * @video_4k_freq:	Maximum frequency when paly 4k video, in KHz
+ * @reboot_freq:	Limit maximum and minimum frequency when reboot, in KHz
+ * @status_min_limit:	Minimum frequency of some status frequency, in KHz
+ * @status_max_limit:	Minimum frequency of all status frequency, in KHz
+ * @low_temp:		Low temperature trip point, in millicelsius
+ * @high_temp:		High temperature trip point, in millicelsius
+ * @temp_hysteresis:	A low hysteresis value on low_temp, in millicelsius
+ * @is_low_temp:	True if current temperature less than low_temp
+ * @is_high_temp:	True if current temperature greater than high_temp
+ * @is_low_temp_enabled:	True if device node contains low temperature
+ *				configuration
+ * @is_status_freq_fixed:	True if enter into some status
+ */
+struct monitor_dev_info {
+	struct device *dev;
+	struct volt_adjust_table *low_temp_adjust_table;
+	struct temp_opp_table *opp_table;
+	struct monitor_dev_profile *devp;
+	struct list_head node;
+	struct temp_freq_table *high_limit_table;
+	struct mutex volt_adjust_mutex;
+	struct freq_qos_request max_temp_freq_req;
+	struct freq_qos_request min_sta_freq_req;
+	struct freq_qos_request max_sta_freq_req;
+	struct dev_pm_qos_request dev_max_freq_req;
+	struct regulator *early_reg;
+	struct regulator **regulators;
+	struct dev_pm_set_opp_data *set_opp_data;
+	struct clk *clk;
+	unsigned long low_limit;
+	unsigned long high_limit;
+	unsigned long max_volt;
+	unsigned long low_temp_min_volt;
+	unsigned long high_temp_max_volt;
+	unsigned int video_4k_freq;
+	unsigned int reboot_freq;
+	unsigned int init_freq;
+	unsigned int status_min_limit;
+	unsigned int status_max_limit;
+	unsigned int early_min_volt;
+	unsigned int regulator_count;
+	int low_temp;
+	int high_temp;
+	int temp_hysteresis;
+	bool is_low_temp;
+	bool is_high_temp;
+	bool is_low_temp_enabled;
+};
+
+struct monitor_dev_profile {
+	enum monitor_dev_type type;
+	void *data;
+	bool is_checked;
+	int (*low_temp_adjust)(struct monitor_dev_info *info, bool is_low);
+	int (*high_temp_adjust)(struct monitor_dev_info *info, bool is_low);
+	int (*update_volt)(struct monitor_dev_info *info);
+	int (*set_opp)(struct dev_pm_set_opp_data *data);
+	struct cpumask allowed_cpus;
+	struct rockchip_opp_info *opp_info;
+};
+
+#if IS_ENABLED(CONFIG_ROCKCHIP_SYSTEM_MONITOR)
+struct monitor_dev_info *
+rockchip_system_monitor_register(struct device *dev,
+				 struct monitor_dev_profile *devp);
+void rockchip_system_monitor_unregister(struct monitor_dev_info *info);
+int rockchip_monitor_cpu_low_temp_adjust(struct monitor_dev_info *info,
+					 bool is_low);
+int rockchip_monitor_cpu_high_temp_adjust(struct monitor_dev_info *info,
+					  bool is_high);
+void rockchip_monitor_volt_adjust_lock(struct monitor_dev_info *info);
+void rockchip_monitor_volt_adjust_unlock(struct monitor_dev_info *info);
+int rockchip_monitor_check_rate_volt(struct monitor_dev_info *info);
+int rockchip_monitor_dev_low_temp_adjust(struct monitor_dev_info *info,
+					 bool is_low);
+int rockchip_monitor_dev_high_temp_adjust(struct monitor_dev_info *info,
+					  bool is_high);
+int rockchip_monitor_suspend_low_temp_adjust(int cpu);
+#else
+static inline struct monitor_dev_info *
+rockchip_system_monitor_register(struct device *dev,
+				 struct monitor_dev_profile *devp)
+{
+	return ERR_PTR(-ENOTSUPP);
+};
+
+static inline void
+rockchip_system_monitor_unregister(struct monitor_dev_info *info)
+{
+}
+
+static inline int
+rockchip_monitor_cpu_low_temp_adjust(struct monitor_dev_info *info, bool is_low)
+{
+	return 0;
+};
+
+static inline int
+rockchip_monitor_cpu_high_temp_adjust(struct monitor_dev_info *info,
+				      bool is_high)
+{
+	return 0;
+};
+
+static inline void
+rockchip_monitor_volt_adjust_lock(struct monitor_dev_info *info)
+{
+}
+
+static inline void
+rockchip_monitor_volt_adjust_unlock(struct monitor_dev_info *info)
+{
+}
+
+static inline int
+rockchip_monitor_check_rate_volt(struct monitor_dev_info *info)
+{
+	return 0;
+}
+
+static inline int
+rockchip_monitor_dev_low_temp_adjust(struct monitor_dev_info *info, bool is_low)
+{
+	return 0;
+};
+
+static inline int
+rockchip_monitor_dev_high_temp_adjust(struct monitor_dev_info *info,
+				      bool is_high)
+{
+	return 0;
+};
+
+static inline int rockchip_monitor_suspend_low_temp_adjust(int cpu)
+{
+	return 0;
+};
+
+#endif /* CONFIG_ROCKCHIP_SYSTEM_MONITOR */
+
+#endif
diff --git a/include/uapi/linux/rk-dma-heap.h b/include/uapi/linux/rk-dma-heap.h
new file mode 100644
index 000000000..68edb3832
--- /dev/null
+++ b/include/uapi/linux/rk-dma-heap.h
@@ -0,0 +1,55 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ * DMABUF Heaps Userspace API
+ *
+ * Copyright (C) 2011 Google, Inc.
+ * Copyright (C) 2019 Linaro Ltd.
+ * Copyright (C) 2022 Rockchip Electronics Co. Ltd.
+ * Author: Simon Xue <xxm@rock-chips.com>
+ */
+#ifndef _UAPI_LINUX_DMABUF_POOL_H
+#define _UAPI_LINUX_DMABUF_POOL_H
+
+#include <linux/ioctl.h>
+#include <linux/types.h>
+
+/**
+ * DOC: DMABUF Heaps Userspace API
+ */
+
+/* Valid FD_FLAGS are O_CLOEXEC, O_RDONLY, O_WRONLY, O_RDWR */
+#define RK_DMA_HEAP_VALID_FD_FLAGS (O_CLOEXEC | O_ACCMODE)
+
+/* Currently no heap flags */
+#define RK_DMA_HEAP_VALID_HEAP_FLAGS (0)
+
+/**
+ * struct rk_dma_heap_allocation_data - metadata passed from userspace for
+ *                                      allocations
+ * @len:		size of the allocation
+ * @fd:			will be populated with a fd which provides the
+ *			handle to the allocated dma-buf
+ * @fd_flags:		file descriptor flags used when allocating
+ * @heap_flags:		flags passed to heap
+ *
+ * Provided by userspace as an argument to the ioctl
+ */
+struct rk_dma_heap_allocation_data {
+	__u64 len;
+	__u32 fd;
+	__u32 fd_flags;
+	__u64 heap_flags;
+};
+
+#define RK_DMA_HEAP_IOC_MAGIC		'R'
+
+/**
+ * DOC: RK_DMA_HEAP_IOCTL_ALLOC - allocate memory from pool
+ *
+ * Takes a rk_dma_heap_allocation_data struct and returns it with the fd field
+ * populated with the dmabuf handle of the allocation.
+ */
+#define RK_DMA_HEAP_IOCTL_ALLOC	_IOWR(RK_DMA_HEAP_IOC_MAGIC, 0x0,\
+				      struct rk_dma_heap_allocation_data)
+
+#endif /* _UAPI_LINUX_DMABUF_POOL_H */
\ No newline at end of file
