diff --git a/arch/arm64/boot/dts/rockchip/rk3566-orangepi-3b.dts b/arch/arm64/boot/dts/rockchip/rk3566-orangepi-3b.dts
index 6f058a852..2997a8433 100644
--- a/arch/arm64/boot/dts/rockchip/rk3566-orangepi-3b.dts
+++ b/arch/arm64/boot/dts/rockchip/rk3566-orangepi-3b.dts
@@ -739,10 +739,6 @@ &rknpu {
 	status = "okay";
 };
 
-&rknpu_mmu {
-	status = "okay";
-};
-
 &saradc {
 	status = "okay";
 	vref-supply = <&vcca_1v8>;
diff --git a/arch/arm64/boot/dts/rockchip/rk3566.dtsi b/arch/arm64/boot/dts/rockchip/rk3566.dtsi
index 6c4b17d27..0570451c0 100644
--- a/arch/arm64/boot/dts/rockchip/rk3566.dtsi
+++ b/arch/arm64/boot/dts/rockchip/rk3566.dtsi
@@ -4,6 +4,240 @@
 
 / {
 	compatible = "rockchip,rk3566";
+
+	rknpu: npu@fde40000 {
+		compatible = "rockchip,rk3568-rknpu", "rockchip,rknpu";
+		reg = <0x0 0xfde40000 0x0 0x10000>;
+		interrupts = <GIC_SPI 151 IRQ_TYPE_LEVEL_HIGH>;
+		clocks = <&scmi_clk 2>, <&cru CLK_NPU>, <&cru ACLK_NPU>, <&cru HCLK_NPU>;
+		clock-names = "scmi_clk", "clk", "aclk", "hclk";
+		assigned-clocks = <&cru CLK_NPU>;
+		assigned-clock-rates = <600000000>;
+		resets = <&cru SRST_A_NPU>, <&cru SRST_H_NPU>;
+		reset-names = "srst_a", "srst_h";
+		power-domains = <&power RK3568_PD_NPU>;
+		operating-points-v2 = <&npu_opp_table>;
+		iommus = <&rknpu_mmu>;
+		status = "disabled";
+	};
+
+	bus_npu: bus-npu {
+		compatible = "rockchip,rk3568-bus";
+		rockchip,busfreq-policy = "clkfreq";
+		clocks = <&scmi_clk 2>;
+		clock-names = "bus";
+		operating-points-v2 = <&bus_npu_opp_table>;
+		status = "disabled";
+	};
+
+	bus_npu_opp_table: bus-npu-opp-table {
+		compatible = "operating-points-v2";
+		opp-shared;
+
+		nvmem-cells = <&core_pvtm>;
+		nvmem-cell-names = "pvtm";
+		rockchip,pvtm-voltage-sel = <
+			0        84000   0
+			84001    91000   1
+			91001    100000  2
+		>;
+		rockchip,pvtm-ch = <0 5>;
+
+		opp-700000000 {
+			opp-hz = /bits/ 64 <700000000>;
+			opp-microvolt = <900000>;
+			opp-microvolt-L0 = <900000>;
+			opp-microvolt-L1 = <850000>;
+			opp-microvolt-L2 = <850000>;
+		};
+		opp-900000000 {
+			opp-hz = /bits/ 64 <900000000>;
+			opp-microvolt = <900000>;
+		};
+		opp-1000000000 {
+			opp-hz = /bits/ 64 <1000000000>;
+			opp-microvolt = <950000>;
+			opp-microvolt-L0 = <950000>;
+			opp-microvolt-L1 = <925000>;
+			opp-microvolt-L2 = <850000>;
+		};
+	};
+
+	rknpu_mmu: iommu@fde4b000 {
+		compatible = "rockchip,iommu-v2";
+		reg = <0x0 0xfde4b000 0x0 0x40>;
+		interrupts = <GIC_SPI 151 IRQ_TYPE_LEVEL_HIGH>;
+		interrupt-names = "rknpu_mmu";
+		clocks = <&cru ACLK_NPU>, <&cru HCLK_NPU>;
+		clock-names = "aclk", "iface";
+		power-domains = <&power RK3568_PD_NPU>;
+		#iommu-cells = <0>;
+		status = "disabled";
+	};
+
+	npu_opp_table: npu-opp-table {
+		compatible = "operating-points-v2";
+
+		mbist-vmin = <825000 900000 950000>;
+		nvmem-cells = <&npu_leakage>, <&core_pvtm>, <&mbist_vmin>, <&npu_opp_info>;
+		nvmem-cell-names = "leakage", "pvtm", "mbist-vmin", "opp-info";
+		rockchip,max-volt = <1000000>;
+		rockchip,temp-hysteresis = <5000>;
+		rockchip,low-temp = <0>;
+		rockchip,low-temp-adjust-volt = <
+			/* MHz    MHz    uV */
+			   0      1000    50000
+		>;
+		rockchip,pvtm-voltage-sel = <
+			0        84000   0
+			84001    87000   1
+			87001    91000   2
+			91001    100000  3
+		>;
+		rockchip,pvtm-ch = <0 5>;
+
+		opp-200000000 {
+			opp-hz = /bits/ 64 <200000000>;
+			opp-microvolt = <850000 850000 1000000>;
+		};
+		opp-300000000 {
+			opp-hz = /bits/ 64 <297000000>;
+			opp-microvolt = <850000 850000 1000000>;
+		};
+		opp-400000000 {
+			opp-hz = /bits/ 64 <400000000>;
+			opp-microvolt = <850000 850000 1000000>;
+		};
+		opp-600000000 {
+			opp-hz = /bits/ 64 <600000000>;
+			opp-microvolt = <850000 850000 1000000>;
+		};
+		opp-700000000 {
+			opp-hz = /bits/ 64 <700000000>;
+			opp-microvolt = <875000 875000 1000000>;
+			opp-microvolt-L0 = <875000 875000 1000000>;
+			opp-microvolt-L1 = <850000 850000 1000000>;
+			opp-microvolt-L2 = <850000 850000 1000000>;
+			opp-microvolt-L3 = <850000 850000 1000000>;
+		};
+		opp-800000000 {
+			opp-hz = /bits/ 64 <800000000>;
+			opp-microvolt = <925000 925000 1000000>;
+			opp-microvolt-L0 = <925000 925000 1000000>;
+			opp-microvolt-L1 = <900000 900000 1000000>;
+			opp-microvolt-L2 = <875000 875000 1000000>;
+			opp-microvolt-L3 = <875000 875000 1000000>;
+		};
+		opp-900000000 {
+			opp-hz = /bits/ 64 <900000000>;
+			opp-microvolt = <975000 975000 1000000>;
+			opp-microvolt-L0 = <975000 975000 1000000>;
+			opp-microvolt-L1 = <950000 950000 1000000>;
+			opp-microvolt-L2 = <925000 925000 1000000>;
+			opp-microvolt-L3 = <900000 900000 1000000>;
+		};
+		opp-1000000000 {
+			opp-hz = /bits/ 64 <1000000000>;
+			opp-microvolt = <1000000 1000000 1000000>;
+			opp-microvolt-L0 = <1000000 1000000 1000000>;
+			opp-microvolt-L1 = <975000 975000 1000000>;
+			opp-microvolt-L2 = <950000 950000 1000000>;
+			opp-microvolt-L3 = <925000 925000 1000000>;
+			status = "disabled";
+		};
+	};
+
+	otp: otp@fe38c000 {
+		compatible = "rockchip,rk3568-otp";
+		reg = <0x0 0xfe38c000 0x0 0x4000>;
+		#address-cells = <1>;
+		#size-cells = <1>;
+		clocks = <&cru CLK_OTPC_NS_USR>, <&cru CLK_OTPC_NS_SBPI>,
+			 <&cru PCLK_OTPC_NS>, <&cru PCLK_OTPPHY>;
+		clock-names = "usr", "sbpi", "apb", "phy";
+		resets = <&cru SRST_OTPPHY>;
+		reset-names = "otp_phy";
+
+		/* Data cells */
+		cpu_code: cpu-code@2 {
+			reg = <0x02 0x2>;
+		};
+		otp_cpu_version: cpu-version@8 {
+			reg = <0x08 0x1>;
+			bits = <3 3>;
+		};
+		mbist_vmin: mbist-vmin@9 {
+			reg = <0x09 0x1>;
+			bits = <0 4>;
+		};
+		otp_id: id@a {
+			reg = <0x0a 0x10>;
+		};
+		cpu_leakage: cpu-leakage@1a {
+			reg = <0x1a 0x1>;
+		};
+		log_leakage: log-leakage@1b {
+			reg = <0x1b 0x1>;
+		};
+		npu_leakage: npu-leakage@1c {
+			reg = <0x1c 0x1>;
+		};
+		gpu_leakage: gpu-leakage@1d {
+			reg = <0x1d 0x1>;
+		};
+		core_pvtm:core-pvtm@2a {
+			reg = <0x2a 0x2>;
+		};
+		cpu_tsadc_trim_l: cpu-tsadc-trim-l@2e {
+			reg = <0x2e 0x1>;
+		};
+		cpu_tsadc_trim_h: cpu-tsadc-trim-h@2f {
+			reg = <0x2f 0x1>;
+			bits = <0 4>;
+		};
+		gpu_tsadc_trim_l: npu-tsadc-trim-l@30 {
+			reg = <0x30 0x1>;
+		};
+		gpu_tsadc_trim_h: npu-tsadc-trim-h@31 {
+			reg = <0x31 0x1>;
+			bits = <0 4>;
+		};
+		tsadc_trim_base_frac: tsadc-trim-base-frac@31 {
+			reg = <0x31 0x1>;
+			bits = <4 4>;
+		};
+		tsadc_trim_base: tsadc-trim-base@32 {
+			reg = <0x32 0x1>;
+		};
+		cpu_opp_info: cpu-opp-info@36 {
+			reg = <0x36 0x6>;
+		};
+		gpu_opp_info: gpu-opp-info@3c {
+			reg = <0x3c 0x6>;
+		};
+		npu_opp_info: npu-opp-info@42 {
+			reg = <0x42 0x6>;
+		};
+		dmc_opp_info: dmc-opp-info@48 {
+			reg = <0x48 0x6>;
+		};
+	};
+
+	pvtm@fde90000 {
+		compatible = "rockchip,rk3568-npu-pvtm";
+		reg = <0x0 0xfde90000 0x0 0x100>;
+		#address-cells = <1>;
+		#size-cells = <0>;
+		pvtm@2 {
+			reg = <2>;
+			clocks = <&cru CLK_NPU_PVTM>, <&cru PCLK_NPU_PVTM>,
+				 <&cru HCLK_NPU_PRE>;
+			clock-names = "clk", "pclk", "hclk";
+			resets = <&cru SRST_NPU_PVTM>, <&cru SRST_P_NPU_PVTM>;
+			reset-names = "rts", "rst-p";
+			thermal-zone = "soc-thermal";
+		};
+	};
 };
 
 &pipegrf {
diff --git a/arch/arm64/boot/dts/rockchip/rk356x.dtsi b/arch/arm64/boot/dts/rockchip/rk356x.dtsi
index b7e2b475f..791bebb6f 100644
--- a/arch/arm64/boot/dts/rockchip/rk356x.dtsi
+++ b/arch/arm64/boot/dts/rockchip/rk356x.dtsi
@@ -509,6 +509,15 @@ power: power-controller {
 			#address-cells = <1>;
 			#size-cells = <0>;
 
+			/* These power domains are grouped by VD_NPU */
+			pd_npu@RK3568_PD_NPU {
+				reg = <RK3568_PD_NPU>;
+				clocks = <&cru ACLK_NPU_PRE>,
+					 <&cru HCLK_NPU_PRE>,
+					 <&cru PCLK_NPU_PRE>;
+				pm_qos = <&qos_npu>;
+			};
+			
 			/* These power domains are grouped by VD_GPU */
 			power-domain@RK3568_PD_GPU {
 				reg = <RK3568_PD_GPU>;
diff --git a/drivers/Kconfig b/drivers/Kconfig
index efb66e25f..c0da5f7e8 100644
--- a/drivers/Kconfig
+++ b/drivers/Kconfig
@@ -243,4 +243,6 @@ source "drivers/hte/Kconfig"
 
 source "drivers/cdx/Kconfig"
 
+source "drivers/rknpu/Kconfig"
+
 endmenu
diff --git a/drivers/Makefile b/drivers/Makefile
index 1bec7819a..af95975a2 100644
--- a/drivers/Makefile
+++ b/drivers/Makefile
@@ -199,3 +199,5 @@ obj-$(CONFIG_DRM_ACCEL)		+= accel/
 obj-$(CONFIG_CDX_BUS)		+= cdx/
 
 obj-$(CONFIG_S390)		+= s390/
+
+obj-$(CONFIG_ROCKCHIP_RKNPU)	+= rknpu/
\ No newline at end of file
diff --git a/drivers/clk/rockchip/clk-cpu.c b/drivers/clk/rockchip/clk-cpu.c
index 6ea7fba9f..19d6be12d 100644
--- a/drivers/clk/rockchip/clk-cpu.c
+++ b/drivers/clk/rockchip/clk-cpu.c
@@ -397,3 +397,168 @@ struct clk *rockchip_clk_register_cpuclk(const char *name,
 	kfree(cpuclk);
 	return ERR_PTR(ret);
 }
+
+static int rockchip_cpuclk_v2_pre_rate_change(struct rockchip_cpuclk *cpuclk,
+					      struct clk_notifier_data *ndata)
+{
+	unsigned long new_rate = roundup(ndata->new_rate, 1000);
+	const struct rockchip_cpuclk_rate_table *rate;
+	unsigned long flags;
+
+	rate = rockchip_get_cpuclk_settings(cpuclk, new_rate);
+	if (!rate) {
+		pr_err("%s: Invalid rate : %lu for cpuclk\n",
+		       __func__, new_rate);
+		return -EINVAL;
+	}
+
+	if (new_rate > ndata->old_rate) {
+		spin_lock_irqsave(cpuclk->lock, flags);
+		rockchip_cpuclk_set_dividers(cpuclk, rate);
+		spin_unlock_irqrestore(cpuclk->lock, flags);
+	}
+
+	return 0;
+}
+
+static int rockchip_cpuclk_v2_post_rate_change(struct rockchip_cpuclk *cpuclk,
+					       struct clk_notifier_data *ndata)
+{
+	unsigned long new_rate = roundup(ndata->new_rate, 1000);
+	const struct rockchip_cpuclk_rate_table *rate;
+	unsigned long flags;
+
+	rate = rockchip_get_cpuclk_settings(cpuclk, new_rate);
+	if (!rate) {
+		pr_err("%s: Invalid rate : %lu for cpuclk\n",
+		       __func__, new_rate);
+		return -EINVAL;
+	}
+
+	if (new_rate < ndata->old_rate) {
+		spin_lock_irqsave(cpuclk->lock, flags);
+		rockchip_cpuclk_set_dividers(cpuclk, rate);
+		spin_unlock_irqrestore(cpuclk->lock, flags);
+	}
+
+	return 0;
+}
+
+static int rockchip_cpuclk_v2_notifier_cb(struct notifier_block *nb,
+					  unsigned long event, void *data)
+{
+	struct clk_notifier_data *ndata = data;
+	struct rockchip_cpuclk *cpuclk = to_rockchip_cpuclk_nb(nb);
+	int ret = 0;
+
+	pr_debug("%s: event %lu, old_rate %lu, new_rate: %lu\n",
+		 __func__, event, ndata->old_rate, ndata->new_rate);
+	if (event == PRE_RATE_CHANGE)
+		ret = rockchip_cpuclk_v2_pre_rate_change(cpuclk, ndata);
+	else if (event == POST_RATE_CHANGE)
+		ret = rockchip_cpuclk_v2_post_rate_change(cpuclk, ndata);
+
+	return notifier_from_errno(ret);
+}
+
+struct clk *rockchip_clk_register_cpuclk_v2(const char *name,
+					    const char *const *parent_names,
+					    u8 num_parents, void __iomem *base,
+					    int muxdiv_offset, u8 mux_shift,
+					    u8 mux_width, u8 mux_flags,
+					    int div_offset, u8 div_shift,
+					    u8 div_width, u8 div_flags,
+					    unsigned long flags, spinlock_t *lock,
+					    const struct rockchip_cpuclk_rate_table *rates,
+					    int nrates)
+{
+	struct rockchip_cpuclk *cpuclk;
+	struct clk_hw *hw;
+	struct clk_mux *mux = NULL;
+	struct clk_divider *div = NULL;
+	const struct clk_ops *mux_ops = NULL, *div_ops = NULL;
+	int ret;
+
+	if (num_parents > 1) {
+		mux = kzalloc(sizeof(*mux), GFP_KERNEL);
+		if (!mux)
+			return ERR_PTR(-ENOMEM);
+
+		mux->reg = base + muxdiv_offset;
+		mux->shift = mux_shift;
+		mux->mask = BIT(mux_width) - 1;
+		mux->flags = mux_flags;
+		mux->lock = lock;
+		mux_ops = (mux_flags & CLK_MUX_READ_ONLY) ? &clk_mux_ro_ops
+							: &clk_mux_ops;
+	}
+
+	if (div_width > 0) {
+		div = kzalloc(sizeof(*div), GFP_KERNEL);
+		if (!div) {
+			ret = -ENOMEM;
+			goto free_mux;
+		}
+
+		div->flags = div_flags;
+		if (div_offset)
+			div->reg = base + div_offset;
+		else
+			div->reg = base + muxdiv_offset;
+		div->shift = div_shift;
+		div->width = div_width;
+		div->lock = lock;
+		div_ops = (div_flags & CLK_DIVIDER_READ_ONLY)
+						? &clk_divider_ro_ops
+						: &clk_divider_ops;
+	}
+
+	hw = clk_hw_register_composite(NULL, name, parent_names, num_parents,
+				       mux ? &mux->hw : NULL, mux_ops,
+				       div ? &div->hw : NULL, div_ops,
+				       NULL, NULL, flags);
+	if (IS_ERR(hw)) {
+		ret = PTR_ERR(hw);
+		goto free_div;
+	}
+
+	cpuclk = kzalloc(sizeof(*cpuclk), GFP_KERNEL);
+	if (!cpuclk) {
+		ret = -ENOMEM;
+		goto unregister_clk;
+	}
+
+	cpuclk->reg_base = base;
+	cpuclk->lock = lock;
+	cpuclk->clk_nb.notifier_call = rockchip_cpuclk_v2_notifier_cb;
+	ret = clk_notifier_register(hw->clk, &cpuclk->clk_nb);
+	if (ret) {
+		pr_err("%s: failed to register clock notifier for %s\n",
+		       __func__, name);
+		goto free_cpuclk;
+	}
+
+	if (nrates > 0) {
+		cpuclk->rate_count = nrates;
+		cpuclk->rate_table = kmemdup(rates,
+					     sizeof(*rates) * nrates,
+					     GFP_KERNEL);
+		if (!cpuclk->rate_table) {
+			ret = -ENOMEM;
+			goto free_cpuclk;
+		}
+	}
+
+	return hw->clk;
+
+free_cpuclk:
+	kfree(cpuclk);
+unregister_clk:
+	clk_hw_unregister_composite(hw);
+free_div:
+	kfree(div);
+free_mux:
+	kfree(mux);
+
+	return ERR_PTR(ret);
+}
\ No newline at end of file
diff --git a/drivers/clk/rockchip/clk-pll.c b/drivers/clk/rockchip/clk-pll.c
index 2d42eb628..8aa9c3014 100644
--- a/drivers/clk/rockchip/clk-pll.c
+++ b/drivers/clk/rockchip/clk-pll.c
@@ -15,6 +15,9 @@
 #include <linux/iopoll.h>
 #include <linux/regmap.h>
 #include <linux/clk.h>
+#include <linux/gcd.h>
+#include <linux/clk/rockchip.h>
+#include <linux/mfd/syscon.h>
 #include "clk.h"
 
 #define PLL_MODE_MASK		0x3
@@ -38,15 +41,400 @@ struct rockchip_clk_pll {
 	u8			flags;
 	const struct rockchip_pll_rate_table *rate_table;
 	unsigned int		rate_count;
+	int			sel;
+	unsigned long		scaling;
 	spinlock_t		*lock;
 
 	struct rockchip_clk_provider *ctx;
+
+#ifdef CONFIG_ROCKCHIP_CLK_BOOST
+	bool			boost_enabled;
+	u32			boost_backup_pll_usage;
+	unsigned long		boost_backup_pll_rate;
+	unsigned long		boost_low_rate;
+	unsigned long		boost_high_rate;
+	struct regmap		*boost;
+#endif
+#ifdef CONFIG_DEBUG_FS
+	struct hlist_node	debug_node;
+#endif
 };
 
 #define to_rockchip_clk_pll(_hw) container_of(_hw, struct rockchip_clk_pll, hw)
 #define to_rockchip_clk_pll_nb(nb) \
 			container_of(nb, struct rockchip_clk_pll, clk_nb)
 
+#ifdef CONFIG_ROCKCHIP_CLK_BOOST
+static void rockchip_boost_disable_low(struct rockchip_clk_pll *pll);
+#ifdef CONFIG_DEBUG_FS
+static HLIST_HEAD(clk_boost_list);
+static DEFINE_MUTEX(clk_boost_lock);
+#endif
+#else
+static inline void rockchip_boost_disable_low(struct rockchip_clk_pll *pll) {}
+#endif
+
+#define MHZ			(1000UL * 1000UL)
+#define KHZ			(1000UL)
+
+/* CLK_PLL_TYPE_RK3066_AUTO type ops */
+#define PLL_FREF_MIN		(269 * KHZ)
+#define PLL_FREF_MAX		(2200 * MHZ)
+
+#define PLL_FVCO_MIN		(440 * MHZ)
+#define PLL_FVCO_MAX		(2200 * MHZ)
+
+#define PLL_FOUT_MIN		(27500 * KHZ)
+#define PLL_FOUT_MAX		(2200 * MHZ)
+
+#define PLL_NF_MAX		(4096)
+#define PLL_NR_MAX		(64)
+#define PLL_NO_MAX		(16)
+
+/* CLK_PLL_TYPE_RK3036/3366/3399_AUTO type ops */
+#define MIN_FOUTVCO_FREQ	(800 * MHZ)
+#define MAX_FOUTVCO_FREQ	(2000 * MHZ)
+
+static struct rockchip_pll_rate_table auto_table;
+
+int rockchip_pll_clk_adaptive_scaling(struct clk *clk, int sel)
+{
+	struct clk *parent = clk_get_parent(clk);
+	struct rockchip_clk_pll *pll;
+
+	if (IS_ERR_OR_NULL(parent))
+		return -EINVAL;
+
+	pll = to_rockchip_clk_pll(__clk_get_hw(parent));
+	if (!pll)
+		return -EINVAL;
+
+	pll->sel = sel;
+
+	return 0;
+}
+EXPORT_SYMBOL(rockchip_pll_clk_adaptive_scaling);
+
+int rockchip_pll_clk_rate_to_scale(struct clk *clk, unsigned long rate)
+{
+	const struct rockchip_pll_rate_table *rate_table;
+	struct clk *parent = clk_get_parent(clk);
+	struct rockchip_clk_pll *pll;
+	unsigned int i;
+
+	if (IS_ERR_OR_NULL(parent))
+		return -EINVAL;
+
+	pll = to_rockchip_clk_pll(__clk_get_hw(parent));
+	if (!pll)
+		return -EINVAL;
+
+	rate_table = pll->rate_table;
+	for (i = 0; i < pll->rate_count; i++) {
+		if (rate >= rate_table[i].rate)
+			return i;
+	}
+
+	return -EINVAL;
+}
+EXPORT_SYMBOL(rockchip_pll_clk_rate_to_scale);
+
+int rockchip_pll_clk_scale_to_rate(struct clk *clk, unsigned int scale)
+{
+	const struct rockchip_pll_rate_table *rate_table;
+	struct clk *parent = clk_get_parent(clk);
+	struct rockchip_clk_pll *pll;
+	unsigned int i;
+
+	if (IS_ERR_OR_NULL(parent))
+		return -EINVAL;
+
+	pll = to_rockchip_clk_pll(__clk_get_hw(parent));
+	if (!pll)
+		return -EINVAL;
+
+	rate_table = pll->rate_table;
+	for (i = 0; i < pll->rate_count; i++) {
+		if (i == scale)
+			return rate_table[i].rate;
+	}
+
+	return -EINVAL;
+}
+EXPORT_SYMBOL(rockchip_pll_clk_scale_to_rate);
+
+static struct rockchip_pll_rate_table *rk_pll_rate_table_get(void)
+{
+	return &auto_table;
+}
+
+static int rockchip_pll_clk_set_postdiv(unsigned long fout_hz,
+					u32 *postdiv1,
+					u32 *postdiv2,
+					u32 *foutvco)
+{
+	unsigned long freq;
+
+	if (fout_hz < MIN_FOUTVCO_FREQ) {
+		for (*postdiv1 = 1; *postdiv1 <= 7; (*postdiv1)++) {
+			for (*postdiv2 = 1; *postdiv2 <= 7; (*postdiv2)++) {
+				freq = fout_hz * (*postdiv1) * (*postdiv2);
+				if (freq >= MIN_FOUTVCO_FREQ &&
+				    freq <= MAX_FOUTVCO_FREQ) {
+					*foutvco = freq;
+					return 0;
+				}
+			}
+		}
+		pr_err("CANNOT FIND postdiv1/2 to make fout in range from 800M to 2000M,fout = %lu\n",
+		       fout_hz);
+	} else {
+		*postdiv1 = 1;
+		*postdiv2 = 1;
+	}
+	return 0;
+}
+
+static struct rockchip_pll_rate_table *
+rockchip_pll_clk_set_by_auto(struct rockchip_clk_pll *pll,
+			     unsigned long fin_hz,
+			     unsigned long fout_hz)
+{
+	struct rockchip_pll_rate_table *rate_table = rk_pll_rate_table_get();
+	/* FIXME set postdiv1/2 always 1*/
+	u32 foutvco = fout_hz;
+	u64 fin_64, frac_64;
+	u32 f_frac, postdiv1, postdiv2;
+	unsigned long clk_gcd = 0;
+
+	if (fin_hz == 0 || fout_hz == 0 || fout_hz == fin_hz)
+		return NULL;
+
+	rockchip_pll_clk_set_postdiv(fout_hz, &postdiv1, &postdiv2, &foutvco);
+	rate_table->postdiv1 = postdiv1;
+	rate_table->postdiv2 = postdiv2;
+	rate_table->dsmpd = 1;
+
+	if (fin_hz / MHZ * MHZ == fin_hz && fout_hz / MHZ * MHZ == fout_hz) {
+		fin_hz /= MHZ;
+		foutvco /= MHZ;
+		clk_gcd = gcd(fin_hz, foutvco);
+		rate_table->refdiv = fin_hz / clk_gcd;
+		rate_table->fbdiv = foutvco / clk_gcd;
+
+		rate_table->frac = 0;
+
+		pr_debug("fin = %lu, fout = %lu, clk_gcd = %lu, refdiv = %u, fbdiv = %u, postdiv1 = %u, postdiv2 = %u, frac = %u\n",
+			 fin_hz, fout_hz, clk_gcd, rate_table->refdiv,
+			 rate_table->fbdiv, rate_table->postdiv1,
+			 rate_table->postdiv2, rate_table->frac);
+	} else {
+		pr_debug("frac div running, fin_hz = %lu, fout_hz = %lu, fin_INT_mhz = %lu, fout_INT_mhz = %lu\n",
+			 fin_hz, fout_hz,
+			 fin_hz / MHZ * MHZ,
+			 fout_hz / MHZ * MHZ);
+		pr_debug("frac get postdiv1 = %u,  postdiv2 = %u, foutvco = %u\n",
+			 rate_table->postdiv1, rate_table->postdiv2, foutvco);
+		clk_gcd = gcd(fin_hz / MHZ, foutvco / MHZ);
+		rate_table->refdiv = fin_hz / MHZ / clk_gcd;
+		rate_table->fbdiv = foutvco / MHZ / clk_gcd;
+		pr_debug("frac get refdiv = %u,  fbdiv = %u\n",
+			 rate_table->refdiv, rate_table->fbdiv);
+
+		rate_table->frac = 0;
+
+		f_frac = (foutvco % MHZ);
+		fin_64 = fin_hz;
+		do_div(fin_64, (u64)rate_table->refdiv);
+		frac_64 = (u64)f_frac << 24;
+		do_div(frac_64, fin_64);
+		rate_table->frac = (u32)frac_64;
+		if (rate_table->frac > 0)
+			rate_table->dsmpd = 0;
+		pr_debug("frac = %x\n", rate_table->frac);
+	}
+	return rate_table;
+}
+
+static struct rockchip_pll_rate_table *
+rockchip_rk3066_pll_clk_set_by_auto(struct rockchip_clk_pll *pll,
+				    unsigned long fin_hz,
+				    unsigned long fout_hz)
+{
+	struct rockchip_pll_rate_table *rate_table = rk_pll_rate_table_get();
+	u32 nr, nf, no, nonr;
+	u32 nr_out, nf_out, no_out;
+	u32 n;
+	u32 numerator, denominator;
+	u64 fref, fvco, fout;
+	unsigned long clk_gcd = 0;
+
+	nr_out = PLL_NR_MAX + 1;
+	no_out = 0;
+	nf_out = 0;
+
+	if (fin_hz == 0 || fout_hz == 0 || fout_hz == fin_hz)
+		return NULL;
+
+	clk_gcd = gcd(fin_hz, fout_hz);
+
+	numerator = fout_hz / clk_gcd;
+	denominator = fin_hz / clk_gcd;
+
+	for (n = 1;; n++) {
+		nf = numerator * n;
+		nonr = denominator * n;
+		if (nf > PLL_NF_MAX || nonr > (PLL_NO_MAX * PLL_NR_MAX))
+			break;
+
+		for (no = 1; no <= PLL_NO_MAX; no++) {
+			if (!(no == 1 || !(no % 2)))
+				continue;
+
+			if (nonr % no)
+				continue;
+			nr = nonr / no;
+
+			if (nr > PLL_NR_MAX)
+				continue;
+
+			fref = fin_hz / nr;
+			if (fref < PLL_FREF_MIN || fref > PLL_FREF_MAX)
+				continue;
+
+			fvco = fref * nf;
+			if (fvco < PLL_FVCO_MIN || fvco > PLL_FVCO_MAX)
+				continue;
+
+			fout = fvco / no;
+			if (fout < PLL_FOUT_MIN || fout > PLL_FOUT_MAX)
+				continue;
+
+			/* select the best from all available PLL settings */
+			if ((no > no_out) ||
+			    ((no == no_out) && (nr < nr_out))) {
+				nr_out = nr;
+				nf_out = nf;
+				no_out = no;
+			}
+		}
+	}
+
+	/* output the best PLL setting */
+	if ((nr_out <= PLL_NR_MAX) && (no_out > 0)) {
+		rate_table->nr = nr_out;
+		rate_table->nf = nf_out;
+		rate_table->no = no_out;
+	} else {
+		return NULL;
+	}
+
+	return rate_table;
+}
+
+static u32
+rockchip_rk3588_pll_frac_get(u32 m, u32 p, u32 s, u64 fin_hz, u64 fvco)
+{
+	u64 fref, fout, ffrac;
+	u32 k = 0;
+
+	fref = fin_hz / p;
+	ffrac = fvco - (m * fref);
+	fout = ffrac * 65536;
+	k = fout / fref;
+	if (k > 32767) {
+		fref = fin_hz / p;
+		ffrac = ((m + 1) * fref) - fvco;
+		fout = ffrac * 65536;
+		k = ((fout * 10 / fref) + 7) / 10;
+		if (k > 32767)
+			k = 0;
+		else
+			k = ~k + 1;
+	}
+	return k;
+}
+
+static struct rockchip_pll_rate_table *
+rockchip_rk3588_pll_frac_by_auto(unsigned long fin_hz,  unsigned long fout_hz)
+{
+	struct rockchip_pll_rate_table *rate_table = rk_pll_rate_table_get();
+	u64 fvco_min = 2250 * MHZ, fvco_max = 4500 * MHZ;
+	u32 p, m, s, k;
+	u64 fvco;
+
+	for (s = 0; s <= 6; s++) {
+		fvco = (u64)fout_hz << s;
+		if (fvco < fvco_min || fvco > fvco_max)
+			continue;
+		for (p = 1; p <= 4; p++) {
+			for (m = 64; m <= 1023; m++) {
+				if ((fvco >= m * fin_hz / p) && (fvco < (m + 1) * fin_hz / p)) {
+					k = rockchip_rk3588_pll_frac_get(m, p, s,
+									 (u64)fin_hz,
+									 fvco);
+					if (!k)
+						continue;
+					rate_table->p = p;
+					rate_table->s = s;
+					rate_table->k = k;
+					if (k > 32767)
+						rate_table->m = m + 1;
+					else
+						rate_table->m = m;
+					return rate_table;
+				}
+			}
+		}
+	}
+	return NULL;
+}
+
+static struct rockchip_pll_rate_table *
+rockchip_rk3588_pll_clk_set_by_auto(struct rockchip_clk_pll *pll,
+				    unsigned long fin_hz,
+				    unsigned long fout_hz)
+{
+	struct rockchip_pll_rate_table *rate_table = rk_pll_rate_table_get();
+	u64 fvco_min = 2250 * MHZ, fvco_max = 4500 * MHZ;
+	u64 fout_min = 37 * MHZ, fout_max = 4500 * MHZ;
+	u32 p, m, s;
+	u64 fvco;
+
+	if (fin_hz == 0 || fout_hz == 0 || fout_hz == fin_hz)
+		return NULL;
+
+	if (fout_hz > fout_max || fout_hz < fout_min)
+		return NULL;
+
+	if (fin_hz / MHZ * MHZ == fin_hz && fout_hz / MHZ * MHZ == fout_hz) {
+		for (s = 0; s <= 6; s++) {
+			fvco = (u64)fout_hz << s;
+			if (fvco < fvco_min || fvco > fvco_max)
+				continue;
+			for (p = 2; p <= 4; p++) {
+				for (m = 64; m <= 1023; m++) {
+					if (fvco == m * fin_hz / p) {
+						rate_table->p = p;
+						rate_table->m = m;
+						rate_table->s = s;
+						rate_table->k = 0;
+						return rate_table;
+					}
+				}
+			}
+		}
+		pr_err("CANNOT FIND Fout by auto,fout = %lu\n", fout_hz);
+	} else {
+		rate_table = rockchip_rk3588_pll_frac_by_auto(fin_hz, fout_hz);
+		if (!rate_table)
+			pr_err("CANNOT FIND Fout by auto,fout = %lu\n", fout_hz);
+		else
+			return rate_table;
+	}
+	return NULL;
+}
+
 static const struct rockchip_pll_rate_table *rockchip_get_pll_settings(
 			    struct rockchip_clk_pll *pll, unsigned long rate)
 {
@@ -54,28 +442,29 @@ static const struct rockchip_pll_rate_table *rockchip_get_pll_settings(
 	int i;
 
 	for (i = 0; i < pll->rate_count; i++) {
-		if (rate == rate_table[i].rate)
+		if (rate == rate_table[i].rate) {
+			if (i < pll->sel) {
+				pll->scaling = rate;
+				return &rate_table[pll->sel];
+			}
+			pll->scaling = 0;
 			return &rate_table[i];
+		}
 	}
+	pll->scaling = 0;
 
-	return NULL;
+	if (pll->type == pll_rk3066)
+		return rockchip_rk3066_pll_clk_set_by_auto(pll, 24 * MHZ, rate);
+	else if (pll->type == pll_rk3588 || pll->type == pll_rk3588_core)
+		return rockchip_rk3588_pll_clk_set_by_auto(pll, 24 * MHZ, rate);
+	else
+		return rockchip_pll_clk_set_by_auto(pll, 24 * MHZ, rate);
 }
 
 static long rockchip_pll_round_rate(struct clk_hw *hw,
 			    unsigned long drate, unsigned long *prate)
 {
-	struct rockchip_clk_pll *pll = to_rockchip_clk_pll(hw);
-	const struct rockchip_pll_rate_table *rate_table = pll->rate_table;
-	int i;
-
-	/* Assumming rate_table is in descending order */
-	for (i = 0; i < pll->rate_count; i++) {
-		if (drate >= rate_table[i].rate)
-			return rate_table[i].rate;
-	}
-
-	/* return minimum supported value */
-	return rate_table[i - 1].rate;
+	return drate;
 }
 
 /*
@@ -114,6 +503,7 @@ static int rockchip_pll_wait_lock(struct rockchip_clk_pll *pll)
 #define RK3036_PLLCON1_DSMPD_MASK		0x1
 #define RK3036_PLLCON1_DSMPD_SHIFT		12
 #define RK3036_PLLCON1_PWRDOWN			BIT(13)
+#define RK3036_PLLCON1_PLLPDSEL			BIT(15)
 #define RK3036_PLLCON2_FRAC_MASK		0xffffff
 #define RK3036_PLLCON2_FRAC_SHIFT		0
 
@@ -136,6 +526,30 @@ static int rockchip_rk3036_pll_wait_lock(struct rockchip_clk_pll *pll)
 	return ret;
 }
 
+static unsigned long __maybe_unused
+rockchip_rk3036_pll_con_to_rate(struct rockchip_clk_pll *pll,
+				u32 con0, u32 con1)
+{
+	unsigned int fbdiv, postdiv1, refdiv, postdiv2;
+	u64 rate64 = 24000000;
+
+	fbdiv = ((con0 >> RK3036_PLLCON0_FBDIV_SHIFT) &
+		  RK3036_PLLCON0_FBDIV_MASK);
+	postdiv1 = ((con0 >> RK3036_PLLCON0_POSTDIV1_SHIFT) &
+		     RK3036_PLLCON0_POSTDIV1_MASK);
+	refdiv = ((con1 >> RK3036_PLLCON1_REFDIV_SHIFT) &
+		   RK3036_PLLCON1_REFDIV_MASK);
+	postdiv2 = ((con1 >> RK3036_PLLCON1_POSTDIV2_SHIFT) &
+		     RK3036_PLLCON1_POSTDIV2_MASK);
+
+	rate64 *= fbdiv;
+	do_div(rate64, refdiv);
+	do_div(rate64, postdiv1);
+	do_div(rate64, postdiv2);
+
+	return (unsigned long)rate64;
+}
+
 static void rockchip_rk3036_pll_get_params(struct rockchip_clk_pll *pll,
 					struct rockchip_pll_rate_table *rate)
 {
@@ -165,7 +579,10 @@ static unsigned long rockchip_rk3036_pll_recalc_rate(struct clk_hw *hw,
 {
 	struct rockchip_clk_pll *pll = to_rockchip_clk_pll(hw);
 	struct rockchip_pll_rate_table cur;
-	u64 rate64 = prate;
+	u64 rate64 = prate, frac_rate64 = prate;
+
+	if (pll->sel && pll->scaling)
+		return pll->scaling;
 
 	rockchip_rk3036_pll_get_params(pll, &cur);
 
@@ -174,7 +591,7 @@ static unsigned long rockchip_rk3036_pll_recalc_rate(struct clk_hw *hw,
 
 	if (cur.dsmpd == 0) {
 		/* fractional mode */
-		u64 frac_rate64 = prate * cur.frac;
+		frac_rate64 *= cur.frac;
 
 		do_div(frac_rate64, cur.refdiv);
 		rate64 += frac_rate64 >> 24;
@@ -204,10 +621,12 @@ static int rockchip_rk3036_pll_set_params(struct rockchip_clk_pll *pll,
 	rockchip_rk3036_pll_get_params(pll, &cur);
 	cur.rate = 0;
 
-	cur_parent = pll_mux_ops->get_parent(&pll_mux->hw);
-	if (cur_parent == PLL_MODE_NORM) {
-		pll_mux_ops->set_parent(&pll_mux->hw, PLL_MODE_SLOW);
-		rate_change_remuxed = 1;
+	if (!(pll->flags & ROCKCHIP_PLL_FIXED_MODE)) {
+		cur_parent = pll_mux_ops->get_parent(&pll_mux->hw);
+		if (cur_parent == PLL_MODE_NORM) {
+			pll_mux_ops->set_parent(&pll_mux->hw, PLL_MODE_SLOW);
+			rate_change_remuxed = 1;
+		}
 	}
 
 	/* update pll values */
@@ -231,6 +650,9 @@ static int rockchip_rk3036_pll_set_params(struct rockchip_clk_pll *pll,
 	pllcon |= rate->frac << RK3036_PLLCON2_FRAC_SHIFT;
 	writel_relaxed(pllcon, pll->reg_base + RK3036_PLLCON(2));
 
+	if (IS_ENABLED(CONFIG_ROCKCHIP_CLK_BOOST))
+		rockchip_boost_disable_low(pll);
+
 	/* wait for the pll to lock */
 	ret = rockchip_rk3036_pll_wait_lock(pll);
 	if (ret) {
@@ -268,17 +690,25 @@ static int rockchip_rk3036_pll_set_rate(struct clk_hw *hw, unsigned long drate,
 static int rockchip_rk3036_pll_enable(struct clk_hw *hw)
 {
 	struct rockchip_clk_pll *pll = to_rockchip_clk_pll(hw);
+	const struct clk_ops *pll_mux_ops = pll->pll_mux_ops;
+	struct clk_mux *pll_mux = &pll->pll_mux;
 
 	writel(HIWORD_UPDATE(0, RK3036_PLLCON1_PWRDOWN, 0),
 	       pll->reg_base + RK3036_PLLCON(1));
 	rockchip_rk3036_pll_wait_lock(pll);
 
+	pll_mux_ops->set_parent(&pll_mux->hw, PLL_MODE_NORM);
+
 	return 0;
 }
 
 static void rockchip_rk3036_pll_disable(struct clk_hw *hw)
 {
 	struct rockchip_clk_pll *pll = to_rockchip_clk_pll(hw);
+	const struct clk_ops *pll_mux_ops = pll->pll_mux_ops;
+	struct clk_mux *pll_mux = &pll->pll_mux;
+
+	pll_mux_ops->set_parent(&pll_mux->hw, PLL_MODE_SLOW);
 
 	writel(HIWORD_UPDATE(RK3036_PLLCON1_PWRDOWN,
 			     RK3036_PLLCON1_PWRDOWN, 0),
@@ -412,6 +842,9 @@ static unsigned long rockchip_rk3066_pll_recalc_rate(struct clk_hw *hw,
 		return prate;
 	}
 
+	if (pll->sel && pll->scaling)
+		return pll->scaling;
+
 	rockchip_rk3066_pll_get_params(pll, &cur);
 
 	rate64 *= cur.nf;
@@ -485,9 +918,18 @@ static int rockchip_rk3066_pll_set_rate(struct clk_hw *hw, unsigned long drate,
 {
 	struct rockchip_clk_pll *pll = to_rockchip_clk_pll(hw);
 	const struct rockchip_pll_rate_table *rate;
+	unsigned long old_rate = rockchip_rk3066_pll_recalc_rate(hw, prate);
+	struct regmap *grf = pll->ctx->grf;
+	int ret;
 
-	pr_debug("%s: changing %s to %lu with a parent rate of %lu\n",
-		 __func__, clk_hw_get_name(hw), drate, prate);
+	if (IS_ERR(grf)) {
+		pr_debug("%s: grf regmap not available, aborting rate change\n",
+			 __func__);
+		return PTR_ERR(grf);
+	}
+
+	pr_debug("%s: changing %s from %lu to %lu with a parent rate of %lu\n",
+		 __func__, clk_hw_get_name(hw), old_rate, drate, prate);
 
 	/* Get required rate settings from table */
 	rate = rockchip_get_pll_settings(pll, drate);
@@ -497,7 +939,11 @@ static int rockchip_rk3066_pll_set_rate(struct clk_hw *hw, unsigned long drate,
 		return -EINVAL;
 	}
 
-	return rockchip_rk3066_pll_set_params(pll, rate);
+	ret = rockchip_rk3066_pll_set_params(pll, rate);
+	if (ret)
+		pll->scaling = 0;
+
+	return ret;
 }
 
 static int rockchip_rk3066_pll_enable(struct clk_hw *hw)
@@ -649,6 +1095,9 @@ static unsigned long rockchip_rk3399_pll_recalc_rate(struct clk_hw *hw,
 	struct rockchip_pll_rate_table cur;
 	u64 rate64 = prate;
 
+	if (pll->sel && pll->scaling)
+		return pll->scaling;
+
 	rockchip_rk3399_pll_get_params(pll, &cur);
 
 	rate64 *= cur.fbdiv;
@@ -692,6 +1141,11 @@ static int rockchip_rk3399_pll_set_params(struct rockchip_clk_pll *pll,
 		rate_change_remuxed = 1;
 	}
 
+	/* set pll power down */
+	writel(HIWORD_UPDATE(RK3399_PLLCON3_PWRDOWN,
+			     RK3399_PLLCON3_PWRDOWN, 0),
+	       pll->reg_base + RK3399_PLLCON(3));
+
 	/* update pll values */
 	writel_relaxed(HIWORD_UPDATE(rate->fbdiv, RK3399_PLLCON0_FBDIV_MASK,
 						  RK3399_PLLCON0_FBDIV_SHIFT),
@@ -715,6 +1169,11 @@ static int rockchip_rk3399_pll_set_params(struct rockchip_clk_pll *pll,
 					    RK3399_PLLCON3_DSMPD_SHIFT),
 		       pll->reg_base + RK3399_PLLCON(3));
 
+	/* set pll power up */
+	writel(HIWORD_UPDATE(0,
+			     RK3399_PLLCON3_PWRDOWN, 0),
+	       pll->reg_base + RK3399_PLLCON(3));
+
 	/* wait for the pll to lock */
 	ret = rockchip_rk3399_pll_wait_lock(pll);
 	if (ret) {
@@ -734,9 +1193,11 @@ static int rockchip_rk3399_pll_set_rate(struct clk_hw *hw, unsigned long drate,
 {
 	struct rockchip_clk_pll *pll = to_rockchip_clk_pll(hw);
 	const struct rockchip_pll_rate_table *rate;
+	unsigned long old_rate = rockchip_rk3399_pll_recalc_rate(hw, prate);
+	int ret;
 
-	pr_debug("%s: changing %s to %lu with a parent rate of %lu\n",
-		 __func__, __clk_get_name(hw->clk), drate, prate);
+	pr_debug("%s: changing %s from %lu to %lu with a parent rate of %lu\n",
+		 __func__, __clk_get_name(hw->clk), old_rate, drate, prate);
 
 	/* Get required rate settings from table */
 	rate = rockchip_get_pll_settings(pll, drate);
@@ -746,7 +1207,11 @@ static int rockchip_rk3399_pll_set_rate(struct clk_hw *hw, unsigned long drate,
 		return -EINVAL;
 	}
 
-	return rockchip_rk3399_pll_set_params(pll, rate);
+	ret = rockchip_rk3399_pll_set_params(pll, rate);
+	if (ret)
+		pll->scaling = 0;
+
+	return ret;
 }
 
 static int rockchip_rk3399_pll_enable(struct clk_hw *hw)
@@ -842,21 +1307,21 @@ static const struct clk_ops rockchip_rk3399_pll_clk_ops = {
 	.init = rockchip_rk3399_pll_init,
 };
 
-/*
+/**
  * PLL used in RK3588
  */
 
-#define RK3588_PLLCON(i)               (i * 0x4)
-#define RK3588_PLLCON0_M_MASK          0x3ff
-#define RK3588_PLLCON0_M_SHIFT         0
-#define RK3588_PLLCON1_P_MASK          0x3f
-#define RK3588_PLLCON1_P_SHIFT         0
-#define RK3588_PLLCON1_S_MASK          0x7
-#define RK3588_PLLCON1_S_SHIFT         6
-#define RK3588_PLLCON2_K_MASK          0xffff
-#define RK3588_PLLCON2_K_SHIFT         0
-#define RK3588_PLLCON1_PWRDOWN         BIT(13)
-#define RK3588_PLLCON6_LOCK_STATUS     BIT(15)
+#define RK3588_PLLCON(i)		(i * 0x4)
+#define RK3588_PLLCON0_M_MASK		0x3ff
+#define RK3588_PLLCON0_M_SHIFT		0
+#define RK3588_PLLCON1_P_MASK		0x3f
+#define RK3588_PLLCON1_P_SHIFT		0
+#define RK3588_PLLCON1_S_MASK		0x7
+#define RK3588_PLLCON1_S_SHIFT		6
+#define RK3588_PLLCON2_K_MASK		0xffff
+#define RK3588_PLLCON2_K_SHIFT		0
+#define RK3588_PLLCON1_PWRDOWN		BIT(13)
+#define RK3588_PLLCON6_LOCK_STATUS	BIT(15)
 
 static int rockchip_rk3588_pll_wait_lock(struct rockchip_clk_pll *pll)
 {
@@ -877,38 +1342,68 @@ static int rockchip_rk3588_pll_wait_lock(struct rockchip_clk_pll *pll)
 	return ret;
 }
 
+static long rockchip_rk3588_pll_round_rate(struct clk_hw *hw,
+			    unsigned long drate, unsigned long *prate)
+{
+	if ((drate < 37 * MHZ) || (drate > 4500 * MHZ))
+		return -EINVAL;
+	else
+		return drate;
+}
+
 static void rockchip_rk3588_pll_get_params(struct rockchip_clk_pll *pll,
-					   struct rockchip_pll_rate_table *rate)
+					struct rockchip_pll_rate_table *rate)
 {
 	u32 pllcon;
 
 	pllcon = readl_relaxed(pll->reg_base + RK3588_PLLCON(0));
-	rate->m = ((pllcon >> RK3588_PLLCON0_M_SHIFT) & RK3588_PLLCON0_M_MASK);
+	rate->m = ((pllcon >> RK3588_PLLCON0_M_SHIFT)
+				& RK3588_PLLCON0_M_MASK);
 
 	pllcon = readl_relaxed(pll->reg_base + RK3588_PLLCON(1));
-	rate->p = ((pllcon >> RK3588_PLLCON1_P_SHIFT) & RK3588_PLLCON1_P_MASK);
-	rate->s = ((pllcon >> RK3588_PLLCON1_S_SHIFT) & RK3588_PLLCON1_S_MASK);
+	rate->p = ((pllcon >> RK3588_PLLCON1_P_SHIFT)
+				& RK3588_PLLCON1_P_MASK);
+	rate->s = ((pllcon >> RK3588_PLLCON1_S_SHIFT)
+				& RK3588_PLLCON1_S_MASK);
 
 	pllcon = readl_relaxed(pll->reg_base + RK3588_PLLCON(2));
-	rate->k = ((pllcon >> RK3588_PLLCON2_K_SHIFT) & RK3588_PLLCON2_K_MASK);
+	rate->k = ((pllcon >> RK3588_PLLCON2_K_SHIFT)
+				& RK3588_PLLCON2_K_MASK);
 }
 
-static unsigned long rockchip_rk3588_pll_recalc_rate(struct clk_hw *hw, unsigned long prate)
+static unsigned long rockchip_rk3588_pll_recalc_rate(struct clk_hw *hw,
+						     unsigned long prate)
 {
 	struct rockchip_clk_pll *pll = to_rockchip_clk_pll(hw);
 	struct rockchip_pll_rate_table cur;
 	u64 rate64 = prate, postdiv;
 
+	if (pll->sel && pll->scaling)
+		return pll->scaling;
+
 	rockchip_rk3588_pll_get_params(pll, &cur);
+	if (cur.p == 0)
+		return prate;
 
 	rate64 *= cur.m;
 	do_div(rate64, cur.p);
 
-	if (cur.k) {
+	if (cur.k & BIT(15)) {
+		/* fractional mode */
+		u64 frac_rate64;
+
+		cur.k = (~(cur.k - 1)) & RK3588_PLLCON2_K_MASK;
+		frac_rate64 = prate * cur.k;
+		postdiv = cur.p;
+		postdiv *= 65536;
+		do_div(frac_rate64, postdiv);
+		rate64 -= frac_rate64;
+	} else {
 		/* fractional mode */
 		u64 frac_rate64 = prate * cur.k;
 
-		postdiv = cur.p * 65535;
+		postdiv = cur.p;
+		postdiv *= 65536;
 		do_div(frac_rate64, postdiv);
 		rate64 += frac_rate64;
 	}
@@ -918,7 +1413,7 @@ static unsigned long rockchip_rk3588_pll_recalc_rate(struct clk_hw *hw, unsigned
 }
 
 static int rockchip_rk3588_pll_set_params(struct rockchip_clk_pll *pll,
-					  const struct rockchip_pll_rate_table *rate)
+				const struct rockchip_pll_rate_table *rate)
 {
 	const struct clk_ops *pll_mux_ops = pll->pll_mux_ops;
 	struct clk_mux *pll_mux = &pll->pll_mux;
@@ -928,7 +1423,7 @@ static int rockchip_rk3588_pll_set_params(struct rockchip_clk_pll *pll,
 	int ret;
 
 	pr_debug("%s: rate settings for %lu p: %d, m: %d, s: %d, k: %d\n",
-		 __func__, rate->rate, rate->p, rate->m, rate->s, rate->k);
+		__func__, rate->rate, rate->p, rate->m, rate->s, rate->k);
 
 	rockchip_rk3588_pll_get_params(pll, &cur);
 	cur.rate = 0;
@@ -944,21 +1439,26 @@ static int rockchip_rk3588_pll_set_params(struct rockchip_clk_pll *pll,
 	/* set pll power down */
 	writel(HIWORD_UPDATE(RK3588_PLLCON1_PWRDOWN,
 			     RK3588_PLLCON1_PWRDOWN, 0),
-	       pll->reg_base + RK3399_PLLCON(1));
+	       pll->reg_base + RK3588_PLLCON(1));
 
 	/* update pll values */
-	writel_relaxed(HIWORD_UPDATE(rate->m, RK3588_PLLCON0_M_MASK, RK3588_PLLCON0_M_SHIFT),
-		       pll->reg_base + RK3399_PLLCON(0));
+	writel_relaxed(HIWORD_UPDATE(rate->m, RK3588_PLLCON0_M_MASK,
+						  RK3588_PLLCON0_M_SHIFT),
+		       pll->reg_base + RK3588_PLLCON(0));
 
-	writel_relaxed(HIWORD_UPDATE(rate->p, RK3588_PLLCON1_P_MASK, RK3588_PLLCON1_P_SHIFT) |
-		       HIWORD_UPDATE(rate->s, RK3588_PLLCON1_S_MASK, RK3588_PLLCON1_S_SHIFT),
-		       pll->reg_base + RK3399_PLLCON(1));
+	writel_relaxed(HIWORD_UPDATE(rate->p, RK3588_PLLCON1_P_MASK,
+						   RK3588_PLLCON1_P_SHIFT) |
+		       HIWORD_UPDATE(rate->s, RK3588_PLLCON1_S_MASK,
+						     RK3588_PLLCON1_S_SHIFT),
+		       pll->reg_base + RK3588_PLLCON(1));
 
-	writel_relaxed(HIWORD_UPDATE(rate->k, RK3588_PLLCON2_K_MASK, RK3588_PLLCON2_K_SHIFT),
-		       pll->reg_base + RK3399_PLLCON(2));
+	writel_relaxed(HIWORD_UPDATE(rate->k, RK3588_PLLCON2_K_MASK,
+				     RK3588_PLLCON2_K_SHIFT),
+		       pll->reg_base + RK3588_PLLCON(2));
 
 	/* set pll power up */
-	writel(HIWORD_UPDATE(0, RK3588_PLLCON1_PWRDOWN, 0),
+	writel(HIWORD_UPDATE(0,
+			     RK3588_PLLCON1_PWRDOWN, 0),
 	       pll->reg_base + RK3588_PLLCON(1));
 
 	/* wait for the pll to lock */
@@ -980,9 +1480,11 @@ static int rockchip_rk3588_pll_set_rate(struct clk_hw *hw, unsigned long drate,
 {
 	struct rockchip_clk_pll *pll = to_rockchip_clk_pll(hw);
 	const struct rockchip_pll_rate_table *rate;
+	unsigned long old_rate = rockchip_rk3588_pll_recalc_rate(hw, prate);
+	int ret;
 
-	pr_debug("%s: changing %s to %lu with a parent rate of %lu\n",
-		 __func__, __clk_get_name(hw->clk), drate, prate);
+	pr_debug("%s: changing %s from %lu to %lu with a parent rate of %lu\n",
+		 __func__, __clk_get_name(hw->clk), old_rate, drate, prate);
 
 	/* Get required rate settings from table */
 	rate = rockchip_get_pll_settings(pll, drate);
@@ -992,25 +1494,38 @@ static int rockchip_rk3588_pll_set_rate(struct clk_hw *hw, unsigned long drate,
 		return -EINVAL;
 	}
 
-	return rockchip_rk3588_pll_set_params(pll, rate);
+	ret = rockchip_rk3588_pll_set_params(pll, rate);
+	if (ret)
+		pll->scaling = 0;
+
+	return ret;
 }
 
 static int rockchip_rk3588_pll_enable(struct clk_hw *hw)
 {
 	struct rockchip_clk_pll *pll = to_rockchip_clk_pll(hw);
+	const struct clk_ops *pll_mux_ops = pll->pll_mux_ops;
+	struct clk_mux *pll_mux = &pll->pll_mux;
 
 	writel(HIWORD_UPDATE(0, RK3588_PLLCON1_PWRDOWN, 0),
 	       pll->reg_base + RK3588_PLLCON(1));
 	rockchip_rk3588_pll_wait_lock(pll);
 
+	pll_mux_ops->set_parent(&pll_mux->hw, PLL_MODE_NORM);
+
 	return 0;
 }
 
 static void rockchip_rk3588_pll_disable(struct clk_hw *hw)
 {
 	struct rockchip_clk_pll *pll = to_rockchip_clk_pll(hw);
+	const struct clk_ops *pll_mux_ops = pll->pll_mux_ops;
+	struct clk_mux *pll_mux = &pll->pll_mux;
+
+	pll_mux_ops->set_parent(&pll_mux->hw, PLL_MODE_SLOW);
 
-	writel(HIWORD_UPDATE(RK3588_PLLCON1_PWRDOWN, RK3588_PLLCON1_PWRDOWN, 0),
+	writel(HIWORD_UPDATE(RK3588_PLLCON1_PWRDOWN,
+			     RK3588_PLLCON1_PWRDOWN, 0),
 	       pll->reg_base + RK3588_PLLCON(1));
 }
 
@@ -1041,7 +1556,7 @@ static const struct clk_ops rockchip_rk3588_pll_clk_norate_ops = {
 
 static const struct clk_ops rockchip_rk3588_pll_clk_ops = {
 	.recalc_rate = rockchip_rk3588_pll_recalc_rate,
-	.round_rate = rockchip_pll_round_rate,
+	.round_rate = rockchip_rk3588_pll_round_rate,
 	.set_rate = rockchip_rk3588_pll_set_rate,
 	.enable = rockchip_rk3588_pll_enable,
 	.disable = rockchip_rk3588_pll_disable,
@@ -1049,6 +1564,163 @@ static const struct clk_ops rockchip_rk3588_pll_clk_ops = {
 	.init = rockchip_rk3588_pll_init,
 };
 
+#ifdef CONFIG_ROCKCHIP_CLK_COMPENSATION
+int rockchip_pll_clk_compensation(struct clk *clk, int ppm)
+{
+	struct clk *parent = clk_get_parent(clk);
+	struct rockchip_clk_pll *pll;
+	static u32 frac, fbdiv, s, p;
+	bool negative;
+	u32 pllcon, pllcon0, pllcon2, fbdiv_mask, frac_mask, frac_shift;
+	u64 fracdiv, m, n;
+
+	if ((ppm > 1000) || (ppm < -1000))
+		return -EINVAL;
+
+	if (IS_ERR_OR_NULL(parent))
+		return -EINVAL;
+
+	pll = to_rockchip_clk_pll(__clk_get_hw(parent));
+	if (!pll)
+		return -EINVAL;
+
+	switch (pll->type) {
+	case pll_rk3036:
+	case pll_rk3328:
+		pllcon0 = RK3036_PLLCON(0);
+		pllcon2 = RK3036_PLLCON(2);
+		fbdiv_mask = RK3036_PLLCON0_FBDIV_MASK;
+		frac_mask = RK3036_PLLCON2_FRAC_MASK;
+		frac_shift = RK3036_PLLCON2_FRAC_SHIFT;
+		if (!frac)
+			writel(HIWORD_UPDATE(RK3036_PLLCON1_PLLPDSEL,
+					     RK3036_PLLCON1_PLLPDSEL, 0),
+			       pll->reg_base + RK3036_PLLCON(1));
+		break;
+	case pll_rk3066:
+		return -EINVAL;
+	case pll_rk3399:
+		pllcon0 = RK3399_PLLCON(0);
+		pllcon2 = RK3399_PLLCON(2);
+		fbdiv_mask = RK3399_PLLCON0_FBDIV_MASK;
+		frac_mask = RK3399_PLLCON2_FRAC_MASK;
+		frac_shift = RK3399_PLLCON2_FRAC_SHIFT;
+		break;
+	case pll_rk3588:
+		pllcon0 = RK3588_PLLCON(0);
+		pllcon2 = RK3588_PLLCON(2);
+		fbdiv_mask = RK3588_PLLCON0_M_MASK;
+		frac_mask = RK3588_PLLCON2_K_MASK;
+		frac_shift = RK3588_PLLCON2_K_SHIFT;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	negative = !!(ppm & BIT(31));
+	ppm = negative ? ~ppm + 1 : ppm;
+
+	switch (pll->type) {
+	case pll_rk3036:
+	case pll_rk3328:
+	case pll_rk3066:
+	case pll_rk3399:
+		/*
+		 *   delta frac                 frac          ppm
+		 * -------------- = (fbdiv + ----------) * ---------
+		 *    1 << 24                 1 << 24       1000000
+		 *
+		 */
+		if (!frac) {
+			frac = readl_relaxed(pll->reg_base + pllcon2) & frac_mask;
+			fbdiv = readl_relaxed(pll->reg_base + pllcon0) & fbdiv_mask;
+		}
+		m = div64_u64((uint64_t)frac * ppm, 1000000);
+		n = div64_u64((uint64_t)ppm << 24, 1000000) * fbdiv;
+
+		fracdiv = negative ? frac - (m + n) : frac + (m + n);
+
+		if (!frac || fracdiv > frac_mask)
+			return -EINVAL;
+
+		pllcon = readl_relaxed(pll->reg_base + pllcon2);
+		pllcon &= ~(frac_mask << frac_shift);
+		pllcon |= fracdiv << frac_shift;
+		writel_relaxed(pllcon, pll->reg_base + pllcon2);
+		break;
+	case pll_rk3588:
+		if (!fbdiv) {
+			frac = readl_relaxed(pll->reg_base + pllcon2) & frac_mask;
+			fbdiv = readl_relaxed(pll->reg_base + pllcon0) & fbdiv_mask;
+		}
+		if (!frac) {
+			pllcon = readl_relaxed(pll->reg_base + RK3588_PLLCON(1));
+			s = ((pllcon >> RK3588_PLLCON1_S_SHIFT)
+				& RK3588_PLLCON1_S_MASK);
+			p = ((pllcon >> RK3588_PLLCON1_P_SHIFT)
+				& RK3588_PLLCON1_P_MASK);
+			m = div64_u64((uint64_t)clk_get_rate(clk) * ppm, 24000000);
+			n = div64_u64((uint64_t)m * 65536 * p * (1 << s), 1000000);
+
+			if (n > 32767)
+				return -EINVAL;
+			fracdiv = negative ? ~n + 1 : n;
+		} else if (frac & BIT(15)) {
+			frac = (~(frac - 1)) & RK3588_PLLCON2_K_MASK;
+			m = div64_u64((uint64_t)frac * ppm, 100000);
+			n = div64_u64((uint64_t)ppm * 65536 * fbdiv, 100000);
+			if (negative) {
+				fracdiv = frac + (div64_u64(m + n, 10));
+				if (fracdiv > 32767)
+					return -EINVAL;
+				fracdiv = ~fracdiv + 1;
+			} else {
+				s = div64_u64(m + n, 10);
+				if (frac >= s) {
+					fracdiv = frac - s;
+					if (fracdiv > 32767)
+						return -EINVAL;
+					fracdiv = ~fracdiv + 1;
+				} else {
+					fracdiv = s - frac;
+					if (fracdiv > 32767)
+						return -EINVAL;
+				}
+			}
+		} else {
+			m = div64_u64((uint64_t)frac * ppm, 100000);
+			n = div64_u64((uint64_t)ppm * 65536 * fbdiv, 100000);
+			if (!negative) {
+				fracdiv = frac + (div64_u64(m + n, 10));
+				if (fracdiv > 32767)
+					return -EINVAL;
+			} else {
+				s = div64_u64(m + n, 10);
+				if (frac >= s) {
+					fracdiv = frac - s;
+					if (fracdiv > 32767)
+						return -EINVAL;
+				} else {
+					fracdiv = s - frac;
+					if (fracdiv > 32767)
+						return -EINVAL;
+					fracdiv = ~fracdiv + 1;
+				}
+			}
+		}
+
+		writel_relaxed(HIWORD_UPDATE(fracdiv, frac_mask, frac_shift),
+			       pll->reg_base + pllcon2);
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	return  0;
+}
+EXPORT_SYMBOL(rockchip_pll_clk_compensation);
+#endif
+
 /*
  * Common registering of pll clocks
  */
@@ -1093,13 +1765,7 @@ struct clk *rockchip_clk_register_pll(struct rockchip_clk_provider *ctx,
 	pll_mux->flags = 0;
 	pll_mux->lock = &ctx->lock;
 	pll_mux->hw.init = &init;
-
-	if (pll_type == pll_rk3036 ||
-	    pll_type == pll_rk3066 ||
-	    pll_type == pll_rk3328 ||
-	    pll_type == pll_rk3399 ||
-	    pll_type == pll_rk3588)
-		pll_mux->flags |= CLK_MUX_HIWORD_MASK;
+	pll_mux->flags |= CLK_MUX_HIWORD_MASK;
 
 	/* the actual muxing is xin24m, pll-output, xin32k */
 	pll_parents[0] = parent_names[0];
@@ -1122,8 +1788,15 @@ struct clk *rockchip_clk_register_pll(struct rockchip_clk_provider *ctx,
 	/* now create the actual pll */
 	init.name = pll_name;
 
-	/* keep all plls untouched for now */
-	init.flags = flags | CLK_IGNORE_UNUSED;
+#ifndef CONFIG_ROCKCHIP_LOW_PERFORMANCE
+	if (clk_pll_flags & ROCKCHIP_PLL_ALLOW_POWER_DOWN)
+		init.flags = flags;
+	else
+		/* keep all plls untouched for now */
+		init.flags = flags | CLK_IGNORE_UNUSED;
+#else
+	init.flags = flags;
+#endif
 
 	init.parent_names = &parent_names[0];
 	init.num_parents = 1;
@@ -1153,18 +1826,23 @@ struct clk *rockchip_clk_register_pll(struct rockchip_clk_provider *ctx,
 		else
 			init.ops = &rockchip_rk3036_pll_clk_ops;
 		break;
+#ifdef CONFIG_ROCKCHIP_PLL_RK3066
 	case pll_rk3066:
 		if (!pll->rate_table || IS_ERR(ctx->grf))
 			init.ops = &rockchip_rk3066_pll_clk_norate_ops;
 		else
 			init.ops = &rockchip_rk3066_pll_clk_ops;
 		break;
+#endif
+#ifdef CONFIG_ROCKCHIP_PLL_RK3399
 	case pll_rk3399:
 		if (!pll->rate_table)
 			init.ops = &rockchip_rk3399_pll_clk_norate_ops;
 		else
 			init.ops = &rockchip_rk3399_pll_clk_ops;
 		break;
+#endif
+#ifdef CONFIG_ROCKCHIP_PLL_RK3588
 	case pll_rk3588:
 	case pll_rk3588_core:
 		if (!pll->rate_table)
@@ -1173,6 +1851,7 @@ struct clk *rockchip_clk_register_pll(struct rockchip_clk_provider *ctx,
 			init.ops = &rockchip_rk3588_pll_clk_ops;
 		init.flags = flags;
 		break;
+#endif
 	default:
 		pr_warn("%s: Unknown pll type for pll clk %s\n",
 			__func__, name);
@@ -1204,3 +1883,318 @@ struct clk *rockchip_clk_register_pll(struct rockchip_clk_provider *ctx,
 	kfree(pll);
 	return mux_clk;
 }
+
+#ifdef CONFIG_ROCKCHIP_CLK_BOOST
+static unsigned long rockchip_pll_con_to_rate(struct rockchip_clk_pll *pll,
+					      u32 con0, u32 con1)
+{
+	switch (pll->type) {
+	case pll_rk3036:
+	case pll_rk3328:
+		return rockchip_rk3036_pll_con_to_rate(pll, con0, con1);
+	case pll_rk3066:
+		break;
+	case pll_rk3399:
+		break;
+	default:
+		pr_warn("%s: Unknown pll type\n", __func__);
+	}
+
+	return 0;
+}
+
+void rockchip_boost_init(struct clk_hw *hw)
+{
+	struct rockchip_clk_pll *pll;
+	struct device_node *np;
+	u32 value, con0, con1;
+
+	if (!hw)
+		return;
+	pll = to_rockchip_clk_pll(hw);
+	np = of_parse_phandle(pll->ctx->cru_node, "rockchip,boost", 0);
+	if (!np) {
+		pr_debug("%s: failed to get boost np\n", __func__);
+		return;
+	}
+	pll->boost = syscon_node_to_regmap(np);
+	if (IS_ERR(pll->boost)) {
+		pr_debug("%s: failed to get boost regmap\n", __func__);
+		return;
+	}
+
+	if (!of_property_read_u32(np, "rockchip,boost-low-con0", &con0) &&
+	    !of_property_read_u32(np, "rockchip,boost-low-con1", &con1)) {
+		pr_debug("boost-low-con=0x%x 0x%x\n", con0, con1);
+		regmap_write(pll->boost, BOOST_PLL_L_CON(0),
+			     HIWORD_UPDATE(con0, BOOST_PLL_CON_MASK, 0));
+		regmap_write(pll->boost, BOOST_PLL_L_CON(1),
+			     HIWORD_UPDATE(con1, BOOST_PLL_CON_MASK, 0));
+		pll->boost_low_rate = rockchip_pll_con_to_rate(pll, con0,
+							       con1);
+		pr_debug("boost-low-rate=%lu\n", pll->boost_low_rate);
+	}
+	if (!of_property_read_u32(np, "rockchip,boost-high-con0", &con0) &&
+	    !of_property_read_u32(np, "rockchip,boost-high-con1", &con1)) {
+		pr_debug("boost-high-con=0x%x 0x%x\n", con0, con1);
+		regmap_write(pll->boost, BOOST_PLL_H_CON(0),
+			     HIWORD_UPDATE(con0, BOOST_PLL_CON_MASK, 0));
+		regmap_write(pll->boost, BOOST_PLL_H_CON(1),
+			     HIWORD_UPDATE(con1, BOOST_PLL_CON_MASK, 0));
+		pll->boost_high_rate = rockchip_pll_con_to_rate(pll, con0,
+								con1);
+		pr_debug("boost-high-rate=%lu\n", pll->boost_high_rate);
+	}
+	if (!of_property_read_u32(np, "rockchip,boost-backup-pll", &value)) {
+		pr_debug("boost-backup-pll=0x%x\n", value);
+		regmap_write(pll->boost, BOOST_CLK_CON,
+			     HIWORD_UPDATE(value, BOOST_BACKUP_PLL_MASK,
+					   BOOST_BACKUP_PLL_SHIFT));
+	}
+	if (!of_property_read_u32(np, "rockchip,boost-backup-pll-usage",
+				  &pll->boost_backup_pll_usage)) {
+		pr_debug("boost-backup-pll-usage=0x%x\n",
+			 pll->boost_backup_pll_usage);
+		regmap_write(pll->boost, BOOST_CLK_CON,
+			     HIWORD_UPDATE(pll->boost_backup_pll_usage,
+					   BOOST_BACKUP_PLL_USAGE_MASK,
+					   BOOST_BACKUP_PLL_USAGE_SHIFT));
+	}
+	if (!of_property_read_u32(np, "rockchip,boost-switch-threshold",
+				  &value)) {
+		pr_debug("boost-switch-threshold=0x%x\n", value);
+		regmap_write(pll->boost, BOOST_SWITCH_THRESHOLD, value);
+	}
+	if (!of_property_read_u32(np, "rockchip,boost-statis-threshold",
+				  &value)) {
+		pr_debug("boost-statis-threshold=0x%x\n", value);
+		regmap_write(pll->boost, BOOST_STATIS_THRESHOLD, value);
+	}
+	if (!of_property_read_u32(np, "rockchip,boost-statis-enable",
+				  &value)) {
+		pr_debug("boost-statis-enable=0x%x\n", value);
+		regmap_write(pll->boost, BOOST_BOOST_CON,
+			     HIWORD_UPDATE(value, BOOST_STATIS_ENABLE_MASK,
+					   BOOST_STATIS_ENABLE_SHIFT));
+	}
+	if (!of_property_read_u32(np, "rockchip,boost-enable", &value)) {
+		pr_debug("boost-enable=0x%x\n", value);
+		regmap_write(pll->boost, BOOST_BOOST_CON,
+			     HIWORD_UPDATE(value, BOOST_ENABLE_MASK,
+					   BOOST_ENABLE_SHIFT));
+		if (value)
+			pll->boost_enabled = true;
+	}
+#ifdef CONFIG_DEBUG_FS
+	if (pll->boost_enabled) {
+		mutex_lock(&clk_boost_lock);
+		hlist_add_head(&pll->debug_node, &clk_boost_list);
+		mutex_unlock(&clk_boost_lock);
+	}
+#endif
+}
+
+void rockchip_boost_enable_recovery_sw_low(struct clk_hw *hw)
+{
+	struct rockchip_clk_pll *pll;
+	unsigned int val;
+
+	if (!hw)
+		return;
+	pll = to_rockchip_clk_pll(hw);
+	if (!pll->boost_enabled)
+		return;
+
+	regmap_write(pll->boost, BOOST_BOOST_CON,
+		     HIWORD_UPDATE(1, BOOST_RECOVERY_MASK,
+				   BOOST_RECOVERY_SHIFT));
+	do {
+		regmap_read(pll->boost, BOOST_FSM_STATUS, &val);
+	} while (!(val & BOOST_BUSY_STATE));
+
+	regmap_write(pll->boost, BOOST_BOOST_CON,
+		     HIWORD_UPDATE(1, BOOST_SW_CTRL_MASK,
+				   BOOST_SW_CTRL_SHIFT) |
+		     HIWORD_UPDATE(1, BOOST_LOW_FREQ_EN_MASK,
+				   BOOST_LOW_FREQ_EN_SHIFT));
+}
+
+static void rockchip_boost_disable_low(struct rockchip_clk_pll *pll)
+{
+	if (!pll->boost_enabled)
+		return;
+
+	regmap_write(pll->boost, BOOST_BOOST_CON,
+		     HIWORD_UPDATE(0, BOOST_LOW_FREQ_EN_MASK,
+				   BOOST_LOW_FREQ_EN_SHIFT));
+}
+
+void rockchip_boost_disable_recovery_sw(struct clk_hw *hw)
+{
+	struct rockchip_clk_pll *pll;
+
+	if (!hw)
+		return;
+	pll = to_rockchip_clk_pll(hw);
+	if (!pll->boost_enabled)
+		return;
+
+	regmap_write(pll->boost, BOOST_BOOST_CON,
+		     HIWORD_UPDATE(0, BOOST_RECOVERY_MASK,
+				   BOOST_RECOVERY_SHIFT));
+	regmap_write(pll->boost, BOOST_BOOST_CON,
+		     HIWORD_UPDATE(0, BOOST_SW_CTRL_MASK,
+				   BOOST_SW_CTRL_SHIFT));
+}
+
+void rockchip_boost_add_core_div(struct clk_hw *hw, unsigned long prate)
+{
+	struct rockchip_clk_pll *pll;
+	unsigned int div;
+
+	if (!hw)
+		return;
+	pll = to_rockchip_clk_pll(hw);
+	if (!pll->boost_enabled || pll->boost_backup_pll_rate == prate)
+		return;
+
+	/* todo */
+	if (pll->boost_backup_pll_usage == BOOST_BACKUP_PLL_USAGE_TARGET)
+		return;
+	/*
+	 * cpu clock rate should be less than or equal to
+	 * low rate when change pll rate in boost module
+	 */
+	if (pll->boost_low_rate && prate > pll->boost_low_rate) {
+		div =  DIV_ROUND_UP(prate, pll->boost_low_rate) - 1;
+		regmap_write(pll->boost, BOOST_CLK_CON,
+			     HIWORD_UPDATE(div, BOOST_CORE_DIV_MASK,
+					   BOOST_CORE_DIV_SHIFT));
+		pll->boost_backup_pll_rate = prate;
+	}
+}
+
+#ifdef CONFIG_DEBUG_FS
+#include <linux/debugfs.h>
+
+#ifndef MODULE
+static int boost_summary_show(struct seq_file *s, void *data)
+{
+	struct rockchip_clk_pll *pll = (struct rockchip_clk_pll *)s->private;
+	u32 boost_count = 0;
+	u32 freq_cnt0 = 0, freq_cnt1 = 0;
+	u64 freq_cnt = 0, high_freq_time = 0;
+	u32 short_count = 0, short_threshold = 0;
+	u32 interval_time = 0;
+
+	seq_puts(s, " device    boost_count   high_freq_count  high_freq_time  short_count  short_threshold  interval_count\n");
+	seq_puts(s, "------------------------------------------------------------------------------------------------------\n");
+	seq_printf(s, " %s\n", clk_hw_get_name(&pll->hw));
+
+	regmap_read(pll->boost, BOOST_SWITCH_CNT, &boost_count);
+
+	regmap_read(pll->boost, BOOST_HIGH_PERF_CNT0, &freq_cnt0);
+	regmap_read(pll->boost, BOOST_HIGH_PERF_CNT1, &freq_cnt1);
+	freq_cnt = ((u64)freq_cnt1 << 32) + (u64)freq_cnt0;
+	high_freq_time = freq_cnt;
+	do_div(high_freq_time, 24);
+
+	regmap_read(pll->boost, BOOST_SHORT_SWITCH_CNT, &short_count);
+	regmap_read(pll->boost, BOOST_STATIS_THRESHOLD, &short_threshold);
+	regmap_read(pll->boost, BOOST_SWITCH_THRESHOLD, &interval_time);
+
+	seq_printf(s, "%22u %17llu %15llu %12u %16u %15u\n",
+		   boost_count, freq_cnt, high_freq_time, short_count,
+		   short_threshold, interval_time);
+
+	return 0;
+}
+
+static int boost_summary_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, boost_summary_show, inode->i_private);
+}
+
+static const struct file_operations boost_summary_fops = {
+	.open		= boost_summary_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+static int boost_config_show(struct seq_file *s, void *data)
+{
+	struct rockchip_clk_pll *pll = (struct rockchip_clk_pll *)s->private;
+
+	seq_printf(s, "boost_enabled:   %d\n", pll->boost_enabled);
+	seq_printf(s, "boost_low_rate:  %lu\n", pll->boost_low_rate);
+	seq_printf(s, "boost_high_rate: %lu\n", pll->boost_high_rate);
+
+	return 0;
+}
+
+static int boost_config_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, boost_config_show, inode->i_private);
+}
+
+static const struct file_operations boost_config_fops = {
+	.open		= boost_config_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+static int boost_debug_create_one(struct rockchip_clk_pll *pll,
+				  struct dentry *rootdir)
+{
+	struct dentry *pdentry, *d;
+
+	pdentry = debugfs_lookup(clk_hw_get_name(&pll->hw), rootdir);
+	if (!pdentry) {
+		pr_err("%s: failed to lookup %s dentry\n", __func__,
+		       clk_hw_get_name(&pll->hw));
+		return -ENOMEM;
+	}
+
+	d = debugfs_create_file("boost_summary", 0444, pdentry,
+				pll, &boost_summary_fops);
+	if (!d) {
+		pr_err("%s: failed to create boost_summary file\n", __func__);
+		return -ENOMEM;
+	}
+
+	d = debugfs_create_file("boost_config", 0444, pdentry,
+				pll, &boost_config_fops);
+	if (!d) {
+		pr_err("%s: failed to create boost config file\n", __func__);
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+static int __init boost_debug_init(void)
+{
+	struct rockchip_clk_pll *pll;
+	struct dentry *rootdir;
+
+	rootdir = debugfs_lookup("clk", NULL);
+	if (!rootdir) {
+		pr_err("%s: failed to lookup clk dentry\n", __func__);
+		return -ENOMEM;
+	}
+
+	mutex_lock(&clk_boost_lock);
+
+	hlist_for_each_entry(pll, &clk_boost_list, debug_node)
+		boost_debug_create_one(pll, rootdir);
+
+	mutex_unlock(&clk_boost_lock);
+
+	return 0;
+}
+late_initcall(boost_debug_init);
+#endif /* MODULE */
+#endif /* CONFIG_DEBUG_FS */
+#endif /* CONFIG_ROCKCHIP_CLK_BOOST */
diff --git a/drivers/clk/rockchip/clk.c b/drivers/clk/rockchip/clk.c
index 4059d9365..14d603c11 100644
--- a/drivers/clk/rockchip/clk.c
+++ b/drivers/clk/rockchip/clk.c
@@ -21,10 +21,15 @@
 #include <linux/mfd/syscon.h>
 #include <linux/regmap.h>
 #include <linux/reboot.h>
+#include <linux/rational.h>
 
 #include "../clk-fractional-divider.h"
 #include "clk.h"
 
+#ifdef MODULE
+static HLIST_HEAD(clk_ctx_list);
+#endif
+
 /*
  * Register a clock branch.
  * Most clock branches have a form like
@@ -185,11 +190,51 @@ static void rockchip_fractional_approximation(struct clk_hw *hw,
 	unsigned long p_rate, p_parent_rate;
 	struct clk_hw *p_parent;
 
+	if (rate == 0) {
+		pr_warn("%s p_rate(%ld), rate(%ld), maybe invalid frequency setting!\n",
+			clk_hw_get_name(hw), *parent_rate, rate);
+		*m = 0;
+		*n = 1;
+		return;
+	}
+
 	p_rate = clk_hw_get_rate(clk_hw_get_parent(hw));
 	if ((rate * 20 > p_rate) && (p_rate % rate != 0)) {
 		p_parent = clk_hw_get_parent(clk_hw_get_parent(hw));
-		p_parent_rate = clk_hw_get_rate(p_parent);
-		*parent_rate = p_parent_rate;
+		if (!p_parent) {
+			*parent_rate = p_rate;
+		} else {
+			p_parent_rate = clk_hw_get_rate(p_parent);
+			*parent_rate = p_parent_rate;
+		}
+
+		if (*parent_rate == 0) {
+			pr_warn("%s p_rate(%ld), rate(%ld), maybe invalid frequency setting!\n",
+				clk_hw_get_name(hw), *parent_rate, rate);
+			*m = 0;
+			*n = 1;
+			return;
+		}
+
+		if (*parent_rate < rate * 20) {
+			/*
+			 * Fractional frequency divider to do
+			 * integer frequency divider does not
+			 * need 20 times the limit.
+			 */
+			if (!(*parent_rate % rate)) {
+				*m = 1;
+				*n = *parent_rate / rate;
+				return;
+			} else {
+				pr_warn("%s p_rate(%ld) is low than rate(%ld)*20, use integer or half-div\n",
+					clk_hw_get_name(hw),
+					*parent_rate, rate);
+				*m = 0;
+				*n = 1;
+				return;
+			}
+		}
 	}
 
 	fd->flags |= CLK_FRAC_DIVIDER_POWER_OF_TWO_PS;
@@ -386,6 +431,12 @@ struct rockchip_clk_provider *rockchip_clk_init(struct device_node *np,
 
 	ctx->grf = syscon_regmap_lookup_by_phandle(ctx->cru_node,
 						   "rockchip,grf");
+	// ctx->pmugrf = syscon_regmap_lookup_by_phandle(ctx->cru_node,
+	// 					   "rockchip,pmugrf");
+
+#ifdef MODULE
+	hlist_add_head(&ctx->list_node, &clk_ctx_list);
+#endif
 
 	return ctx;
 
@@ -448,7 +499,8 @@ void rockchip_clk_register_branches(struct rockchip_clk_provider *ctx,
 					list->parent_names, list->num_parents,
 					flags,
 					ctx->reg_base + list->muxdiv_offset,
-					list->mux_shift, list->mux_width,
+					list->mux_shift,
+					BIT(list->mux_width) - 1,
 					list->mux_flags, list->mux_table,
 					&ctx->lock);
 			else
@@ -466,6 +518,13 @@ void rockchip_clk_register_branches(struct rockchip_clk_provider *ctx,
 				list->mux_shift, list->mux_width,
 				list->mux_flags);
 			break;
+		// case branch_muxpmugrf:
+		// 	clk = rockchip_clk_register_muxgrf(list->name,
+		// 		list->parent_names, list->num_parents,
+		// 		flags, ctx->pmugrf, list->muxdiv_offset,
+		// 		list->mux_shift, list->mux_width,
+		// 		list->mux_flags);
+		// 	break;
 		case branch_divider:
 			if (list->div_table)
 				clk = clk_register_divider_table(NULL,
@@ -509,6 +568,14 @@ void rockchip_clk_register_branches(struct rockchip_clk_provider *ctx,
 				ctx->reg_base + list->gate_offset,
 				list->gate_shift, list->gate_flags, &ctx->lock);
 			break;
+		// case branch_gate_no_set_rate:
+		// 	flags &= ~CLK_SET_RATE_PARENT;
+
+		// 	clk = clk_register_gate(NULL, list->name,
+		// 		list->parent_names[0], flags,
+		// 		ctx->reg_base + list->gate_offset,
+		// 		list->gate_shift, list->gate_flags, &ctx->lock);
+		// 	break;
 		case branch_composite:
 			clk = rockchip_clk_register_branch(list->name,
 				list->parent_names, list->num_parents,
@@ -530,11 +597,13 @@ void rockchip_clk_register_branches(struct rockchip_clk_provider *ctx,
 			);
 			break;
 		case branch_inverter:
+#ifdef CONFIG_ROCKCHIP_CLK_INV
 			clk = rockchip_clk_register_inverter(
 				list->name, list->parent_names,
 				list->num_parents,
 				ctx->reg_base + list->muxdiv_offset,
 				list->div_shift, list->div_flags, &ctx->lock);
+#endif
 			break;
 		case branch_factor:
 			clk = rockchip_clk_register_factor_branch(
@@ -610,6 +679,45 @@ void rockchip_clk_protect_critical(const char *const clocks[],
 }
 EXPORT_SYMBOL_GPL(rockchip_clk_protect_critical);
 
+void rockchip_clk_register_armclk_v2(struct rockchip_clk_provider *ctx,
+				     struct rockchip_clk_branch *list,
+				     const struct rockchip_cpuclk_rate_table *rates,
+				     int nrates)
+{
+	struct clk *clk;
+
+	clk = rockchip_clk_register_cpuclk_v2(list->name, list->parent_names,
+					      list->num_parents, ctx->reg_base,
+					      list->muxdiv_offset, list->mux_shift,
+					      list->mux_width, list->mux_flags,
+					      list->div_offset, list->div_shift,
+					      list->div_width, list->div_flags,
+					      list->flags, &ctx->lock, rates, nrates);
+	if (IS_ERR(clk)) {
+		pr_err("%s: failed to register clock %s: %ld\n",
+		       __func__, list->name, PTR_ERR(clk));
+		return;
+	}
+
+	rockchip_clk_add_lookup(ctx, clk, list->id);
+}
+EXPORT_SYMBOL_GPL(rockchip_clk_register_armclk_v2);
+
+void (*rk_dump_cru)(void);
+EXPORT_SYMBOL(rk_dump_cru);
+
+static int rk_clk_panic(struct notifier_block *this,
+			unsigned long ev, void *ptr)
+{
+	if (rk_dump_cru)
+		rk_dump_cru();
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block rk_clk_panic_block = {
+	.notifier_call = rk_clk_panic,
+};
+
 static void __iomem *rst_base;
 static unsigned int reg_restart;
 static void (*cb_restart)(void);
@@ -642,5 +750,86 @@ rockchip_register_restart_notifier(struct rockchip_clk_provider *ctx,
 	if (ret)
 		pr_err("%s: cannot register restart handler, %d\n",
 		       __func__, ret);
+	atomic_notifier_chain_register(&panic_notifier_list,
+				       &rk_clk_panic_block);
 }
 EXPORT_SYMBOL_GPL(rockchip_register_restart_notifier);
+
+#ifdef MODULE
+static struct clk **protect_clocks;
+static unsigned int protect_nclocks;
+
+int rockchip_clk_protect(struct rockchip_clk_provider *ctx,
+			 unsigned int *clocks, unsigned int nclocks)
+{
+	struct clk *clk = NULL;
+	int i = 0;
+
+	if (protect_clocks || !ctx || !clocks || !ctx->clk_data.clks)
+		return 0;
+
+	protect_clocks = kcalloc(nclocks, sizeof(void *), GFP_KERNEL);
+	if (!protect_clocks)
+		return -ENOMEM;
+
+	for (i = 0; i < nclocks; i++) {
+		if (clocks[i] >= ctx->clk_data.clk_num) {
+			pr_err("%s: invalid clock id %u\n", __func__, clocks[i]);
+			continue;
+		}
+		clk = ctx->clk_data.clks[clocks[i]];
+		if (clk) {
+			clk_prepare_enable(clk);
+			protect_clocks[i] = clk;
+		}
+	}
+	protect_nclocks = nclocks;
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(rockchip_clk_protect);
+
+void rockchip_clk_unprotect(void)
+{
+	int i = 0;
+
+	if (!protect_clocks || !protect_nclocks)
+		return;
+
+	for (i = 0; i < protect_nclocks; i++) {
+		if (protect_clocks[i])
+			clk_disable_unprepare(protect_clocks[i]);
+	}
+	protect_nclocks = 0;
+	kfree(protect_clocks);
+	protect_clocks = NULL;
+
+}
+EXPORT_SYMBOL_GPL(rockchip_clk_unprotect);
+
+void rockchip_clk_disable_unused(void)
+{
+	struct rockchip_clk_provider *ctx;
+	struct clk *clk;
+	struct clk_hw *hw;
+	int i = 0, flag = 0;
+
+	hlist_for_each_entry(ctx, &clk_ctx_list, list_node) {
+		for (i = 0; i < ctx->clk_data.clk_num; i++) {
+			clk = ctx->clk_data.clks[i];
+			if (clk && !IS_ERR(clk)) {
+				hw = __clk_get_hw(clk);
+				if (hw)
+					flag = clk_hw_get_flags(hw);
+				if (flag & CLK_IGNORE_UNUSED)
+					continue;
+				if (flag & CLK_IS_CRITICAL)
+					continue;
+				clk_prepare_enable(clk);
+				clk_disable_unprepare(clk);
+			}
+		}
+	}
+}
+EXPORT_SYMBOL_GPL(rockchip_clk_disable_unused);
+#endif /* MODULE */
diff --git a/drivers/clk/rockchip/clk.h b/drivers/clk/rockchip/clk.h
index 758ebaf22..3b630fe06 100644
--- a/drivers/clk/rockchip/clk.h
+++ b/drivers/clk/rockchip/clk.h
@@ -19,6 +19,7 @@
 
 #include <linux/io.h>
 #include <linux/clk-provider.h>
+#include <linux/panic_notifier.h>
 
 struct clk;
 
@@ -37,12 +38,25 @@ struct clk;
 #define BOOST_SWITCH_THRESHOLD		0x0024
 #define BOOST_FSM_STATUS		0x0028
 #define BOOST_PLL_L_CON(x)		((x) * 0x4 + 0x2c)
+#define BOOST_PLL_CON_MASK		0xffff
+#define BOOST_CORE_DIV_MASK		0x1f
+#define BOOST_CORE_DIV_SHIFT		0
+#define BOOST_BACKUP_PLL_MASK		0x3
+#define BOOST_BACKUP_PLL_SHIFT		8
+#define BOOST_BACKUP_PLL_USAGE_MASK	0x1
+#define BOOST_BACKUP_PLL_USAGE_SHIFT	12
+#define BOOST_BACKUP_PLL_USAGE_BORROW	0
+#define BOOST_BACKUP_PLL_USAGE_TARGET	1
+#define BOOST_ENABLE_MASK		0x1
+#define BOOST_ENABLE_SHIFT		0
 #define BOOST_RECOVERY_MASK		0x1
 #define BOOST_RECOVERY_SHIFT		1
 #define BOOST_SW_CTRL_MASK		0x1
 #define BOOST_SW_CTRL_SHIFT		2
 #define BOOST_LOW_FREQ_EN_MASK		0x1
 #define BOOST_LOW_FREQ_EN_SHIFT		3
+#define BOOST_STATIS_ENABLE_MASK	0x1
+#define BOOST_STATIS_ENABLE_SHIFT	4
 #define BOOST_BUSY_STATE		BIT(8)
 
 #define PX30_PLL_CON(x)			((x) * 0x4)
@@ -65,6 +79,64 @@ struct clk;
 #define PX30_PMU_CLKGATE_CON(x)		((x) * 0x4 + 0x80)
 #define PX30_PMU_MODE			0x0020
 
+#define RV1106_TOPCRU_BASE		0x10000
+#define RV1106_PERICRU_BASE		0x12000
+#define RV1106_VICRU_BASE		0x14000
+#define RV1106_NPUCRU_BASE		0x16000
+#define RV1106_CORECRU_BASE		0x18000
+#define RV1106_VEPUCRU_BASE		0x1A000
+#define RV1106_VOCRU_BASE		0x1C000
+#define RV1106_DDRCRU_BASE		0x1E000
+#define RV1106_SUBDDRCRU_BASE		0x1F000
+
+#define RV1106_VI_GRF_BASE		0x50000
+#define RV1106_VO_GRF_BASE		0x60000
+
+#define RV1106_PMUCLKSEL_CON(x)		((x) * 0x4 + 0x300)
+#define RV1106_PMUCLKGATE_CON(x)	((x) * 0x4 + 0x800)
+#define RV1106_PMUSOFTRST_CON(x)	((x) * 0x4 + 0xa00)
+#define RV1106_PLL_CON(x)		((x) * 0x4 + RV1106_TOPCRU_BASE)
+#define RV1106_MODE_CON			(0x280 + RV1106_TOPCRU_BASE)
+#define RV1106_CLKSEL_CON(x)		((x) * 0x4 + 0x300 + RV1106_TOPCRU_BASE)
+#define RV1106_CLKGATE_CON(x)		((x) * 0x4 + 0x800 + RV1106_TOPCRU_BASE)
+#define RV1106_SOFTRST_CON(x)		((x) * 0x4 + 0xa00 + RV1106_TOPCRU_BASE)
+#define RV1106_GLB_SRST_FST		(0xc08 + RV1106_TOPCRU_BASE)
+#define RV1106_GLB_SRST_SND		(0xc0c + RV1106_TOPCRU_BASE)
+#define RV1106_SDIO_CON0		(0x1c + RV1106_VO_GRF_BASE)
+#define RV1106_SDIO_CON1		(0x20 + RV1106_VO_GRF_BASE)
+#define RV1106_SDMMC_CON0		(0x4 + RV1106_VI_GRF_BASE)
+#define RV1106_SDMMC_CON1		(0x8 + RV1106_VI_GRF_BASE)
+#define RV1106_EMMC_CON0		(0x20)
+#define RV1106_EMMC_CON1		(0x24)
+#define RV1106_PERICLKSEL_CON(x)	((x) * 0x4 + 0x300 + RV1106_PERICRU_BASE)
+#define RV1106_PERICLKGATE_CON(x)	((x) * 0x4 + 0x800 + RV1106_PERICRU_BASE)
+#define RV1106_PERISOFTRST_CON(x)	((x) * 0x4 + 0xa00 + RV1106_PERICRU_BASE)
+#define RV1106_VICLKSEL_CON(x)		((x) * 0x4 + 0x300 + RV1106_VICRU_BASE)
+#define RV1106_VICLKGATE_CON(x)		((x) * 0x4 + 0x800 + RV1106_VICRU_BASE)
+#define RV1106_VISOFTRST_CON(x)		((x) * 0x4 + 0xa00 + RV1106_VICRU_BASE)
+#define RV1106_VICLKSEL_CON(x)		((x) * 0x4 + 0x300 + RV1106_VICRU_BASE)
+#define RV1106_VICLKGATE_CON(x)		((x) * 0x4 + 0x800 + RV1106_VICRU_BASE)
+#define RV1106_VISOFTRST_CON(x)		((x) * 0x4 + 0xa00 + RV1106_VICRU_BASE)
+#define RV1106_NPUCLKSEL_CON(x)		((x) * 0x4 + 0x300 + RV1106_NPUCRU_BASE)
+#define RV1106_NPUCLKGATE_CON(x)	((x) * 0x4 + 0x800 + RV1106_NPUCRU_BASE)
+#define RV1106_NPUSOFTRST_CON(x)	((x) * 0x4 + 0xa00 + RV1106_NPUCRU_BASE)
+#define RV1106_CORECLKSEL_CON(x)	((x) * 0x4 + 0x300 + RV1106_CORECRU_BASE)
+#define RV1106_CORECLKGATE_CON(x)	((x) * 0x4 + 0x800 + RV1106_CORECRU_BASE)
+#define RV1106_CORESOFTRST_CON(x)	((x) * 0x4 + 0xa00 + RV1106_CORECRU_BASE)
+#define RV1106_VEPUCLKSEL_CON(x)	((x) * 0x4 + 0x300 + RV1106_VEPUCRU_BASE)
+#define RV1106_VEPUCLKGATE_CON(x)	((x) * 0x4 + 0x800 + RV1106_VEPUCRU_BASE)
+#define RV1106_VEPUSOFTRST_CON(x)	((x) * 0x4 + 0xa00 + RV1106_VEPUCRU_BASE)
+#define RV1106_VOCLKSEL_CON(x)		((x) * 0x4 + 0x300 + RV1106_VOCRU_BASE)
+#define RV1106_VOCLKGATE_CON(x)		((x) * 0x4 + 0x800 + RV1106_VOCRU_BASE)
+#define RV1106_VOSOFTRST_CON(x)		((x) * 0x4 + 0xa00 + RV1106_VOCRU_BASE)
+#define RV1106_DDRCLKSEL_CON(x)		((x) * 0x4 + 0x300 + RV1106_DDRCRU_BASE)
+#define RV1106_DDRCLKGATE_CON(x)	((x) * 0x4 + 0x800 + RV1106_DDRCRU_BASE)
+#define RV1106_DDRSOFTRST_CON(x)	((x) * 0x4 + 0xa00 + RV1106_DDRCRU_BASE)
+#define RV1106_SUBDDRCLKSEL_CON(x)	((x) * 0x4 + 0x300 + RV1106_SUBDDRCRU_BASE)
+#define RV1106_SUBDDRCLKGATE_CON(x)	((x) * 0x4 + 0x800 + RV1106_SUBDDRCRU_BASE)
+#define RV1106_SUBDDRSOFTRST_CON(x)	((x) * 0x4 + 0xa00 + RV1106_SUBDDRCRU_BASE)
+#define RV1106_SUBDDRMODE_CON		(0x280 + RV1106_SUBDDRCRU_BASE)
+
 #define RV1108_PLL_CON(x)		((x) * 0x4)
 #define RV1108_CLKSEL_CON(x)		((x) * 0x4 + 0x60)
 #define RV1108_CLKGATE_CON(x)		((x) * 0x4 + 0x120)
@@ -98,6 +170,32 @@ struct clk;
 #define RV1126_EMMC_CON0		0x450
 #define RV1126_EMMC_CON1		0x454
 
+/*
+ * register positions shared by RK1808 RK2928, RK3036,
+ * RK3066, RK3188 and RK3228
+ */
+
+#define RK1808_PLL_CON(x)		((x) * 0x4)
+#define RK1808_MODE_CON			0xa0
+#define RK1808_MISC_CON			0xa4
+#define RK1808_MISC1_CON		0xa8
+#define RK1808_GLB_SRST_FST		0xb8
+#define RK1808_GLB_SRST_SND		0xbc
+#define RK1808_CLKSEL_CON(x)		((x) * 0x4 + 0x100)
+#define RK1808_CLKGATE_CON(x)		((x) * 0x4 + 0x230)
+#define RK1808_SOFTRST_CON(x)		((x) * 0x4 + 0x300)
+#define RK1808_SDMMC_CON0		0x380
+#define RK1808_SDMMC_CON1		0x384
+#define RK1808_SDIO_CON0		0x388
+#define RK1808_SDIO_CON1		0x38c
+#define RK1808_EMMC_CON0		0x390
+#define RK1808_EMMC_CON1		0x394
+
+#define RK1808_PMU_PLL_CON(x)		((x) * 0x4 + 0x4000)
+#define RK1808_PMU_MODE_CON		0x4020
+#define RK1808_PMU_CLKSEL_CON(x)	((x) * 0x4 + 0x4040)
+#define RK1808_PMU_CLKGATE_CON(x)	((x) * 0x4 + 0x4080)
+
 #define RK2928_PLL_CON(x)		((x) * 0x4)
 #define RK2928_MODE_CON		0x40
 #define RK2928_CLKSEL_CON(x)	((x) * 0x4 + 0x44)
@@ -207,6 +305,73 @@ struct clk;
 #define RK3399_PMU_CLKGATE_CON(x)	((x) * 0x4 + 0x100)
 #define RK3399_PMU_SOFTRST_CON(x)	((x) * 0x4 + 0x110)
 
+#define RK3528_PMU_CRU_BASE		0x10000
+#define RK3528_PCIE_CRU_BASE		0x20000
+#define RK3528_DDRPHY_CRU_BASE		0x28000
+#define RK3528_VPU_GRF_BASE		0x40000
+#define RK3528_VO_GRF_BASE		0x60000
+#define RK3528_SDMMC_CON0		(RK3528_VO_GRF_BASE + 0x24)
+#define RK3528_SDMMC_CON1		(RK3528_VO_GRF_BASE + 0x28)
+#define RK3528_SDIO0_CON0		(RK3528_VPU_GRF_BASE + 0x4)
+#define RK3528_SDIO0_CON1		(RK3528_VPU_GRF_BASE + 0x8)
+#define RK3528_SDIO1_CON0		(RK3528_VPU_GRF_BASE + 0xc)
+#define RK3528_SDIO1_CON1		(RK3528_VPU_GRF_BASE + 0x10)
+#define RK3528_PLL_CON(x)		RK2928_PLL_CON(x)
+#define RK3528_PCIE_PLL_CON(x)		((x) * 0x4 + RK3528_PCIE_CRU_BASE)
+#define RK3528_DDRPHY_PLL_CON(x)	((x) * 0x4 + RK3528_DDRPHY_CRU_BASE)
+#define RK3528_MODE_CON			0x280
+#define RK3528_CLKSEL_CON(x)		((x) * 0x4 + 0x300)
+#define RK3528_CLKGATE_CON(x)		((x) * 0x4 + 0x800)
+#define RK3528_SOFTRST_CON(x)		((x) * 0x4 + 0xa00)
+#define RK3528_PMU_CLKSEL_CON(x)	((x) * 0x4 + 0x300 + RK3528_PMU_CRU_BASE)
+#define RK3528_PMU_CLKGATE_CON(x)	((x) * 0x4 + 0x800 + RK3528_PMU_CRU_BASE)
+#define RK3528_PCIE_CLKSEL_CON(x)	((x) * 0x4 + 0x300 + RK3528_PCIE_CRU_BASE)
+#define RK3528_PCIE_CLKGATE_CON(x)	((x) * 0x4 + 0x800 + RK3528_PCIE_CRU_BASE)
+#define RK3528_DDRPHY_CLKGATE_CON(x)	((x) * 0x4 + 0x800 + RK3528_DDRPHY_CRU_BASE)
+#define RK3528_DDRPHY_MODE_CON		(0x280 + RK3528_DDRPHY_CRU_BASE)
+#define RK3528_GLB_CNT_TH		0xc00
+#define RK3528_GLB_SRST_FST		0xc08
+#define RK3528_GLB_SRST_SND		0xc0c
+
+#define RK3562_PMU0_CRU_BASE		0x10000
+#define RK3562_PMU1_CRU_BASE		0x18000
+#define RK3562_DDR_CRU_BASE		0x20000
+#define RK3562_SUBDDR_CRU_BASE		0x28000
+#define RK3562_PERI_CRU_BASE		0x30000
+
+#define RK3562_PLL_CON(x)		RK2928_PLL_CON(x)
+#define RK3562_PMU1_PLL_CON(x)		((x) * 0x4 + RK3562_PMU1_CRU_BASE + 0x40)
+#define RK3562_SUBDDR_PLL_CON(x)	((x) * 0x4 + RK3562_SUBDDR_CRU_BASE + 0x20)
+#define RK3562_MODE_CON			0x600
+#define RK3562_PMU1_MODE_CON		(RK3562_PMU1_CRU_BASE + 0x380)
+#define RK3562_SUBDDR_MODE_CON		(RK3562_SUBDDR_CRU_BASE + 0x380)
+#define RK3562_CLKSEL_CON(x)		((x) * 0x4 + 0x100)
+#define RK3562_CLKGATE_CON(x)		((x) * 0x4 + 0x300)
+#define RK3562_SOFTRST_CON(x)		((x) * 0x4 + 0x400)
+#define RK3562_DDR_CLKSEL_CON(x)	((x) * 0x4 + RK3562_DDR_CRU_BASE + 0x100)
+#define RK3562_DDR_CLKGATE_CON(x)	((x) * 0x4 + RK3562_DDR_CRU_BASE + 0x180)
+#define RK3562_DDR_SOFTRST_CON(x)	((x) * 0x4 + RK3562_DDR_CRU_BASE + 0x200)
+#define RK3562_SUBDDR_CLKSEL_CON(x)	((x) * 0x4 + RK3562_SUBDDR_CRU_BASE + 0x100)
+#define RK3562_SUBDDR_CLKGATE_CON(x)	((x) * 0x4 + RK3562_SUBDDR_CRU_BASE + 0x180)
+#define RK3562_SUBDDR_SOFTRST_CON(x)	((x) * 0x4 + RK3562_SUBDDR_CRU_BASE + 0x200)
+#define RK3562_PERI_CLKSEL_CON(x)	((x) * 0x4 + RK3562_PERI_CRU_BASE + 0x100)
+#define RK3562_PERI_CLKGATE_CON(x)	((x) * 0x4 + RK3562_PERI_CRU_BASE + 0x300)
+#define RK3562_PERI_SOFTRST_CON(x)	((x) * 0x4 + RK3562_PERI_CRU_BASE + 0x400)
+#define RK3562_PMU0_CLKSEL_CON(x)	((x) * 0x4 + RK3562_PMU0_CRU_BASE + 0x100)
+#define RK3562_PMU0_CLKGATE_CON(x)	((x) * 0x4 + RK3562_PMU0_CRU_BASE + 0x180)
+#define RK3562_PMU0_SOFTRST_CON(x)	((x) * 0x4 + RK3562_PMU0_CRU_BASE + 0x200)
+#define RK3562_PMU1_CLKSEL_CON(x)	((x) * 0x4 + RK3562_PMU1_CRU_BASE + 0x100)
+#define RK3562_PMU1_CLKGATE_CON(x)	((x) * 0x4 + RK3562_PMU1_CRU_BASE + 0x180)
+#define RK3562_PMU1_SOFTRST_CON(x)	((x) * 0x4 + RK3562_PMU1_CRU_BASE + 0x200)
+#define RK3562_GLB_SRST_FST		0x614
+#define RK3562_GLB_SRST_SND		0x618
+#define RK3562_GLB_RST_CON		0x61c
+#define RK3562_GLB_RST_ST		0x620
+#define RK3562_SDMMC0_CON0		0x624
+#define RK3562_SDMMC0_CON1		0x628
+#define RK3562_SDMMC1_CON0		0x62c
+#define RK3562_SDMMC1_CON1		0x630
+
 #define RK3568_PLL_CON(x)		RK2928_PLL_CON(x)
 #define RK3568_MODE_CON0		0xc0
 #define RK3568_MISC_CON0		0xc4
@@ -407,7 +572,13 @@ struct rockchip_pll_clock {
 	struct rockchip_pll_rate_table *rate_table;
 };
 
+/*
+ * PLL flags
+ */
 #define ROCKCHIP_PLL_SYNC_RATE		BIT(0)
+/* normal mode only. now only for pll_rk3036, pll_rk3328 type */
+#define ROCKCHIP_PLL_FIXED_MODE		BIT(1)
+#define ROCKCHIP_PLL_ALLOW_POWER_DOWN	BIT(2)
 
 #define PLL(_type, _id, _name, _pnames, _flags, _con, _mode, _mshift,	\
 		_lshift, _pflags, _rtable)				\
@@ -434,6 +605,14 @@ struct clk *rockchip_clk_register_pll(struct rockchip_clk_provider *ctx,
 		struct rockchip_pll_rate_table *rate_table,
 		unsigned long flags, u8 clk_pll_flags);
 
+void rockchip_boost_init(struct clk_hw *hw);
+
+void rockchip_boost_enable_recovery_sw_low(struct clk_hw *hw);
+
+void rockchip_boost_disable_recovery_sw(struct clk_hw *hw);
+
+void rockchip_boost_add_core_div(struct clk_hw *hw, unsigned long prate);
+
 struct rockchip_cpuclk_clksel {
 	int reg;
 	u32 val;
@@ -478,6 +657,17 @@ struct clk *rockchip_clk_register_cpuclk(const char *name,
 			const struct rockchip_cpuclk_rate_table *rates,
 			int nrates, void __iomem *reg_base, spinlock_t *lock);
 
+struct clk *rockchip_clk_register_cpuclk_v2(const char *name,
+					    const char *const *parent_names,
+					    u8 num_parents, void __iomem *base,
+					    int muxdiv_offset, u8 mux_shift,
+					    u8 mux_width, u8 mux_flags,
+					    int div_offset, u8 div_shift,
+					    u8 div_width, u8 div_flags,
+					    unsigned long flags, spinlock_t *lock,
+					    const struct rockchip_cpuclk_rate_table *rates,
+					    int nrates);
+
 struct clk *rockchip_clk_register_mmc(const char *name,
 				const char *const *parent_names, u8 num_parents,
 				void __iomem *reg, int shift);
@@ -986,6 +1176,13 @@ void rockchip_clk_register_armclk(struct rockchip_clk_provider *ctx,
 			const struct rockchip_cpuclk_rate_table *rates,
 			int nrates);
 void rockchip_clk_protect_critical(const char *const clocks[], int nclocks);
+void rockchip_clk_register_armclk_v2(struct rockchip_clk_provider *ctx,
+				     struct rockchip_clk_branch *list,
+				     const struct rockchip_cpuclk_rate_table *rates,
+				     int nrates);
+int rockchip_pll_clk_rate_to_scale(struct clk *clk, unsigned long rate);
+int rockchip_pll_clk_scale_to_rate(struct clk *clk, unsigned int scale);
+int rockchip_pll_clk_adaptive_scaling(struct clk *clk, int sel);
 void rockchip_register_restart_notifier(struct rockchip_clk_provider *ctx,
 					unsigned int reg, void (*cb)(void));
 
diff --git a/drivers/net/wireless/uwe5622/unisocwifi/cfg80211.c b/drivers/net/wireless/uwe5622/unisocwifi/cfg80211.c
index eb0c38afd..1d09976fc 100755
--- a/drivers/net/wireless/uwe5622/unisocwifi/cfg80211.c
+++ b/drivers/net/wireless/uwe5622/unisocwifi/cfg80211.c
@@ -429,7 +429,7 @@ int sprdwl_init_fw(struct sprdwl_vif *vif)
 	if (!vif->ndev)
 		mac = vif->wdev.address;
 	else
-		mac = vif->ndev->dev_addr;
+		mac = (u8 *)vif->ndev->dev_addr;
 
 	if (sprdwl_open_fw(priv, &vif_ctx_id, vif->mode, mac)) {
 		wl_ndev_log(L_ERR, vif->ndev, "%s failed!\n", __func__);
diff --git a/drivers/net/wireless/uwe5622/unisocwifi/main.c b/drivers/net/wireless/uwe5622/unisocwifi/main.c
index f1d6d7285..52842eda4 100755
--- a/drivers/net/wireless/uwe5622/unisocwifi/main.c
+++ b/drivers/net/wireless/uwe5622/unisocwifi/main.c
@@ -944,12 +944,12 @@ static int sprdwl_set_mac(struct net_device *dev, void *addr)
 		if (!is_zero_ether_addr(sa->sa_data)) {
 			vif->has_rand_mac = true;
 			memcpy(vif->random_mac, sa->sa_data, ETH_ALEN);
-			memcpy(dev->dev_addr, sa->sa_data, ETH_ALEN);
+			dev_addr_mod(dev, 0, sa->sa_data, ETH_ALEN);
 		} else {
 			vif->has_rand_mac = false;
 			netdev_info(dev, "need clear random mac for sta/softap mode\n");
 			memset(vif->random_mac, 0, ETH_ALEN);
-			memcpy(dev->dev_addr, vif->mac, ETH_ALEN);
+			dev_addr_mod(dev, 0, vif->mac, ETH_ALEN);
 		}
 	}
 	/*return success to pass vts test*/
@@ -1448,7 +1448,8 @@ static struct sprdwl_vif *sprdwl_register_netdev(struct sprdwl_priv *priv,
 	ndev->features |= NETIF_F_SG;
 	SET_NETDEV_DEV(ndev, wiphy_dev(priv->wiphy));
 
-	sprdwl_set_mac_addr(vif, addr, ndev->dev_addr);
+	// FIXME: set mac address
+	sprdwl_set_mac_addr(vif, addr, (u8 *)ndev->dev_addr);
 
 #ifdef CONFIG_P2P_INTF
 	if (type == NL80211_IFTYPE_P2P_DEVICE)
diff --git a/drivers/rknpu/Kconfig b/drivers/rknpu/Kconfig
new file mode 100644
index 000000000..c3343eece
--- /dev/null
+++ b/drivers/rknpu/Kconfig
@@ -0,0 +1,60 @@
+# SPDX-License-Identifier: GPL-2.0
+menu "RKNPU"
+	depends on ARCH_ROCKCHIP
+
+config ROCKCHIP_RKNPU
+	tristate "ROCKCHIP_RKNPU"
+	depends on DRM || DMABUF_HEAPS_ROCKCHIP_CMA_HEAP
+	help
+	  rknpu module.
+
+if ROCKCHIP_RKNPU
+
+config ROCKCHIP_RKNPU_DEBUG_FS
+	bool "RKNPU debugfs"
+	depends on DEBUG_FS
+	default y
+	help
+	  Enable debugfs to debug RKNPU usage.
+
+config ROCKCHIP_RKNPU_PROC_FS
+	bool "RKNPU procfs"
+	depends on PROC_FS
+	help
+	  Enable procfs to debug RKNPU usage.
+
+config ROCKCHIP_RKNPU_FENCE
+	bool "RKNPU fence"
+	depends on SYNC_FILE
+	help
+	  Enable fence support for RKNPU.
+
+config ROCKCHIP_RKNPU_SRAM
+	bool "RKNPU SRAM"
+	depends on NO_GKI
+	help
+	  Enable RKNPU SRAM support
+
+choice
+	prompt "RKNPU memory manager"
+	default ROCKCHIP_RKNPU_DRM_GEM
+	help
+	  Select RKNPU memory manager
+
+config ROCKCHIP_RKNPU_DRM_GEM
+	bool "RKNPU DRM GEM"
+	depends on DRM
+	help
+	  Enable RKNPU memory manager by DRM GEM.
+
+config ROCKCHIP_RKNPU_DMA_HEAP
+	bool "RKNPU DMA heap"
+	depends on DMABUF_HEAPS_ROCKCHIP_CMA_HEAP
+	help
+	  Enable RKNPU memory manager by DMA Heap.
+
+endchoice
+
+endif
+
+endmenu
diff --git a/drivers/rknpu/Makefile b/drivers/rknpu/Makefile
new file mode 100644
index 000000000..5cf2d56ed
--- /dev/null
+++ b/drivers/rknpu/Makefile
@@ -0,0 +1,16 @@
+# SPDX-License-Identifier: GPL-2.0
+obj-$(CONFIG_ROCKCHIP_RKNPU) += rknpu.o
+
+ccflags-y += -I$(srctree)/$(src)/include
+ccflags-y += -I$(src)/include
+ccflags-y += -Werror
+
+rknpu-y += rknpu_drv.o
+rknpu-y += rknpu_reset.o
+rknpu-y += rknpu_job.o
+rknpu-y += rknpu_debugger.o
+rknpu-y += rknpu_iommu.o
+rknpu-$(CONFIG_PM_DEVFREQ) += rknpu_devfreq.o
+rknpu-$(CONFIG_ROCKCHIP_RKNPU_SRAM) += rknpu_mm.o
+rknpu-$(CONFIG_ROCKCHIP_RKNPU_FENCE) += rknpu_fence.o
+rknpu-$(CONFIG_ROCKCHIP_RKNPU_DRM_GEM) += rknpu_gem.o
diff --git a/drivers/rknpu/include/rknpu_debugger.h b/drivers/rknpu/include/rknpu_debugger.h
new file mode 100644
index 000000000..3f4420d44
--- /dev/null
+++ b/drivers/rknpu/include/rknpu_debugger.h
@@ -0,0 +1,88 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#ifndef __LINUX_RKNPU_DEBUGGER_H_
+#define __LINUX_RKNPU_DEBUGGER_H_
+
+#include <linux/seq_file.h>
+
+/*
+ * struct rknpu_debugger - rknpu debugger information
+ *
+ * This structure represents a debugger to be created by the rknpu driver
+ * or core.
+ */
+struct rknpu_debugger {
+#ifdef CONFIG_ROCKCHIP_RKNPU_DEBUG_FS
+	/* Directory of debugfs file */
+	struct dentry *debugfs_dir;
+	struct list_head debugfs_entry_list;
+	struct mutex debugfs_lock;
+#endif
+#ifdef CONFIG_ROCKCHIP_RKNPU_PROC_FS
+	/* Directory of procfs file */
+	struct proc_dir_entry *procfs_dir;
+	struct list_head procfs_entry_list;
+	struct mutex procfs_lock;
+#endif
+};
+
+/*
+ * struct rknpu_debugger_list - debugfs/procfs info list entry
+ *
+ * This structure represents a debugfs/procfs file to be created by the npu
+ * driver or core.
+ */
+struct rknpu_debugger_list {
+	/* File name */
+	const char *name;
+	/*
+	 * Show callback. &seq_file->private will be set to the &struct
+	 * rknpu_debugger_node corresponding to the instance of this info
+	 * on a given &struct rknpu_debugger.
+	 */
+	int (*show)(struct seq_file *seq, void *data);
+	/*
+	 * Write callback. &seq_file->private will be set to the &struct
+	 * rknpu_debugger_node corresponding to the instance of this info
+	 * on a given &struct rknpu_debugger.
+	 */
+	ssize_t (*write)(struct file *file, const char __user *ubuf, size_t len,
+			 loff_t *offp);
+	/* Procfs/Debugfs private data. */
+	void *data;
+};
+
+/*
+ * struct rknpu_debugger_node - Nodes for debugfs/procfs
+ *
+ * This structure represents each instance of procfs/debugfs created from the
+ * template.
+ */
+struct rknpu_debugger_node {
+	struct rknpu_debugger *debugger;
+
+	/* template for this node. */
+	const struct rknpu_debugger_list *info_ent;
+
+	/* Each Procfs/Debugfs file. */
+#ifdef CONFIG_ROCKCHIP_RKNPU_DEBUG_FS
+	struct dentry *dent;
+#endif
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_PROC_FS
+	struct proc_dir_entry *pent;
+#endif
+
+	struct list_head list;
+};
+
+struct rknpu_device;
+
+int rknpu_debugger_init(struct rknpu_device *rknpu_dev);
+int rknpu_debugger_remove(struct rknpu_device *rknpu_dev);
+
+#endif /* __LINUX_RKNPU_FENCE_H_ */
diff --git a/drivers/rknpu/include/rknpu_devfreq.h b/drivers/rknpu/include/rknpu_devfreq.h
new file mode 100644
index 000000000..3edeb1fe4
--- /dev/null
+++ b/drivers/rknpu/include/rknpu_devfreq.h
@@ -0,0 +1,46 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Finley Xiao <finley.xiao@rock-chips.com>
+ */
+
+#ifndef __LINUX_RKNPU_DEVFREQ_H
+#define __LINUX_RKNPU_DEVFREQ_H
+
+#ifdef CONFIG_PM_DEVFREQ
+void rknpu_devfreq_lock(struct rknpu_device *rknpu_dev);
+void rknpu_devfreq_unlock(struct rknpu_device *rknpu_dev);
+int rknpu_devfreq_init(struct rknpu_device *rknpu_dev);
+void rknpu_devfreq_remove(struct rknpu_device *rknpu_dev);
+int rknpu_devfreq_runtime_suspend(struct device *dev);
+int rknpu_devfreq_runtime_resume(struct device *dev);
+#else
+static inline int rknpu_devfreq_init(struct rknpu_device *rknpu_dev)
+{
+	return -EOPNOTSUPP;
+}
+
+static inline void rknpu_devfreq_remove(struct rknpu_device *rknpu_dev)
+{
+}
+
+static inline void rknpu_devfreq_lock(struct rknpu_device *rknpu_dev)
+{
+}
+
+static inline void rknpu_devfreq_unlock(struct rknpu_device *rknpu_dev)
+{
+}
+
+static inline int rknpu_devfreq_runtime_suspend(struct device *dev)
+{
+	return -EOPNOTSUPP;
+}
+
+static inline int rknpu_devfreq_runtime_resume(struct device *dev)
+{
+	return -EOPNOTSUPP;
+}
+#endif /* CONFIG_PM_DEVFREQ */
+
+#endif /* __LINUX_RKNPU_DEVFREQ_H_ */
diff --git a/drivers/rknpu/include/rknpu_drv.h b/drivers/rknpu/include/rknpu_drv.h
new file mode 100644
index 000000000..98fba97c4
--- /dev/null
+++ b/drivers/rknpu/include/rknpu_drv.h
@@ -0,0 +1,169 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#ifndef __LINUX_RKNPU_DRV_H_
+#define __LINUX_RKNPU_DRV_H_
+
+#include <linux/completion.h>
+#include <linux/device.h>
+#include <linux/kref.h>
+#include <linux/platform_device.h>
+#include <linux/spinlock.h>
+#include <linux/regulator/consumer.h>
+#include <linux/version.h>
+#include <linux/hrtimer.h>
+#include <linux/miscdevice.h>
+
+#include <soc/rockchip/rockchip_opp_select.h>
+#include <soc/rockchip/rockchip_system_monitor.h>
+#include <soc/rockchip/rockchip_ipa.h>
+
+#include "rknpu_job.h"
+#include "rknpu_fence.h"
+#include "rknpu_debugger.h"
+#include "rknpu_mm.h"
+
+#define DRIVER_NAME "rknpu"
+#define DRIVER_DESC "RKNPU driver"
+#define DRIVER_DATE "20231121"
+#define DRIVER_MAJOR 0
+#define DRIVER_MINOR 9
+#define DRIVER_PATCHLEVEL 3
+
+#define LOG_TAG "RKNPU"
+
+/* sample interval: 1000ms */
+#define RKNPU_LOAD_INTERVAL 1000000000
+
+#define LOG_INFO(fmt, args...) pr_info(LOG_TAG ": " fmt, ##args)
+#if KERNEL_VERSION(5, 5, 0) <= LINUX_VERSION_CODE
+#define LOG_WARN(fmt, args...) pr_warn(LOG_TAG ": " fmt, ##args)
+#else
+#define LOG_WARN(fmt, args...) pr_warning(LOG_TAG ": " fmt, ##args)
+#endif
+#define LOG_DEBUG(fmt, args...) pr_devel(LOG_TAG ": " fmt, ##args)
+#define LOG_ERROR(fmt, args...) pr_err(LOG_TAG ": " fmt, ##args)
+
+#define LOG_DEV_INFO(dev, fmt, args...) dev_info(dev, LOG_TAG ": " fmt, ##args)
+#define LOG_DEV_WARN(dev, fmt, args...) dev_warn(dev, LOG_TAG ": " fmt, ##args)
+#define LOG_DEV_DEBUG(dev, fmt, args...) dev_dbg(dev, LOG_TAG ": " fmt, ##args)
+#define LOG_DEV_ERROR(dev, fmt, args...) dev_err(dev, LOG_TAG ": " fmt, ##args)
+
+struct rknpu_reset_data {
+	const char *srst_a_name;
+	const char *srst_h_name;
+};
+
+struct rknpu_config {
+	__u32 bw_priority_addr;
+	__u32 bw_priority_length;
+	__u64 dma_mask;
+	__u32 pc_data_amount_scale;
+	__u32 pc_task_number_bits;
+	__u32 pc_task_number_mask;
+	__u32 pc_task_status_offset;
+	__u32 pc_dma_ctrl;
+	__u32 bw_enable;
+	const struct rknpu_irqs_data *irqs;
+	const struct rknpu_reset_data *resets;
+	int num_irqs;
+	int num_resets;
+	__u64 nbuf_phyaddr;
+	__u64 nbuf_size;
+	__u64 max_submit_number;
+	__u32 core_mask;
+};
+
+struct rknpu_timer {
+	ktime_t busy_time;
+	ktime_t total_busy_time;
+};
+
+struct rknpu_subcore_data {
+	struct list_head todo_list;
+	wait_queue_head_t job_done_wq;
+	struct rknpu_job *job;
+	int64_t task_num;
+	struct rknpu_timer timer;
+};
+
+/**
+ * RKNPU device
+ *
+ * @base: IO mapped base address for device
+ * @dev: Device instance
+ * @drm_dev: DRM device instance
+ */
+struct rknpu_device {
+	void __iomem *base[RKNPU_MAX_CORES];
+	struct device *dev;
+#ifdef CONFIG_ROCKCHIP_RKNPU_DRM_GEM
+	struct device *fake_dev;
+	struct drm_device *drm_dev;
+#endif
+#ifdef CONFIG_ROCKCHIP_RKNPU_DMA_HEAP
+	struct miscdevice miscdev;
+	struct rk_dma_heap *heap;
+#endif
+	atomic_t sequence;
+	spinlock_t lock;
+	spinlock_t irq_lock;
+	struct mutex power_lock;
+	struct mutex reset_lock;
+	struct rknpu_subcore_data subcore_datas[RKNPU_MAX_CORES];
+	const struct rknpu_config *config;
+	void __iomem *bw_priority_base;
+	struct rknpu_fence_context *fence_ctx;
+	bool iommu_en;
+	struct reset_control *srst_a[RKNPU_MAX_CORES];
+	struct reset_control *srst_h[RKNPU_MAX_CORES];
+	struct clk_bulk_data *clks;
+	int num_clks;
+	struct regulator *vdd;
+	struct regulator *mem;
+	struct monitor_dev_info *mdev_info;
+	struct ipa_power_model_data *model_data;
+	struct thermal_cooling_device *devfreq_cooling;
+	struct devfreq *devfreq;
+	unsigned long ondemand_freq;
+	struct rockchip_opp_info opp_info;
+	unsigned long current_freq;
+	unsigned long current_volt;
+	int bypass_irq_handler;
+	int bypass_soft_reset;
+	bool soft_reseting;
+	struct device *genpd_dev_npu0;
+	struct device *genpd_dev_npu1;
+	struct device *genpd_dev_npu2;
+	bool multiple_domains;
+	atomic_t power_refcount;
+	atomic_t cmdline_power_refcount;
+	struct delayed_work power_off_work;
+	struct workqueue_struct *power_off_wq;
+	struct rknpu_debugger debugger;
+	struct hrtimer timer;
+	ktime_t kt;
+	phys_addr_t sram_start;
+	phys_addr_t sram_end;
+	phys_addr_t nbuf_start;
+	phys_addr_t nbuf_end;
+	uint32_t sram_size;
+	uint32_t nbuf_size;
+	void __iomem *sram_base_io;
+	void __iomem *nbuf_base_io;
+	struct rknpu_mm *sram_mm;
+	unsigned long power_put_delay;
+};
+
+struct rknpu_session {
+	struct rknpu_device *rknpu_dev;
+	struct list_head list;
+};
+
+int rknpu_power_get(struct rknpu_device *rknpu_dev);
+int rknpu_power_put(struct rknpu_device *rknpu_dev);
+
+#endif /* __LINUX_RKNPU_DRV_H_ */
diff --git a/drivers/rknpu/include/rknpu_fence.h b/drivers/rknpu/include/rknpu_fence.h
new file mode 100644
index 000000000..164f6de41
--- /dev/null
+++ b/drivers/rknpu/include/rknpu_fence.h
@@ -0,0 +1,24 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#ifndef __LINUX_RKNPU_FENCE_H_
+#define __LINUX_RKNPU_FENCE_H_
+
+#include "rknpu_job.h"
+
+struct rknpu_fence_context {
+	unsigned int context;
+	unsigned int seqno;
+	spinlock_t spinlock;
+};
+
+int rknpu_fence_context_alloc(struct rknpu_device *rknpu_dev);
+
+int rknpu_fence_alloc(struct rknpu_job *job);
+
+int rknpu_fence_get_fd(struct rknpu_job *job);
+
+#endif /* __LINUX_RKNPU_FENCE_H_ */
diff --git a/drivers/rknpu/include/rknpu_gem.h b/drivers/rknpu/include/rknpu_gem.h
new file mode 100644
index 000000000..aedcab89d
--- /dev/null
+++ b/drivers/rknpu/include/rknpu_gem.h
@@ -0,0 +1,213 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#ifndef __LINUX_RKNPU_GEM_H
+#define __LINUX_RKNPU_GEM_H
+
+#include <linux/mm_types.h>
+#include <linux/version.h>
+
+#include <drm/drm_device.h>
+#include <drm/drm_vma_manager.h>
+#include <drm/drm_gem.h>
+#include <drm/drm_mode.h>
+
+#if KERNEL_VERSION(4, 14, 0) > LINUX_VERSION_CODE
+#include <drm/drm_mem_util.h>
+#endif
+
+#include "rknpu_mm.h"
+
+#define to_rknpu_obj(x) container_of(x, struct rknpu_gem_object, base)
+
+/*
+ * rknpu drm buffer structure.
+ *
+ * @base: a gem object.
+ *	- a new handle to this gem object would be created
+ *	by drm_gem_handle_create().
+ * @flags: indicate memory type to allocated buffer and cache attribute.
+ * @size: size requested from user, in bytes and this size is aligned
+ *	in page unit.
+ * @cookie: cookie returned by dma_alloc_attrs
+ * @kv_addr: kernel virtual address to allocated memory region.
+ * @dma_addr: bus address(accessed by dma) to allocated memory region.
+ *	- this address could be physical address without IOMMU and
+ *	device address with IOMMU.
+ * @pages: Array of backing pages.
+ * @sgt: Imported sg_table.
+ *
+ * P.S. this object would be transferred to user as kms_bo.handle so
+ *	user can access the buffer through kms_bo.handle.
+ */
+struct rknpu_gem_object {
+	struct drm_gem_object base;
+	unsigned int flags;
+	unsigned long size;
+	unsigned long sram_size;
+	unsigned long nbuf_size;
+	struct rknpu_mm_obj *sram_obj;
+	dma_addr_t iova_start;
+	unsigned long iova_size;
+	void *cookie;
+	void __iomem *kv_addr;
+	dma_addr_t dma_addr;
+	unsigned long dma_attrs;
+	unsigned long num_pages;
+	struct page **pages;
+	struct sg_table *sgt;
+	struct drm_mm_node mm_node;
+};
+
+enum rknpu_cache_type {
+	RKNPU_CACHE_SRAM = 1 << 0,
+	RKNPU_CACHE_NBUF = 1 << 1,
+};
+
+/* create a new buffer with gem object */
+struct rknpu_gem_object *rknpu_gem_object_create(struct drm_device *dev,
+						 unsigned int flags,
+						 unsigned long size,
+						 unsigned long sram_size);
+
+/* destroy a buffer with gem object */
+void rknpu_gem_object_destroy(struct rknpu_gem_object *rknpu_obj);
+
+/* request gem object creation and buffer allocation as the size */
+int rknpu_gem_create_ioctl(struct drm_device *dev, void *data,
+			   struct drm_file *file_priv);
+
+/* get fake-offset of gem object that can be used with mmap. */
+int rknpu_gem_map_ioctl(struct drm_device *dev, void *data,
+			struct drm_file *file_priv);
+
+int rknpu_gem_destroy_ioctl(struct drm_device *dev, void *data,
+			    struct drm_file *file_priv);
+
+/*
+ * get rknpu drm object,
+ * gem object reference count would be increased.
+ */
+static inline void rknpu_gem_object_get(struct drm_gem_object *obj)
+{
+#if KERNEL_VERSION(4, 13, 0) < LINUX_VERSION_CODE
+	drm_gem_object_get(obj);
+#else
+	drm_gem_object_reference(obj);
+#endif
+}
+
+/*
+ * put rknpu drm object acquired from rknpu_gem_object_find() or rknpu_gem_object_get(),
+ * gem object reference count would be decreased.
+ */
+static inline void rknpu_gem_object_put(struct drm_gem_object *obj)
+{
+#if KERNEL_VERSION(5, 9, 0) <= LINUX_VERSION_CODE
+	drm_gem_object_put(obj);
+#elif KERNEL_VERSION(4, 13, 0) < LINUX_VERSION_CODE
+	drm_gem_object_put_unlocked(obj);
+#else
+	drm_gem_object_unreference_unlocked(obj);
+#endif
+}
+
+/*
+ * get rknpu drm object from gem handle, this function could be used for
+ * other drivers such as 2d/3d acceleration drivers.
+ * with this function call, gem object reference count would be increased.
+ */
+static inline struct rknpu_gem_object *
+rknpu_gem_object_find(struct drm_file *filp, unsigned int handle)
+{
+	struct drm_gem_object *obj;
+
+	obj = drm_gem_object_lookup(filp, handle);
+	if (!obj) {
+		// DRM_ERROR("failed to lookup gem object.\n");
+		return NULL;
+	}
+
+	rknpu_gem_object_put(obj);
+
+	return to_rknpu_obj(obj);
+}
+
+/* get buffer information to memory region allocated by gem. */
+int rknpu_gem_get_ioctl(struct drm_device *dev, void *data,
+			struct drm_file *file_priv);
+
+/* free gem object. */
+void rknpu_gem_free_object(struct drm_gem_object *obj);
+
+/* create memory region for drm framebuffer. */
+int rknpu_gem_dumb_create(struct drm_file *file_priv, struct drm_device *dev,
+			  struct drm_mode_create_dumb *args);
+
+#if KERNEL_VERSION(4, 19, 0) > LINUX_VERSION_CODE
+/* map memory region for drm framebuffer to user space. */
+int rknpu_gem_dumb_map_offset(struct drm_file *file_priv,
+			      struct drm_device *dev, uint32_t handle,
+			      uint64_t *offset);
+#endif
+
+/* page fault handler and mmap fault address(virtual) to physical memory. */
+#if KERNEL_VERSION(4, 15, 0) <= LINUX_VERSION_CODE
+vm_fault_t rknpu_gem_fault(struct vm_fault *vmf);
+#elif KERNEL_VERSION(4, 14, 0) <= LINUX_VERSION_CODE
+int rknpu_gem_fault(struct vm_fault *vmf);
+#else
+int rknpu_gem_fault(struct vm_area_struct *vma, struct vm_fault *vmf);
+#endif
+
+int rknpu_gem_mmap_obj(struct drm_gem_object *obj, struct vm_area_struct *vma);
+
+/* set vm_flags and we can change the vm attribute to other one at here. */
+int rknpu_gem_mmap(struct file *filp, struct vm_area_struct *vma);
+
+/* low-level interface prime helpers */
+#if KERNEL_VERSION(4, 13, 0) <= LINUX_VERSION_CODE
+struct drm_gem_object *rknpu_gem_prime_import(struct drm_device *dev,
+					      struct dma_buf *dma_buf);
+#endif
+struct sg_table *rknpu_gem_prime_get_sg_table(struct drm_gem_object *obj);
+struct drm_gem_object *
+rknpu_gem_prime_import_sg_table(struct drm_device *dev,
+				struct dma_buf_attachment *attach,
+				struct sg_table *sgt);
+#if KERNEL_VERSION(6, 1, 0) > LINUX_VERSION_CODE
+void *rknpu_gem_prime_vmap(struct drm_gem_object *obj);
+void rknpu_gem_prime_vunmap(struct drm_gem_object *obj, void *vaddr);
+#else
+int rknpu_gem_prime_vmap(struct drm_gem_object *obj, struct iosys_map *map);
+void rknpu_gem_prime_vunmap(struct drm_gem_object *obj, struct iosys_map *map);
+#endif
+int rknpu_gem_prime_mmap(struct drm_gem_object *obj,
+			 struct vm_area_struct *vma);
+
+int rknpu_gem_sync_ioctl(struct drm_device *dev, void *data,
+			 struct drm_file *file_priv);
+
+static inline void *rknpu_gem_alloc_page(size_t nr_pages)
+{
+#if KERNEL_VERSION(4, 13, 0) <= LINUX_VERSION_CODE
+	return kvmalloc_array(nr_pages, sizeof(struct page *),
+			      GFP_KERNEL | __GFP_ZERO);
+#else
+	return drm_calloc_large(nr_pages, sizeof(struct page *));
+#endif
+}
+
+static inline void rknpu_gem_free_page(void *pages)
+{
+#if KERNEL_VERSION(4, 13, 0) <= LINUX_VERSION_CODE
+	kvfree(pages);
+#else
+	drm_free_large(pages);
+#endif
+}
+
+#endif
diff --git a/drivers/rknpu/include/rknpu_ioctl.h b/drivers/rknpu/include/rknpu_ioctl.h
new file mode 100644
index 000000000..35b46701c
--- /dev/null
+++ b/drivers/rknpu/include/rknpu_ioctl.h
@@ -0,0 +1,324 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#ifndef __LINUX_RKNPU_IOCTL_H
+#define __LINUX_RKNPU_IOCTL_H
+
+#include <linux/ioctl.h>
+#include <linux/types.h>
+
+#if !defined(__KERNEL__)
+#define __user
+#endif
+
+#ifndef __packed
+#define __packed __attribute__((packed))
+#endif
+
+#define RKNPU_OFFSET_VERSION 0x0
+#define RKNPU_OFFSET_VERSION_NUM 0x4
+#define RKNPU_OFFSET_PC_OP_EN 0x8
+#define RKNPU_OFFSET_PC_DATA_ADDR 0x10
+#define RKNPU_OFFSET_PC_DATA_AMOUNT 0x14
+#define RKNPU_OFFSET_PC_TASK_CONTROL 0x30
+#define RKNPU_OFFSET_PC_DMA_BASE_ADDR 0x34
+
+#define RKNPU_OFFSET_INT_MASK 0x20
+#define RKNPU_OFFSET_INT_CLEAR 0x24
+#define RKNPU_OFFSET_INT_STATUS 0x28
+#define RKNPU_OFFSET_INT_RAW_STATUS 0x2c
+
+#define RKNPU_OFFSET_CLR_ALL_RW_AMOUNT 0x8010
+#define RKNPU_OFFSET_DT_WR_AMOUNT 0x8034
+#define RKNPU_OFFSET_DT_RD_AMOUNT 0x8038
+#define RKNPU_OFFSET_WT_RD_AMOUNT 0x803c
+
+#define RKNPU_OFFSET_ENABLE_MASK 0xf008
+
+#define RKNPU_INT_CLEAR 0x1ffff
+
+#define RKNPU_PC_DATA_EXTRA_AMOUNT 4
+
+#define RKNPU_STR_HELPER(x) #x
+
+#define RKNPU_GET_DRV_VERSION_STRING(MAJOR, MINOR, PATCHLEVEL)                 \
+	RKNPU_STR_HELPER(MAJOR)                                                \
+	"." RKNPU_STR_HELPER(MINOR) "." RKNPU_STR_HELPER(PATCHLEVEL)
+#define RKNPU_GET_DRV_VERSION_CODE(MAJOR, MINOR, PATCHLEVEL)                   \
+	(MAJOR * 10000 + MINOR * 100 + PATCHLEVEL)
+#define RKNPU_GET_DRV_VERSION_MAJOR(CODE) (CODE / 10000)
+#define RKNPU_GET_DRV_VERSION_MINOR(CODE) ((CODE % 10000) / 100)
+#define RKNPU_GET_DRV_VERSION_PATCHLEVEL(CODE) (CODE % 100)
+
+/* memory type definitions. */
+enum e_rknpu_mem_type {
+	/* physically continuous memory and used as default. */
+	RKNPU_MEM_CONTIGUOUS = 0 << 0,
+	/* physically non-continuous memory. */
+	RKNPU_MEM_NON_CONTIGUOUS = 1 << 0,
+	/* non-cacheable mapping and used as default. */
+	RKNPU_MEM_NON_CACHEABLE = 0 << 1,
+	/* cacheable mapping. */
+	RKNPU_MEM_CACHEABLE = 1 << 1,
+	/* write-combine mapping. */
+	RKNPU_MEM_WRITE_COMBINE = 1 << 2,
+	/* dma attr kernel mapping */
+	RKNPU_MEM_KERNEL_MAPPING = 1 << 3,
+	/* iommu mapping */
+	RKNPU_MEM_IOMMU = 1 << 4,
+	/* zero mapping */
+	RKNPU_MEM_ZEROING = 1 << 5,
+	/* allocate secure buffer */
+	RKNPU_MEM_SECURE = 1 << 6,
+	/* allocate from dma32 zone */
+	RKNPU_MEM_DMA32 = 1 << 7,
+	/* request SRAM */
+	RKNPU_MEM_TRY_ALLOC_SRAM = 1 << 8,
+	/* request NBUF */
+	RKNPU_MEM_TRY_ALLOC_NBUF = 1 << 9,
+	RKNPU_MEM_MASK = RKNPU_MEM_NON_CONTIGUOUS | RKNPU_MEM_CACHEABLE |
+			 RKNPU_MEM_WRITE_COMBINE | RKNPU_MEM_KERNEL_MAPPING |
+			 RKNPU_MEM_IOMMU | RKNPU_MEM_ZEROING |
+			 RKNPU_MEM_SECURE | RKNPU_MEM_DMA32 |
+			 RKNPU_MEM_TRY_ALLOC_SRAM | RKNPU_MEM_TRY_ALLOC_NBUF
+};
+
+/* sync mode definitions. */
+enum e_rknpu_mem_sync_mode {
+	RKNPU_MEM_SYNC_TO_DEVICE = 1 << 0,
+	RKNPU_MEM_SYNC_FROM_DEVICE = 1 << 1,
+	RKNPU_MEM_SYNC_MASK =
+		RKNPU_MEM_SYNC_TO_DEVICE | RKNPU_MEM_SYNC_FROM_DEVICE
+};
+
+/* job mode definitions. */
+enum e_rknpu_job_mode {
+	RKNPU_JOB_SLAVE = 0 << 0,
+	RKNPU_JOB_PC = 1 << 0,
+	RKNPU_JOB_BLOCK = 0 << 1,
+	RKNPU_JOB_NONBLOCK = 1 << 1,
+	RKNPU_JOB_PINGPONG = 1 << 2,
+	RKNPU_JOB_FENCE_IN = 1 << 3,
+	RKNPU_JOB_FENCE_OUT = 1 << 4,
+	RKNPU_JOB_MASK = RKNPU_JOB_PC | RKNPU_JOB_NONBLOCK |
+			 RKNPU_JOB_PINGPONG | RKNPU_JOB_FENCE_IN |
+			 RKNPU_JOB_FENCE_OUT
+};
+
+/* action definitions */
+enum e_rknpu_action {
+	RKNPU_GET_HW_VERSION = 0,
+	RKNPU_GET_DRV_VERSION = 1,
+	RKNPU_GET_FREQ = 2,
+	RKNPU_SET_FREQ = 3,
+	RKNPU_GET_VOLT = 4,
+	RKNPU_SET_VOLT = 5,
+	RKNPU_ACT_RESET = 6,
+	RKNPU_GET_BW_PRIORITY = 7,
+	RKNPU_SET_BW_PRIORITY = 8,
+	RKNPU_GET_BW_EXPECT = 9,
+	RKNPU_SET_BW_EXPECT = 10,
+	RKNPU_GET_BW_TW = 11,
+	RKNPU_SET_BW_TW = 12,
+	RKNPU_ACT_CLR_TOTAL_RW_AMOUNT = 13,
+	RKNPU_GET_DT_WR_AMOUNT = 14,
+	RKNPU_GET_DT_RD_AMOUNT = 15,
+	RKNPU_GET_WT_RD_AMOUNT = 16,
+	RKNPU_GET_TOTAL_RW_AMOUNT = 17,
+	RKNPU_GET_IOMMU_EN = 18,
+	RKNPU_SET_PROC_NICE = 19,
+	RKNPU_POWER_ON = 20,
+	RKNPU_POWER_OFF = 21,
+	RKNPU_GET_TOTAL_SRAM_SIZE = 22,
+	RKNPU_GET_FREE_SRAM_SIZE = 23,
+};
+
+/**
+ * User-desired buffer creation information structure.
+ *
+ * @handle: The handle of the created GEM object.
+ * @flags: user request for setting memory type or cache attributes.
+ * @size: user-desired memory allocation size.
+ *	- this size value would be page-aligned internally.
+ * @obj_addr: address of RKNPU memory object.
+ * @dma_addr: dma address that access by rknpu.
+ * @sram_size: user-desired sram memory allocation size.
+ *  - this size value would be page-aligned internally.
+ */
+struct rknpu_mem_create {
+	__u32 handle;
+	__u32 flags;
+	__u64 size;
+	__u64 obj_addr;
+	__u64 dma_addr;
+	__u64 sram_size;
+};
+
+/**
+ * A structure for getting a fake-offset that can be used with mmap.
+ *
+ * @handle: handle of gem object.
+ * @reserved: just padding to be 64-bit aligned.
+ * @offset: a fake-offset of gem object.
+ */
+struct rknpu_mem_map {
+	__u32 handle;
+	__u32 reserved;
+	__u64 offset;
+};
+
+/**
+ * For destroying DMA buffer
+ *
+ * @handle:	handle of the buffer.
+ * @reserved: reserved for padding.
+ * @obj_addr: rknpu_mem_object addr.
+ */
+struct rknpu_mem_destroy {
+	__u32 handle;
+	__u32 reserved;
+	__u64 obj_addr;
+};
+
+/**
+ * For synchronizing DMA buffer
+ *
+ * @flags: user request for setting memory type or cache attributes.
+ * @reserved: reserved for padding.
+ * @obj_addr: address of RKNPU memory object.
+ * @offset: offset in bytes from start address of buffer.
+ * @size: size of memory region.
+ *
+ */
+struct rknpu_mem_sync {
+	__u32 flags;
+	__u32 reserved;
+	__u64 obj_addr;
+	__u64 offset;
+	__u64 size;
+};
+
+/**
+ * struct rknpu_task structure for task information
+ *
+ * @flags: flags for task
+ * @op_idx: operator index
+ * @enable_mask: enable mask
+ * @int_mask: interrupt mask
+ * @int_clear: interrupt clear
+ * @int_status: interrupt status
+ * @regcfg_amount: register config number
+ * @regcfg_offset: offset for register config
+ * @regcmd_addr: address for register command
+ *
+ */
+struct rknpu_task {
+	__u32 flags;
+	__u32 op_idx;
+	__u32 enable_mask;
+	__u32 int_mask;
+	__u32 int_clear;
+	__u32 int_status;
+	__u32 regcfg_amount;
+	__u32 regcfg_offset;
+	__u64 regcmd_addr;
+} __packed;
+
+/**
+ * struct rknpu_subcore_task structure for subcore task index
+ *
+ * @task_start: task start index
+ * @task_number: task number
+ *
+ */
+struct rknpu_subcore_task {
+	__u32 task_start;
+	__u32 task_number;
+};
+
+/**
+ * struct rknpu_submit structure for job submit
+ *
+ * @flags: flags for job submit
+ * @timeout: submit timeout
+ * @task_start: task start index
+ * @task_number: task number
+ * @task_counter: task counter
+ * @priority: submit priority
+ * @task_obj_addr: address of task object
+ * @regcfg_obj_addr: address of register config object
+ * @task_base_addr: task base address
+ * @hw_elapse_time: hardware elapse time
+ * @core_mask: core mask of rknpu
+ * @fence_fd: dma fence fd
+ * @subcore_task: subcore task
+ *
+ */
+struct rknpu_submit {
+	__u32 flags;
+	__u32 timeout;
+	__u32 task_start;
+	__u32 task_number;
+	__u32 task_counter;
+	__s32 priority;
+	__u64 task_obj_addr;
+	__u64 regcfg_obj_addr;
+	__u64 task_base_addr;
+	__s64 hw_elapse_time;
+	__u32 core_mask;
+	__s32 fence_fd;
+	struct rknpu_subcore_task subcore_task[5];
+};
+
+/**
+ * struct rknpu_task structure for action (GET, SET or ACT)
+ *
+ * @flags: flags for action
+ * @value: GET or SET value
+ *
+ */
+struct rknpu_action {
+	__u32 flags;
+	__u32 value;
+};
+
+#define RKNPU_ACTION 0x00
+#define RKNPU_SUBMIT 0x01
+#define RKNPU_MEM_CREATE 0x02
+#define RKNPU_MEM_MAP 0x03
+#define RKNPU_MEM_DESTROY 0x04
+#define RKNPU_MEM_SYNC 0x05
+
+#define RKNPU_IOC_MAGIC 'r'
+#define RKNPU_IOW(nr, type) _IOW(RKNPU_IOC_MAGIC, nr, type)
+#define RKNPU_IOR(nr, type) _IOR(RKNPU_IOC_MAGIC, nr, type)
+#define RKNPU_IOWR(nr, type) _IOWR(RKNPU_IOC_MAGIC, nr, type)
+
+#include <drm/drm.h>
+
+#define DRM_IOCTL_RKNPU_ACTION                                                 \
+	DRM_IOWR(DRM_COMMAND_BASE + RKNPU_ACTION, struct rknpu_action)
+#define DRM_IOCTL_RKNPU_SUBMIT                                                 \
+	DRM_IOWR(DRM_COMMAND_BASE + RKNPU_SUBMIT, struct rknpu_submit)
+#define DRM_IOCTL_RKNPU_MEM_CREATE                                             \
+	DRM_IOWR(DRM_COMMAND_BASE + RKNPU_MEM_CREATE, struct rknpu_mem_create)
+#define DRM_IOCTL_RKNPU_MEM_MAP                                                \
+	DRM_IOWR(DRM_COMMAND_BASE + RKNPU_MEM_MAP, struct rknpu_mem_map)
+#define DRM_IOCTL_RKNPU_MEM_DESTROY                                            \
+	DRM_IOWR(DRM_COMMAND_BASE + RKNPU_MEM_DESTROY, struct rknpu_mem_destroy)
+#define DRM_IOCTL_RKNPU_MEM_SYNC                                               \
+	DRM_IOWR(DRM_COMMAND_BASE + RKNPU_MEM_SYNC, struct rknpu_mem_sync)
+
+#define IOCTL_RKNPU_ACTION RKNPU_IOWR(RKNPU_ACTION, struct rknpu_action)
+#define IOCTL_RKNPU_SUBMIT RKNPU_IOWR(RKNPU_SUBMIT, struct rknpu_submit)
+#define IOCTL_RKNPU_MEM_CREATE                                                 \
+	RKNPU_IOWR(RKNPU_MEM_CREATE, struct rknpu_mem_create)
+#define IOCTL_RKNPU_MEM_MAP RKNPU_IOWR(RKNPU_MEM_MAP, struct rknpu_mem_map)
+#define IOCTL_RKNPU_MEM_DESTROY                                                \
+	RKNPU_IOWR(RKNPU_MEM_DESTROY, struct rknpu_mem_destroy)
+#define IOCTL_RKNPU_MEM_SYNC RKNPU_IOWR(RKNPU_MEM_SYNC, struct rknpu_mem_sync)
+
+#endif
diff --git a/drivers/rknpu/include/rknpu_iommu.h b/drivers/rknpu/include/rknpu_iommu.h
new file mode 100644
index 000000000..aa680c997
--- /dev/null
+++ b/drivers/rknpu/include/rknpu_iommu.h
@@ -0,0 +1,40 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#ifndef __LINUX_RKNPU_IOMMU_H
+#define __LINUX_RKNPU_IOMMU_H
+
+#include <linux/mutex.h>
+#include <linux/seq_file.h>
+#include <linux/iommu.h>
+#include <linux/iova.h>
+#include <linux/version.h>
+
+#if KERNEL_VERSION(6, 1, 0) > LINUX_VERSION_CODE
+#include <linux/dma-iommu.h>
+#endif
+
+#include "rknpu_drv.h"
+
+enum iommu_dma_cookie_type {
+	IOMMU_DMA_IOVA_COOKIE,
+	IOMMU_DMA_MSI_COOKIE,
+};
+
+struct rknpu_iommu_dma_cookie {
+	enum iommu_dma_cookie_type type;
+
+	/* Full allocator for IOMMU_DMA_IOVA_COOKIE */
+	struct iova_domain iovad;
+};
+
+dma_addr_t rknpu_iommu_dma_alloc_iova(struct iommu_domain *domain, size_t size,
+				      u64 dma_limit, struct device *dev);
+
+void rknpu_iommu_dma_free_iova(struct rknpu_iommu_dma_cookie *cookie,
+			       dma_addr_t iova, size_t size);
+
+#endif
diff --git a/drivers/rknpu/include/rknpu_job.h b/drivers/rknpu/include/rknpu_job.h
new file mode 100644
index 000000000..cd0d1dfb8
--- /dev/null
+++ b/drivers/rknpu/include/rknpu_job.h
@@ -0,0 +1,80 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#ifndef __LINUX_RKNPU_JOB_H_
+#define __LINUX_RKNPU_JOB_H_
+
+#include <linux/spinlock.h>
+#include <linux/dma-fence.h>
+#include <linux/irq.h>
+
+#include <drm/drm_device.h>
+
+#include "rknpu_ioctl.h"
+
+#define RKNPU_MAX_CORES 3
+
+#define RKNPU_JOB_DONE (1 << 0)
+#define RKNPU_JOB_ASYNC (1 << 1)
+#define RKNPU_JOB_DETACHED (1 << 2)
+
+#define RKNPU_CORE_AUTO_MASK 0x00
+#define RKNPU_CORE0_MASK 0x01
+#define RKNPU_CORE1_MASK 0x02
+#define RKNPU_CORE2_MASK 0x04
+
+struct rknpu_job {
+	struct rknpu_device *rknpu_dev;
+	struct list_head head[RKNPU_MAX_CORES];
+	struct work_struct cleanup_work;
+	bool irq_entry[RKNPU_MAX_CORES];
+	unsigned int flags;
+	int ret;
+	struct rknpu_submit *args;
+	bool args_owner;
+	struct rknpu_task *first_task;
+	struct rknpu_task *last_task;
+	uint32_t int_mask[RKNPU_MAX_CORES];
+	uint32_t int_status[RKNPU_MAX_CORES];
+	struct dma_fence *fence;
+	ktime_t timestamp;
+	uint32_t use_core_num;
+	atomic_t run_count;
+	atomic_t interrupt_count;
+	ktime_t hw_commit_time;
+	ktime_t hw_recoder_time;
+	ktime_t hw_elapse_time;
+	atomic_t submit_count[RKNPU_MAX_CORES];
+};
+
+irqreturn_t rknpu_core0_irq_handler(int irq, void *data);
+irqreturn_t rknpu_core1_irq_handler(int irq, void *data);
+irqreturn_t rknpu_core2_irq_handler(int irq, void *data);
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_DRM_GEM
+int rknpu_submit_ioctl(struct drm_device *dev, void *data,
+		       struct drm_file *file_priv);
+#endif
+#ifdef CONFIG_ROCKCHIP_RKNPU_DMA_HEAP
+int rknpu_submit_ioctl(struct rknpu_device *rknpu_dev, unsigned long data);
+#endif
+
+int rknpu_get_hw_version(struct rknpu_device *rknpu_dev, uint32_t *version);
+
+int rknpu_get_bw_priority(struct rknpu_device *rknpu_dev, uint32_t *priority,
+			  uint32_t *expect, uint32_t *tw);
+
+int rknpu_set_bw_priority(struct rknpu_device *rknpu_dev, uint32_t priority,
+			  uint32_t expect, uint32_t tw);
+
+int rknpu_clear_rw_amount(struct rknpu_device *rknpu_dev);
+
+int rknpu_get_rw_amount(struct rknpu_device *rknpu_dev, uint32_t *dt_wr,
+			uint32_t *dt_rd, uint32_t *wd_rd);
+
+int rknpu_get_total_rw_amount(struct rknpu_device *rknpu_dev, uint32_t *amount);
+
+#endif /* __LINUX_RKNPU_JOB_H_ */
diff --git a/drivers/rknpu/include/rknpu_mem.h b/drivers/rknpu/include/rknpu_mem.h
new file mode 100644
index 000000000..69975408f
--- /dev/null
+++ b/drivers/rknpu/include/rknpu_mem.h
@@ -0,0 +1,46 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#ifndef __LINUX_RKNPU_MEM_H
+#define __LINUX_RKNPU_MEM_H
+
+#include <linux/mm_types.h>
+#include <linux/version.h>
+
+/*
+ * rknpu DMA buffer structure.
+ *
+ * @flags: indicate memory type to allocated buffer and cache attribute.
+ * @size: size requested from user, in bytes and this size is aligned
+ *	in page unit.
+ * @kv_addr: kernel virtual address to allocated memory region.
+ * @dma_addr: bus address(accessed by dma) to allocated memory region.
+ *	- this address could be physical address without IOMMU and
+ *	device address with IOMMU.
+ * @pages: Array of backing pages.
+ * @sgt: Imported sg_table.
+ * @dmabuf: buffer for this attachment.
+ * @owner: Is this memory internally allocated.
+ */
+struct rknpu_mem_object {
+	unsigned long flags;
+	unsigned long size;
+	void __iomem *kv_addr;
+	dma_addr_t dma_addr;
+	struct page **pages;
+	struct sg_table *sgt;
+	struct dma_buf *dmabuf;
+	struct list_head head;
+	unsigned int owner;
+};
+
+int rknpu_mem_create_ioctl(struct rknpu_device *rknpu_dev, unsigned long data,
+			   struct file *file);
+int rknpu_mem_destroy_ioctl(struct rknpu_device *rknpu_dev, unsigned long data,
+			    struct file *file);
+int rknpu_mem_sync_ioctl(struct rknpu_device *rknpu_dev, unsigned long data);
+
+#endif
diff --git a/drivers/rknpu/include/rknpu_mm.h b/drivers/rknpu/include/rknpu_mm.h
new file mode 100644
index 000000000..73ae6d7ce
--- /dev/null
+++ b/drivers/rknpu/include/rknpu_mm.h
@@ -0,0 +1,42 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#ifndef __LINUX_RKNPU_MM_H
+#define __LINUX_RKNPU_MM_H
+
+#include <linux/mutex.h>
+#include <linux/seq_file.h>
+#include <linux/iommu.h>
+#include <linux/iova.h>
+
+#include "rknpu_drv.h"
+
+struct rknpu_mm {
+	void *bitmap;
+	struct mutex lock;
+	unsigned int chunk_size;
+	unsigned int total_chunks;
+	unsigned int free_chunks;
+};
+
+struct rknpu_mm_obj {
+	uint32_t range_start;
+	uint32_t range_end;
+};
+
+int rknpu_mm_create(unsigned int mem_size, unsigned int chunk_size,
+		    struct rknpu_mm **mm);
+
+void rknpu_mm_destroy(struct rknpu_mm *mm);
+
+int rknpu_mm_alloc(struct rknpu_mm *mm, unsigned int size,
+		   struct rknpu_mm_obj **mm_obj);
+
+int rknpu_mm_free(struct rknpu_mm *mm, struct rknpu_mm_obj *mm_obj);
+
+int rknpu_mm_dump(struct seq_file *m, void *data);
+
+#endif
diff --git a/drivers/rknpu/include/rknpu_reset.h b/drivers/rknpu/include/rknpu_reset.h
new file mode 100644
index 000000000..b80e29b32
--- /dev/null
+++ b/drivers/rknpu/include/rknpu_reset.h
@@ -0,0 +1,18 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#ifndef __LINUX_RKNPU_RESET_H
+#define __LINUX_RKNPU_RESET_H
+
+#include <linux/reset.h>
+
+#include "rknpu_drv.h"
+
+int rknpu_reset_get(struct rknpu_device *rknpu_dev);
+
+int rknpu_soft_reset(struct rknpu_device *rknpu_dev);
+
+#endif
diff --git a/drivers/rknpu/rknpu_debugger.c b/drivers/rknpu/rknpu_debugger.c
new file mode 100644
index 000000000..0cfec7fe7
--- /dev/null
+++ b/drivers/rknpu/rknpu_debugger.c
@@ -0,0 +1,605 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#include <linux/slab.h>
+#include <linux/delay.h>
+#include <linux/syscalls.h>
+#include <linux/debugfs.h>
+#include <linux/proc_fs.h>
+#include <linux/devfreq.h>
+#include <linux/clk.h>
+#include <asm/div64.h>
+
+#ifndef FPGA_PLATFORM
+#ifdef CONFIG_PM_DEVFREQ
+#include <../drivers/devfreq/governor.h>
+#endif
+#endif
+
+#include "rknpu_drv.h"
+#include "rknpu_mm.h"
+#include "rknpu_reset.h"
+#include "rknpu_debugger.h"
+
+#define RKNPU_DEBUGGER_ROOT_NAME "rknpu"
+
+#if defined(CONFIG_ROCKCHIP_RKNPU_DEBUG_FS) ||                                 \
+	defined(CONFIG_ROCKCHIP_RKNPU_PROC_FS)
+static int rknpu_version_show(struct seq_file *m, void *data)
+{
+	seq_printf(m, "%s: v%d.%d.%d\n", DRIVER_DESC, DRIVER_MAJOR,
+		   DRIVER_MINOR, DRIVER_PATCHLEVEL);
+
+	return 0;
+}
+
+static int rknpu_load_show(struct seq_file *m, void *data)
+{
+	struct rknpu_debugger_node *node = m->private;
+	struct rknpu_debugger *debugger = node->debugger;
+	struct rknpu_device *rknpu_dev =
+		container_of(debugger, struct rknpu_device, debugger);
+	struct rknpu_subcore_data *subcore_data = NULL;
+	unsigned long flags;
+	int i;
+	int load;
+	uint64_t total_busy_time, div_value;
+
+	seq_puts(m, "NPU load: ");
+	for (i = 0; i < rknpu_dev->config->num_irqs; i++) {
+		subcore_data = &rknpu_dev->subcore_datas[i];
+
+		if (rknpu_dev->config->num_irqs > 1)
+			seq_printf(m, " Core%d: ", i);
+
+		spin_lock_irqsave(&rknpu_dev->irq_lock, flags);
+
+		total_busy_time = subcore_data->timer.total_busy_time;
+
+		spin_unlock_irqrestore(&rknpu_dev->irq_lock, flags);
+
+		div_value = (RKNPU_LOAD_INTERVAL / 100);
+		do_div(total_busy_time, div_value);
+		load = total_busy_time > 100 ? 100 : total_busy_time;
+
+		if (rknpu_dev->config->num_irqs > 1)
+			seq_printf(m, "%2.d%%,", load);
+		else
+			seq_printf(m, "%2.d%%", load);
+	}
+	seq_puts(m, "\n");
+
+	return 0;
+}
+
+static int rknpu_power_show(struct seq_file *m, void *data)
+{
+	struct rknpu_debugger_node *node = m->private;
+	struct rknpu_debugger *debugger = node->debugger;
+	struct rknpu_device *rknpu_dev =
+		container_of(debugger, struct rknpu_device, debugger);
+
+	if (atomic_read(&rknpu_dev->power_refcount) > 0)
+		seq_puts(m, "on\n");
+	else
+		seq_puts(m, "off\n");
+
+	return 0;
+}
+
+static ssize_t rknpu_power_set(struct file *file, const char __user *ubuf,
+			       size_t len, loff_t *offp)
+{
+	struct seq_file *priv = file->private_data;
+	struct rknpu_debugger_node *node = priv->private;
+	struct rknpu_debugger *debugger = node->debugger;
+	struct rknpu_device *rknpu_dev =
+		container_of(debugger, struct rknpu_device, debugger);
+	char buf[8];
+
+	if (len > sizeof(buf) - 1)
+		return -EINVAL;
+	if (copy_from_user(buf, ubuf, len))
+		return -EFAULT;
+	buf[len - 1] = '\0';
+
+	if (strcmp(buf, "on") == 0) {
+		atomic_inc(&rknpu_dev->cmdline_power_refcount);
+		rknpu_power_get(rknpu_dev);
+		LOG_INFO("rknpu power is on!");
+	} else if (strcmp(buf, "off") == 0) {
+		if (atomic_read(&rknpu_dev->power_refcount) > 0 &&
+		    atomic_dec_if_positive(
+			    &rknpu_dev->cmdline_power_refcount) >= 0) {
+			atomic_sub(
+				atomic_read(&rknpu_dev->cmdline_power_refcount),
+				&rknpu_dev->power_refcount);
+			atomic_set(&rknpu_dev->cmdline_power_refcount, 0);
+			rknpu_power_put(rknpu_dev);
+		}
+		if (atomic_read(&rknpu_dev->power_refcount) <= 0)
+			LOG_INFO("rknpu power is off!");
+	} else {
+		LOG_ERROR("rknpu power node params is invalid!");
+	}
+
+	return len;
+}
+
+static int rknpu_power_put_delay_show(struct seq_file *m, void *data)
+{
+	struct rknpu_debugger_node *node = m->private;
+	struct rknpu_debugger *debugger = node->debugger;
+	struct rknpu_device *rknpu_dev =
+		container_of(debugger, struct rknpu_device, debugger);
+
+	seq_printf(m, "%lu\n", rknpu_dev->power_put_delay);
+
+	return 0;
+}
+
+static ssize_t rknpu_power_put_delay_set(struct file *file,
+					 const char __user *ubuf, size_t len,
+					 loff_t *offp)
+{
+	struct seq_file *priv = file->private_data;
+	struct rknpu_debugger_node *node = priv->private;
+	struct rknpu_debugger *debugger = node->debugger;
+	struct rknpu_device *rknpu_dev =
+		container_of(debugger, struct rknpu_device, debugger);
+	char buf[16];
+	unsigned long power_put_delay = 0;
+	int ret = 0;
+
+	if (len > sizeof(buf) - 1)
+		return -EINVAL;
+	if (copy_from_user(buf, ubuf, len))
+		return -EFAULT;
+	buf[len - 1] = '\0';
+
+	ret = kstrtoul(buf, 10, &power_put_delay);
+	if (ret) {
+		LOG_ERROR("failed to parse power put delay string: %s\n", buf);
+		return -EFAULT;
+	}
+
+	rknpu_dev->power_put_delay = power_put_delay;
+
+	LOG_INFO("set rknpu power put delay time %lums\n",
+		 rknpu_dev->power_put_delay);
+
+	return len;
+}
+
+static int rknpu_freq_show(struct seq_file *m, void *data)
+{
+	struct rknpu_debugger_node *node = m->private;
+	struct rknpu_debugger *debugger = node->debugger;
+	struct rknpu_device *rknpu_dev =
+		container_of(debugger, struct rknpu_device, debugger);
+	unsigned long current_freq = 0;
+
+	rknpu_power_get(rknpu_dev);
+
+	current_freq = clk_get_rate(rknpu_dev->clks[0].clk);
+
+	rknpu_power_put(rknpu_dev);
+
+	seq_printf(m, "%lu\n", current_freq);
+
+	return 0;
+}
+
+#ifdef CONFIG_PM_DEVFREQ
+static ssize_t rknpu_freq_set(struct file *file, const char __user *ubuf,
+			      size_t len, loff_t *offp)
+{
+	struct seq_file *priv = file->private_data;
+	struct rknpu_debugger_node *node = priv->private;
+	struct rknpu_debugger *debugger = node->debugger;
+	struct rknpu_device *rknpu_dev =
+		container_of(debugger, struct rknpu_device, debugger);
+	unsigned long current_freq = 0;
+	char buf[16];
+	unsigned long freq = 0;
+	int ret = 0;
+
+	if (len > sizeof(buf) - 1)
+		return -EINVAL;
+	if (copy_from_user(buf, ubuf, len))
+		return -EFAULT;
+	buf[len - 1] = '\0';
+
+	ret = kstrtoul(buf, 10, &freq);
+	if (ret) {
+		LOG_ERROR("failed to parse freq string: %s\n", buf);
+		return -EFAULT;
+	}
+
+	if (!rknpu_dev->devfreq)
+		return -EFAULT;
+
+	rknpu_power_get(rknpu_dev);
+
+	current_freq = clk_get_rate(rknpu_dev->clks[0].clk);
+	if (freq != current_freq) {
+		rknpu_dev->ondemand_freq = freq;
+		mutex_lock(&rknpu_dev->devfreq->lock);
+		update_devfreq(rknpu_dev->devfreq);
+		mutex_unlock(&rknpu_dev->devfreq->lock);
+	}
+
+	rknpu_power_put(rknpu_dev);
+
+	return len;
+}
+#else
+static ssize_t rknpu_freq_set(struct file *file, const char __user *ubuf,
+			      size_t len, loff_t *offp)
+{
+	return -EFAULT;
+}
+#endif
+
+static int rknpu_volt_show(struct seq_file *m, void *data)
+{
+	struct rknpu_debugger_node *node = m->private;
+	struct rknpu_debugger *debugger = node->debugger;
+	struct rknpu_device *rknpu_dev =
+		container_of(debugger, struct rknpu_device, debugger);
+	unsigned long current_volt = 0;
+
+	current_volt = regulator_get_voltage(rknpu_dev->vdd);
+
+	seq_printf(m, "%lu\n", current_volt);
+
+	return 0;
+}
+
+static int rknpu_reset_show(struct seq_file *m, void *data)
+{
+	struct rknpu_debugger_node *node = m->private;
+	struct rknpu_debugger *debugger = node->debugger;
+	struct rknpu_device *rknpu_dev =
+		container_of(debugger, struct rknpu_device, debugger);
+
+	if (!rknpu_dev->bypass_soft_reset)
+		seq_puts(m, "on\n");
+	else
+		seq_puts(m, "off\n");
+
+	return 0;
+}
+
+static ssize_t rknpu_reset_set(struct file *file, const char __user *ubuf,
+			       size_t len, loff_t *offp)
+{
+	struct seq_file *priv = file->private_data;
+	struct rknpu_debugger_node *node = priv->private;
+	struct rknpu_debugger *debugger = node->debugger;
+	struct rknpu_device *rknpu_dev =
+		container_of(debugger, struct rknpu_device, debugger);
+	char buf[8];
+
+	if (len > sizeof(buf) - 1)
+		return -EINVAL;
+	if (copy_from_user(buf, ubuf, len))
+		return -EFAULT;
+	buf[len - 1] = '\0';
+
+	if (strcmp(buf, "1") == 0 &&
+	    atomic_read(&rknpu_dev->power_refcount) > 0)
+		rknpu_soft_reset(rknpu_dev);
+	else if (strcmp(buf, "on") == 0)
+		rknpu_dev->bypass_soft_reset = 0;
+	else if (strcmp(buf, "off") == 0)
+		rknpu_dev->bypass_soft_reset = 1;
+
+	return len;
+}
+
+static struct rknpu_debugger_list rknpu_debugger_root_list[] = {
+	{ "version", rknpu_version_show, NULL, NULL },
+	{ "load", rknpu_load_show, NULL, NULL },
+	{ "power", rknpu_power_show, rknpu_power_set, NULL },
+	{ "freq", rknpu_freq_show, rknpu_freq_set, NULL },
+	{ "volt", rknpu_volt_show, NULL, NULL },
+	{ "delayms", rknpu_power_put_delay_show, rknpu_power_put_delay_set,
+	  NULL },
+	{ "reset", rknpu_reset_show, rknpu_reset_set, NULL },
+#ifdef CONFIG_ROCKCHIP_RKNPU_SRAM
+	{ "mm", rknpu_mm_dump, NULL, NULL },
+#endif
+};
+
+static ssize_t rknpu_debugger_write(struct file *file, const char __user *ubuf,
+				    size_t len, loff_t *offp)
+{
+	struct seq_file *priv = file->private_data;
+	struct rknpu_debugger_node *node = priv->private;
+
+	if (node->info_ent->write)
+		return node->info_ent->write(file, ubuf, len, offp);
+	else
+		return len;
+}
+
+static int rknpu_debugfs_open(struct inode *inode, struct file *file)
+{
+	struct rknpu_debugger_node *node = inode->i_private;
+
+	return single_open(file, node->info_ent->show, node);
+}
+
+static const struct file_operations rknpu_debugfs_fops = {
+	.owner = THIS_MODULE,
+	.open = rknpu_debugfs_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+	.write = rknpu_debugger_write,
+};
+#endif /* #if defined(CONFIG_ROCKCHIP_RKNPU_DEBUG_FS) || defined(CONFIG_ROCKCHIP_RKNPU_PROC_FS) */
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_DEBUG_FS
+static int rknpu_debugfs_remove_files(struct rknpu_debugger *debugger)
+{
+	struct rknpu_debugger_node *pos, *q;
+	struct list_head *entry_list;
+
+	mutex_lock(&debugger->debugfs_lock);
+
+	/* Delete debugfs entry list */
+	entry_list = &debugger->debugfs_entry_list;
+	list_for_each_entry_safe(pos, q, entry_list, list) {
+		if (pos->dent == NULL)
+			continue;
+		list_del(&pos->list);
+		kfree(pos);
+		pos = NULL;
+	}
+
+	/* Delete all debugfs node in this directory */
+	debugfs_remove_recursive(debugger->debugfs_dir);
+	debugger->debugfs_dir = NULL;
+
+	mutex_unlock(&debugger->debugfs_lock);
+
+	return 0;
+}
+
+static int rknpu_debugfs_create_files(const struct rknpu_debugger_list *files,
+				      int count, struct dentry *root,
+				      struct rknpu_debugger *debugger)
+{
+	int i;
+	struct dentry *ent;
+	struct rknpu_debugger_node *tmp;
+
+	for (i = 0; i < count; i++) {
+		tmp = kmalloc(sizeof(struct rknpu_debugger_node), GFP_KERNEL);
+		if (tmp == NULL) {
+			LOG_ERROR(
+				"Cannot alloc node path /sys/kernel/debug/%pd/%s\n",
+				root, files[i].name);
+			goto MALLOC_FAIL;
+		}
+
+		tmp->info_ent = &files[i];
+		tmp->debugger = debugger;
+
+		ent = debugfs_create_file(files[i].name, S_IFREG | S_IRUGO,
+					  root, tmp, &rknpu_debugfs_fops);
+		if (!ent) {
+			LOG_ERROR("Cannot create /sys/kernel/debug/%pd/%s\n",
+				  root, files[i].name);
+			goto CREATE_FAIL;
+		}
+
+		tmp->dent = ent;
+
+		mutex_lock(&debugger->debugfs_lock);
+		list_add_tail(&tmp->list, &debugger->debugfs_entry_list);
+		mutex_unlock(&debugger->debugfs_lock);
+	}
+
+	return 0;
+
+CREATE_FAIL:
+	kfree(tmp);
+MALLOC_FAIL:
+	rknpu_debugfs_remove_files(debugger);
+
+	return -1;
+}
+
+static int rknpu_debugfs_remove(struct rknpu_debugger *debugger)
+{
+	rknpu_debugfs_remove_files(debugger);
+
+	return 0;
+}
+
+static int rknpu_debugfs_init(struct rknpu_debugger *debugger)
+{
+	int ret;
+
+	debugger->debugfs_dir =
+		debugfs_create_dir(RKNPU_DEBUGGER_ROOT_NAME, NULL);
+	if (IS_ERR_OR_NULL(debugger->debugfs_dir)) {
+		LOG_ERROR("failed on mkdir /sys/kernel/debug/%s\n",
+			  RKNPU_DEBUGGER_ROOT_NAME);
+		debugger->debugfs_dir = NULL;
+		return -EIO;
+	}
+
+	ret = rknpu_debugfs_create_files(rknpu_debugger_root_list,
+					 ARRAY_SIZE(rknpu_debugger_root_list),
+					 debugger->debugfs_dir, debugger);
+	if (ret) {
+		LOG_ERROR(
+			"Could not install rknpu_debugger_root_list debugfs\n");
+		goto CREATE_FAIL;
+	}
+
+	return 0;
+
+CREATE_FAIL:
+	rknpu_debugfs_remove(debugger);
+
+	return ret;
+}
+#endif /* #ifdef CONFIG_ROCKCHIP_RKNPU_DEBUG_FS */
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_PROC_FS
+static int rknpu_procfs_open(struct inode *inode, struct file *file)
+{
+#if KERNEL_VERSION(6, 1, 0) > LINUX_VERSION_CODE
+	struct rknpu_debugger_node *node = PDE_DATA(inode);
+#else
+	struct rknpu_debugger_node *node = pde_data(inode);
+#endif
+
+	return single_open(file, node->info_ent->show, node);
+}
+
+static const struct proc_ops rknpu_procfs_fops = {
+	.proc_open = rknpu_procfs_open,
+	.proc_read = seq_read,
+	.proc_lseek = seq_lseek,
+	.proc_release = single_release,
+	.proc_write = rknpu_debugger_write,
+};
+
+static int rknpu_procfs_remove_files(struct rknpu_debugger *debugger)
+{
+	struct rknpu_debugger_node *pos, *q;
+	struct list_head *entry_list;
+
+	mutex_lock(&debugger->procfs_lock);
+
+	/* Delete procfs entry list */
+	entry_list = &debugger->procfs_entry_list;
+	list_for_each_entry_safe(pos, q, entry_list, list) {
+		if (pos->pent == NULL)
+			continue;
+		list_del(&pos->list);
+		kfree(pos);
+		pos = NULL;
+	}
+
+	/* Delete all procfs node in this directory */
+	proc_remove(debugger->procfs_dir);
+	debugger->procfs_dir = NULL;
+
+	mutex_unlock(&debugger->procfs_lock);
+
+	return 0;
+}
+
+static int rknpu_procfs_create_files(const struct rknpu_debugger_list *files,
+				     int count, struct proc_dir_entry *root,
+				     struct rknpu_debugger *debugger)
+{
+	int i;
+	struct proc_dir_entry *ent;
+	struct rknpu_debugger_node *tmp;
+
+	for (i = 0; i < count; i++) {
+		tmp = kmalloc(sizeof(struct rknpu_debugger_node), GFP_KERNEL);
+		if (tmp == NULL) {
+			LOG_ERROR("Cannot alloc node path for /proc/%s/%s\n",
+				  RKNPU_DEBUGGER_ROOT_NAME, files[i].name);
+			goto MALLOC_FAIL;
+		}
+
+		tmp->info_ent = &files[i];
+		tmp->debugger = debugger;
+
+		ent = proc_create_data(files[i].name, S_IFREG | S_IRUGO, root,
+				       &rknpu_procfs_fops, tmp);
+		if (!ent) {
+			LOG_ERROR("Cannot create /proc/%s/%s\n",
+				  RKNPU_DEBUGGER_ROOT_NAME, files[i].name);
+			goto CREATE_FAIL;
+		}
+
+		tmp->pent = ent;
+
+		mutex_lock(&debugger->procfs_lock);
+		list_add_tail(&tmp->list, &debugger->procfs_entry_list);
+		mutex_unlock(&debugger->procfs_lock);
+	}
+
+	return 0;
+
+CREATE_FAIL:
+	kfree(tmp);
+MALLOC_FAIL:
+	rknpu_procfs_remove_files(debugger);
+	return -1;
+}
+
+static int rknpu_procfs_remove(struct rknpu_debugger *debugger)
+{
+	rknpu_procfs_remove_files(debugger);
+
+	return 0;
+}
+
+static int rknpu_procfs_init(struct rknpu_debugger *debugger)
+{
+	int ret;
+
+	debugger->procfs_dir = proc_mkdir(RKNPU_DEBUGGER_ROOT_NAME, NULL);
+	if (IS_ERR_OR_NULL(debugger->procfs_dir)) {
+		pr_err("failed on mkdir /proc/%s\n", RKNPU_DEBUGGER_ROOT_NAME);
+		debugger->procfs_dir = NULL;
+		return -EIO;
+	}
+
+	ret = rknpu_procfs_create_files(rknpu_debugger_root_list,
+					ARRAY_SIZE(rknpu_debugger_root_list),
+					debugger->procfs_dir, debugger);
+	if (ret) {
+		pr_err("Could not install rknpu_debugger_root_list procfs\n");
+		goto CREATE_FAIL;
+	}
+
+	return 0;
+
+CREATE_FAIL:
+	rknpu_procfs_remove(debugger);
+
+	return ret;
+}
+#endif /* #ifdef CONFIG_ROCKCHIP_RKNPU_PROC_FS */
+
+int rknpu_debugger_init(struct rknpu_device *rknpu_dev)
+{
+#ifdef CONFIG_ROCKCHIP_RKNPU_DEBUG_FS
+	mutex_init(&rknpu_dev->debugger.debugfs_lock);
+	INIT_LIST_HEAD(&rknpu_dev->debugger.debugfs_entry_list);
+	rknpu_debugfs_init(&rknpu_dev->debugger);
+#endif
+#ifdef CONFIG_ROCKCHIP_RKNPU_PROC_FS
+	mutex_init(&rknpu_dev->debugger.procfs_lock);
+	INIT_LIST_HEAD(&rknpu_dev->debugger.procfs_entry_list);
+	rknpu_procfs_init(&rknpu_dev->debugger);
+#endif
+	return 0;
+}
+
+int rknpu_debugger_remove(struct rknpu_device *rknpu_dev)
+{
+#ifdef CONFIG_ROCKCHIP_RKNPU_DEBUG_FS
+	rknpu_debugfs_remove(&rknpu_dev->debugger);
+#endif
+#ifdef CONFIG_ROCKCHIP_RKNPU_PROC_FS
+	rknpu_procfs_remove(&rknpu_dev->debugger);
+#endif
+	return 0;
+}
diff --git a/drivers/rknpu/rknpu_devfreq.c b/drivers/rknpu/rknpu_devfreq.c
new file mode 100644
index 000000000..9cda13056
--- /dev/null
+++ b/drivers/rknpu/rknpu_devfreq.c
@@ -0,0 +1,766 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Finley Xiao <finley.xiao@rock-chips.com>
+ */
+
+#include <linux/clk.h>
+#include <linux/clk-provider.h>
+#include <linux/devfreq_cooling.h>
+#include <linux/pm_runtime.h>
+#include <linux/regmap.h>
+#include <../drivers/devfreq/governor.h>
+#include "rknpu_drv.h"
+#include "rknpu_devfreq.h"
+#include <soc/rockchip/rockchip_system_monitor.h>
+
+#define POWER_DOWN_FREQ 200000000
+
+static int npu_devfreq_target(struct device *dev, unsigned long *freq,
+			      u32 flags);
+
+static struct monitor_dev_profile npu_mdevp = {
+	.type = MONITOR_TYPE_DEV,
+	.low_temp_adjust = rockchip_monitor_dev_low_temp_adjust,
+	.high_temp_adjust = rockchip_monitor_dev_high_temp_adjust,
+#if KERNEL_VERSION(6, 1, 0) <= LINUX_VERSION_CODE
+	.check_rate_volt = rockchip_monitor_check_rate_volt,
+#else
+	.update_volt = rockchip_monitor_check_rate_volt,
+#endif
+};
+
+static int npu_devfreq_get_dev_status(struct device *dev,
+				      struct devfreq_dev_status *stat)
+{
+	return 0;
+}
+
+static int npu_devfreq_get_cur_freq(struct device *dev, unsigned long *freq)
+{
+	struct rknpu_device *rknpu_dev = dev_get_drvdata(dev);
+
+	*freq = rknpu_dev->current_freq;
+
+	return 0;
+}
+
+static struct devfreq_dev_profile npu_devfreq_profile = {
+	.polling_ms = 50,
+	.target = npu_devfreq_target,
+	.get_dev_status = npu_devfreq_get_dev_status,
+	.get_cur_freq = npu_devfreq_get_cur_freq,
+};
+
+static int devfreq_rknpu_ondemand_func(struct devfreq *df, unsigned long *freq)
+{
+	struct rknpu_device *rknpu_dev = df->data;
+
+	if (rknpu_dev && rknpu_dev->ondemand_freq)
+		*freq = rknpu_dev->ondemand_freq;
+	else
+		*freq = df->previous_freq;
+
+	return 0;
+}
+
+static int devfreq_rknpu_ondemand_handler(struct devfreq *devfreq,
+					  unsigned int event, void *data)
+{
+	return 0;
+}
+
+static struct devfreq_governor devfreq_rknpu_ondemand = {
+	.name = "rknpu_ondemand",
+	.get_target_freq = devfreq_rknpu_ondemand_func,
+	.event_handler = devfreq_rknpu_ondemand_handler,
+};
+
+static int rk3588_npu_get_soc_info(struct device *dev, struct device_node *np,
+				   int *bin, int *process)
+{
+	int ret = 0;
+	u8 value = 0;
+
+	if (!bin)
+		return 0;
+
+	if (of_property_match_string(np, "nvmem-cell-names",
+				     "specification_serial_number") >= 0) {
+		ret = rockchip_nvmem_cell_read_u8(
+			np, "specification_serial_number", &value);
+		if (ret) {
+			LOG_DEV_ERROR(
+				dev,
+				"Failed to get specification_serial_number\n");
+			return ret;
+		}
+		/* RK3588M */
+		if (value == 0xd)
+			*bin = 1;
+		/* RK3588J */
+		else if (value == 0xa)
+			*bin = 2;
+	}
+	if (*bin < 0)
+		*bin = 0;
+	LOG_DEV_INFO(dev, "bin=%d\n", *bin);
+
+	return ret;
+}
+
+#if KERNEL_VERSION(6, 1, 0) <= LINUX_VERSION_CODE
+static int rk3588_npu_set_soc_info(struct device *dev, struct device_node *np,
+				   struct rockchip_opp_info *opp_info)
+{
+	int bin = opp_info->bin;
+
+	if (opp_info->volt_sel < 0)
+		return 0;
+	if (bin < 0)
+		bin = 0;
+
+	if (!of_property_read_bool(np, "rockchip,supported-hw"))
+		return 0;
+
+	/* SoC Version */
+	opp_info->supported_hw[0] = BIT(bin);
+	/* Speed Grade */
+	opp_info->supported_hw[1] = BIT(opp_info->volt_sel);
+
+	return 0;
+}
+#else
+static int rk3588_npu_set_soc_info(struct device *dev, struct device_node *np,
+				   int bin, int process, int volt_sel)
+{
+	struct opp_table *opp_table;
+	u32 supported_hw[2];
+
+	if (volt_sel < 0)
+		return 0;
+	if (bin < 0)
+		bin = 0;
+
+	if (!of_property_read_bool(np, "rockchip,supported-hw"))
+		return 0;
+
+	/* SoC Version */
+	supported_hw[0] = BIT(bin);
+	/* Speed Grade */
+	supported_hw[1] = BIT(volt_sel);
+	opp_table = dev_pm_opp_set_supported_hw(dev, supported_hw, 2);
+	if (IS_ERR(opp_table)) {
+		LOG_DEV_ERROR(dev, "failed to set supported opp\n");
+		return PTR_ERR(opp_table);
+	}
+
+	return 0;
+}
+#endif
+
+static int rk3588_npu_set_read_margin(struct device *dev,
+				      struct rockchip_opp_info *opp_info,
+				      u32 rm)
+{
+	struct rknpu_device *rknpu_dev = dev_get_drvdata(dev);
+	u32 offset = 0, val = 0;
+	int i, ret = 0;
+
+	if (!opp_info->grf || !opp_info->volt_rm_tbl)
+		return 0;
+
+	if (rm == opp_info->current_rm || rm == UINT_MAX)
+		return 0;
+
+	LOG_DEV_DEBUG(dev, "set rm to %d\n", rm);
+
+	for (i = 0; i < rknpu_dev->config->num_irqs; i++) {
+		ret = regmap_read(opp_info->grf, offset, &val);
+		if (ret < 0) {
+			LOG_DEV_ERROR(dev, "failed to get rm from 0x%x\n",
+				      offset);
+			return ret;
+		}
+		val &= ~0x1c;
+		regmap_write(opp_info->grf, offset, val | (rm << 2));
+		offset += 4;
+	}
+	opp_info->current_rm = rm;
+
+	return 0;
+}
+
+#if KERNEL_VERSION(6, 1, 0) <= LINUX_VERSION_CODE
+static int npu_opp_config_regulators(struct device *dev,
+				     struct dev_pm_opp *old_opp,
+				     struct dev_pm_opp *new_opp,
+				     struct regulator **regulators,
+				     unsigned int count)
+{
+	struct rknpu_device *rknpu_dev = dev_get_drvdata(dev);
+
+	return rockchip_opp_config_regulators(dev, old_opp, new_opp, regulators,
+					      count, &rknpu_dev->opp_info);
+}
+
+static int npu_opp_config_clks(struct device *dev, struct opp_table *opp_table,
+			       struct dev_pm_opp *opp, void *data,
+			       bool scaling_down)
+{
+	struct rknpu_device *rknpu_dev = dev_get_drvdata(dev);
+
+	return rockchip_opp_config_clks(dev, opp_table, opp, data, scaling_down,
+					&rknpu_dev->opp_info);
+}
+#endif
+
+static const struct rockchip_opp_data rk3588_npu_opp_data = {
+	.get_soc_info = rk3588_npu_get_soc_info,
+	.set_soc_info = rk3588_npu_set_soc_info,
+	.set_read_margin = rk3588_npu_set_read_margin,
+#if KERNEL_VERSION(6, 1, 0) <= LINUX_VERSION_CODE
+	.config_regulators = npu_opp_config_regulators,
+	.config_clks = npu_opp_config_clks,
+#endif
+};
+
+static const struct of_device_id rockchip_npu_of_match[] = {
+	{
+		.compatible = "rockchip,rk3588",
+		.data = (void *)&rk3588_npu_opp_data,
+	},
+	{},
+};
+
+#if KERNEL_VERSION(6, 1, 0) <= LINUX_VERSION_CODE
+void rknpu_devfreq_lock(struct rknpu_device *rknpu_dev)
+{
+	if (rknpu_dev->devfreq)
+		rockchip_opp_dvfs_lock(&rknpu_dev->opp_info);
+}
+EXPORT_SYMBOL(rknpu_devfreq_lock);
+
+void rknpu_devfreq_unlock(struct rknpu_device *rknpu_dev)
+{
+	if (rknpu_dev->devfreq)
+		rockchip_opp_dvfs_unlock(&rknpu_dev->opp_info);
+}
+EXPORT_SYMBOL(rknpu_devfreq_unlock);
+
+static int npu_devfreq_target(struct device *dev, unsigned long *freq,
+			      u32 flags)
+{
+	struct rknpu_device *rknpu_dev = dev_get_drvdata(dev);
+	struct rockchip_opp_info *opp_info = &rknpu_dev->opp_info;
+	struct dev_pm_opp *opp;
+	unsigned long opp_volt;
+	int ret = 0;
+
+	if (!opp_info->is_rate_volt_checked)
+		return -EINVAL;
+
+	opp = devfreq_recommended_opp(dev, freq, flags);
+	if (IS_ERR(opp))
+		return PTR_ERR(opp);
+	opp_volt = dev_pm_opp_get_voltage(opp);
+	dev_pm_opp_put(opp);
+
+	if (*freq == rknpu_dev->current_freq)
+		return 0;
+
+	rockchip_opp_dvfs_lock(opp_info);
+	if (pm_runtime_active(dev))
+		opp_info->is_runtime_active = true;
+	else
+		opp_info->is_runtime_active = false;
+	ret = dev_pm_opp_set_rate(dev, *freq);
+	if (!ret) {
+		rknpu_dev->current_freq = *freq;
+		if (rknpu_dev->devfreq)
+			rknpu_dev->devfreq->last_status.current_frequency =
+				*freq;
+		rknpu_dev->current_volt = opp_volt;
+		LOG_DEV_DEBUG(dev, "set rknpu freq: %lu, volt: %lu\n",
+			      rknpu_dev->current_freq, rknpu_dev->current_volt);
+	}
+	rockchip_opp_dvfs_unlock(opp_info);
+
+	return ret;
+}
+
+static const struct rockchip_opp_data rockchip_npu_opp_data = {
+	.config_clks = npu_opp_config_clks,
+};
+
+int rknpu_devfreq_init(struct rknpu_device *rknpu_dev)
+{
+	struct rockchip_opp_info *info = &rknpu_dev->opp_info;
+	struct device *dev = rknpu_dev->dev;
+	struct devfreq_dev_profile *dp;
+	struct dev_pm_opp *opp;
+	unsigned int dyn_power_coeff = 0;
+	int ret = 0;
+
+	info->data = &rockchip_npu_opp_data;
+	rockchip_get_opp_data(rockchip_npu_of_match, info);
+	ret = rockchip_init_opp_table(dev, info, "clk_npu", "rknpu");
+	if (ret) {
+		LOG_DEV_ERROR(dev, "failed to init_opp_table\n");
+		return -EINVAL;
+	}
+
+	rknpu_dev->current_freq = clk_get_rate(rknpu_dev->clks[0].clk);
+	opp = devfreq_recommended_opp(dev, &rknpu_dev->current_freq, 0);
+	if (IS_ERR(opp)) {
+		ret = PTR_ERR(opp);
+		goto err_uinit_table;
+	}
+	dev_pm_opp_put(opp);
+
+	dp = &npu_devfreq_profile;
+	dp->initial_freq = rknpu_dev->current_freq;
+	of_property_read_u32(dev->of_node, "dynamic-power-coefficient",
+			     &dyn_power_coeff);
+	if (dyn_power_coeff)
+		dp->is_cooling_device = true;
+
+	ret = devfreq_add_governor(&devfreq_rknpu_ondemand);
+	if (ret) {
+		LOG_DEV_ERROR(dev, "failed to add rknpu_ondemand governor\n");
+		goto err_uinit_table;
+	}
+
+	rknpu_dev->devfreq = devm_devfreq_add_device(dev, dp, "rknpu_ondemand",
+						     (void *)rknpu_dev);
+	if (IS_ERR(rknpu_dev->devfreq)) {
+		LOG_DEV_ERROR(dev, "failed to add devfreq\n");
+		ret = PTR_ERR(rknpu_dev->devfreq);
+		rknpu_dev->devfreq = NULL;
+		goto err_remove_governor;
+	}
+
+	npu_mdevp.data = rknpu_dev->devfreq;
+	npu_mdevp.opp_info = &rknpu_dev->opp_info;
+	rknpu_dev->mdev_info =
+		rockchip_system_monitor_register(dev, &npu_mdevp);
+	if (IS_ERR(rknpu_dev->mdev_info)) {
+		dev_dbg(dev, "without system monitor\n");
+		rknpu_dev->mdev_info = NULL;
+	}
+
+	rknpu_dev->current_freq = clk_get_rate(rknpu_dev->clks[0].clk);
+	rknpu_dev->ondemand_freq = rknpu_dev->current_freq;
+	rknpu_dev->current_volt = regulator_get_voltage(rknpu_dev->vdd);
+
+	rknpu_dev->devfreq->previous_freq = rknpu_dev->current_freq;
+	if (rknpu_dev->devfreq->suspend_freq)
+		rknpu_dev->devfreq->resume_freq = rknpu_dev->current_freq;
+	rknpu_dev->devfreq->last_status.current_frequency =
+		rknpu_dev->current_freq;
+	rknpu_dev->devfreq->last_status.total_time = 1;
+	rknpu_dev->devfreq->last_status.busy_time = 1;
+
+	return 0;
+
+err_remove_governor:
+	devfreq_remove_governor(&devfreq_rknpu_ondemand);
+err_uinit_table:
+	rockchip_uninit_opp_table(dev, info);
+
+	return ret;
+}
+EXPORT_SYMBOL(rknpu_devfreq_init);
+
+int rknpu_devfreq_runtime_suspend(struct device *dev)
+{
+	struct rknpu_device *rknpu_dev = dev_get_drvdata(dev);
+	struct rockchip_opp_info *opp_info = &rknpu_dev->opp_info;
+
+	if (opp_info->is_scmi_clk) {
+		if (clk_set_rate(opp_info->clk, POWER_DOWN_FREQ))
+			LOG_DEV_ERROR(dev, "failed to restore clk rate\n");
+	}
+	opp_info->current_rm = UINT_MAX;
+
+	return 0;
+}
+EXPORT_SYMBOL(rknpu_devfreq_runtime_suspend);
+
+int rknpu_devfreq_runtime_resume(struct device *dev)
+{
+	struct rknpu_device *rknpu_dev = dev_get_drvdata(dev);
+	struct rockchip_opp_info *opp_info = &rknpu_dev->opp_info;
+	int ret = 0;
+
+	if (!rknpu_dev->current_freq || !rknpu_dev->current_volt)
+		return 0;
+
+	ret = clk_bulk_prepare_enable(opp_info->nclocks, opp_info->clocks);
+	if (ret) {
+		LOG_DEV_INFO(dev, "failed to enable opp clks\n");
+		return ret;
+	}
+
+	if (opp_info->data && opp_info->data->set_read_margin)
+		opp_info->data->set_read_margin(dev, opp_info,
+						opp_info->target_rm);
+	if (opp_info->is_scmi_clk) {
+		if (clk_set_rate(opp_info->clk, rknpu_dev->current_freq))
+			LOG_DEV_ERROR(dev, "failed to set power down rate\n");
+	}
+
+	clk_bulk_disable_unprepare(opp_info->nclocks, opp_info->clocks);
+
+	return ret;
+}
+EXPORT_SYMBOL(rknpu_devfreq_runtime_resume);
+#else
+void rknpu_devfreq_lock(struct rknpu_device *rknpu_dev)
+{
+	rockchip_monitor_volt_adjust_lock(rknpu_dev->mdev_info);
+}
+EXPORT_SYMBOL(rknpu_devfreq_lock);
+
+void rknpu_devfreq_unlock(struct rknpu_device *rknpu_dev)
+{
+	rockchip_monitor_volt_adjust_unlock(rknpu_dev->mdev_info);
+}
+EXPORT_SYMBOL(rknpu_devfreq_unlock);
+
+static int npu_opp_helper(struct dev_pm_set_opp_data *data)
+{
+	struct device *dev = data->dev;
+	struct dev_pm_opp_supply *old_supply_vdd = &data->old_opp.supplies[0];
+	struct dev_pm_opp_supply *old_supply_mem = &data->old_opp.supplies[1];
+	struct dev_pm_opp_supply *new_supply_vdd = &data->new_opp.supplies[0];
+	struct dev_pm_opp_supply *new_supply_mem = &data->new_opp.supplies[1];
+	struct regulator *vdd_reg = data->regulators[0];
+	struct regulator *mem_reg = data->regulators[1];
+	struct clk *clk = data->clk;
+	struct rknpu_device *rknpu_dev = dev_get_drvdata(dev);
+	struct rockchip_opp_info *opp_info = &rknpu_dev->opp_info;
+	unsigned long old_freq = data->old_opp.rate;
+	unsigned long new_freq = data->new_opp.rate;
+	bool is_set_rm = true;
+	bool is_set_clk = true;
+	u32 target_rm = UINT_MAX;
+	int ret = 0;
+
+	if (!pm_runtime_active(dev)) {
+		is_set_rm = false;
+		if (opp_info->scmi_clk)
+			is_set_clk = false;
+	}
+
+	ret = clk_bulk_prepare_enable(opp_info->num_clks, opp_info->clks);
+	if (ret < 0) {
+		LOG_DEV_ERROR(dev, "failed to enable opp clks\n");
+		return ret;
+	}
+	rockchip_get_read_margin(dev, opp_info, new_supply_vdd->u_volt,
+				 &target_rm);
+
+	/* Change frequency */
+	LOG_DEV_DEBUG(dev, "switching OPP: %lu Hz --> %lu Hz\n", old_freq,
+		      new_freq);
+	/* Scaling up? Scale voltage before frequency */
+	if (new_freq >= old_freq) {
+		rockchip_set_intermediate_rate(dev, opp_info, clk, old_freq,
+					       new_freq, true, is_set_clk);
+		ret = regulator_set_voltage(mem_reg, new_supply_mem->u_volt,
+					    INT_MAX);
+		if (ret) {
+			LOG_DEV_ERROR(dev,
+				      "failed to set volt %lu uV for mem reg\n",
+				      new_supply_mem->u_volt);
+			goto restore_voltage;
+		}
+		ret = regulator_set_voltage(vdd_reg, new_supply_vdd->u_volt,
+					    INT_MAX);
+		if (ret) {
+			LOG_DEV_ERROR(dev,
+				      "failed to set volt %lu uV for vdd reg\n",
+				      new_supply_vdd->u_volt);
+			goto restore_voltage;
+		}
+		rockchip_set_read_margin(dev, opp_info, target_rm, is_set_rm);
+		if (is_set_clk && clk_set_rate(clk, new_freq)) {
+			ret = -EINVAL;
+			LOG_DEV_ERROR(dev, "failed to set clk rate: %d\n", ret);
+			goto restore_rm;
+		}
+		/* Scaling down? Scale voltage after frequency */
+	} else {
+		rockchip_set_intermediate_rate(dev, opp_info, clk, old_freq,
+					       new_freq, false, is_set_clk);
+		rockchip_set_read_margin(dev, opp_info, target_rm, is_set_rm);
+		if (is_set_clk && clk_set_rate(clk, new_freq)) {
+			ret = -EINVAL;
+			LOG_DEV_ERROR(dev, "failed to set clk rate: %d\n", ret);
+			goto restore_rm;
+		}
+		ret = regulator_set_voltage(vdd_reg, new_supply_vdd->u_volt,
+					    INT_MAX);
+		if (ret) {
+			LOG_DEV_ERROR(dev,
+				      "failed to set volt %lu uV for vdd reg\n",
+				      new_supply_vdd->u_volt);
+			goto restore_freq;
+		}
+		ret = regulator_set_voltage(mem_reg, new_supply_mem->u_volt,
+					    INT_MAX);
+		if (ret) {
+			LOG_DEV_ERROR(dev,
+				      "failed to set volt %lu uV for mem reg\n",
+				      new_supply_mem->u_volt);
+			goto restore_freq;
+		}
+	}
+
+	clk_bulk_disable_unprepare(opp_info->num_clks, opp_info->clks);
+
+	return 0;
+
+restore_freq:
+	if (is_set_clk && clk_set_rate(clk, old_freq))
+		LOG_DEV_ERROR(dev, "failed to restore old-freq %lu Hz\n",
+			      old_freq);
+restore_rm:
+	rockchip_get_read_margin(dev, opp_info, old_supply_vdd->u_volt,
+				 &target_rm);
+	rockchip_set_read_margin(dev, opp_info, opp_info->current_rm,
+				 is_set_rm);
+restore_voltage:
+	regulator_set_voltage(mem_reg, old_supply_mem->u_volt, INT_MAX);
+	regulator_set_voltage(vdd_reg, old_supply_vdd->u_volt, INT_MAX);
+	clk_bulk_disable_unprepare(opp_info->num_clks, opp_info->clks);
+
+	return ret;
+}
+
+static int npu_devfreq_target(struct device *dev, unsigned long *freq,
+			      u32 flags)
+{
+	struct rknpu_device *rknpu_dev = dev_get_drvdata(dev);
+	struct dev_pm_opp *opp;
+	unsigned long opp_volt;
+	int ret = 0;
+
+	if (!npu_mdevp.is_checked)
+		return -EINVAL;
+
+	opp = devfreq_recommended_opp(dev, freq, flags);
+	if (IS_ERR(opp))
+		return PTR_ERR(opp);
+	opp_volt = dev_pm_opp_get_voltage(opp);
+	dev_pm_opp_put(opp);
+
+	rockchip_monitor_volt_adjust_lock(rknpu_dev->mdev_info);
+
+	ret = dev_pm_opp_set_rate(dev, *freq);
+	if (!ret) {
+		rknpu_dev->current_freq = *freq;
+		if (rknpu_dev->devfreq)
+			rknpu_dev->devfreq->last_status.current_frequency =
+				*freq;
+		rknpu_dev->current_volt = opp_volt;
+		LOG_DEV_DEBUG(dev, "set rknpu freq: %lu, volt: %lu\n",
+			      rknpu_dev->current_freq, rknpu_dev->current_volt);
+	}
+
+	rockchip_monitor_volt_adjust_unlock(rknpu_dev->mdev_info);
+
+	return ret;
+}
+
+static unsigned long npu_get_static_power(struct devfreq *devfreq,
+					  unsigned long voltage)
+{
+	struct device *dev = devfreq->dev.parent;
+	struct rknpu_device *rknpu_dev = dev_get_drvdata(dev);
+
+	if (!rknpu_dev->model_data)
+		return 0;
+
+	return rockchip_ipa_get_static_power(rknpu_dev->model_data, voltage);
+}
+
+static struct devfreq_cooling_power npu_cooling_power = {
+	.get_static_power = &npu_get_static_power,
+};
+
+int rknpu_devfreq_init(struct rknpu_device *rknpu_dev)
+{
+	struct device *dev = rknpu_dev->dev;
+	struct devfreq_dev_profile *dp = &npu_devfreq_profile;
+	struct dev_pm_opp *opp;
+	struct opp_table *reg_table = NULL;
+	struct opp_table *opp_table = NULL;
+	const char *const reg_names[] = { "rknpu", "mem" };
+	int ret = -EINVAL;
+
+	if (strstr(__clk_get_name(rknpu_dev->clks[0].clk), "scmi"))
+		rknpu_dev->opp_info.scmi_clk = rknpu_dev->clks[0].clk;
+
+	if (of_find_property(dev->of_node, "rknpu-supply", NULL) &&
+	    of_find_property(dev->of_node, "mem-supply", NULL)) {
+		reg_table = dev_pm_opp_set_regulators(dev, reg_names, 2);
+		if (IS_ERR(reg_table))
+			return PTR_ERR(reg_table);
+		opp_table =
+			dev_pm_opp_register_set_opp_helper(dev, npu_opp_helper);
+		if (IS_ERR(opp_table)) {
+			dev_pm_opp_put_regulators(reg_table);
+			return PTR_ERR(opp_table);
+		}
+	} else {
+		reg_table = dev_pm_opp_set_regulators(dev, reg_names, 1);
+		if (IS_ERR(reg_table))
+			return PTR_ERR(reg_table);
+	}
+
+	rockchip_get_opp_data(rockchip_npu_of_match, &rknpu_dev->opp_info);
+	ret = rockchip_init_opp_table(dev, &rknpu_dev->opp_info, "npu_leakage",
+				      "rknpu");
+	if (ret) {
+		LOG_DEV_ERROR(dev, "failed to init_opp_table\n");
+		return ret;
+	}
+
+	rknpu_dev->current_freq = clk_get_rate(rknpu_dev->clks[0].clk);
+
+	opp = devfreq_recommended_opp(dev, &rknpu_dev->current_freq, 0);
+	if (IS_ERR(opp)) {
+		ret = PTR_ERR(opp);
+		goto err_remove_table;
+	}
+	dev_pm_opp_put(opp);
+	dp->initial_freq = rknpu_dev->current_freq;
+
+	ret = devfreq_add_governor(&devfreq_rknpu_ondemand);
+	if (ret) {
+		LOG_DEV_ERROR(dev, "failed to add rknpu_ondemand governor\n");
+		goto err_remove_table;
+	}
+
+	rknpu_dev->devfreq = devm_devfreq_add_device(dev, dp, "rknpu_ondemand",
+						     (void *)rknpu_dev);
+	if (IS_ERR(rknpu_dev->devfreq)) {
+		LOG_DEV_ERROR(dev, "failed to add devfreq\n");
+		ret = PTR_ERR(rknpu_dev->devfreq);
+		goto err_remove_governor;
+	}
+
+	npu_mdevp.data = rknpu_dev->devfreq;
+	npu_mdevp.opp_info = &rknpu_dev->opp_info;
+	rknpu_dev->mdev_info =
+		rockchip_system_monitor_register(dev, &npu_mdevp);
+	if (IS_ERR(rknpu_dev->mdev_info)) {
+		LOG_DEV_DEBUG(dev, "without system monitor\n");
+		rknpu_dev->mdev_info = NULL;
+		npu_mdevp.is_checked = true;
+	}
+	rknpu_dev->current_freq = clk_get_rate(rknpu_dev->clks[0].clk);
+	rknpu_dev->ondemand_freq = rknpu_dev->current_freq;
+	rknpu_dev->current_volt = regulator_get_voltage(rknpu_dev->vdd);
+
+	rknpu_dev->devfreq->previous_freq = rknpu_dev->current_freq;
+	if (rknpu_dev->devfreq->suspend_freq)
+		rknpu_dev->devfreq->resume_freq = rknpu_dev->current_freq;
+	rknpu_dev->devfreq->last_status.current_frequency =
+		rknpu_dev->current_freq;
+	rknpu_dev->devfreq->last_status.total_time = 1;
+	rknpu_dev->devfreq->last_status.busy_time = 1;
+
+	of_property_read_u32(dev->of_node, "dynamic-power-coefficient",
+			     (u32 *)&npu_cooling_power.dyn_power_coeff);
+	rknpu_dev->model_data =
+		rockchip_ipa_power_model_init(dev, "npu_leakage");
+	if (IS_ERR_OR_NULL(rknpu_dev->model_data)) {
+		rknpu_dev->model_data = NULL;
+		LOG_DEV_ERROR(dev, "failed to initialize power model\n");
+	} else if (rknpu_dev->model_data->dynamic_coefficient) {
+		npu_cooling_power.dyn_power_coeff =
+			rknpu_dev->model_data->dynamic_coefficient;
+	}
+	if (!npu_cooling_power.dyn_power_coeff) {
+		LOG_DEV_ERROR(dev, "failed to get dynamic-coefficient\n");
+		goto out;
+	}
+
+	rknpu_dev->devfreq_cooling = of_devfreq_cooling_register_power(
+		dev->of_node, rknpu_dev->devfreq, &npu_cooling_power);
+	if (IS_ERR_OR_NULL(rknpu_dev->devfreq_cooling))
+		LOG_DEV_ERROR(dev, "failed to register cooling device\n");
+
+out:
+	return 0;
+
+err_remove_governor:
+	devfreq_remove_governor(&devfreq_rknpu_ondemand);
+err_remove_table:
+	rockchip_uninit_opp_table(dev, &rknpu_dev->opp_info);
+
+	rknpu_dev->devfreq = NULL;
+
+	return ret;
+}
+EXPORT_SYMBOL(rknpu_devfreq_init);
+
+int rknpu_devfreq_runtime_suspend(struct device *dev)
+{
+	struct rknpu_device *rknpu_dev = dev_get_drvdata(dev);
+	struct rockchip_opp_info *opp_info = &rknpu_dev->opp_info;
+
+	if (opp_info->scmi_clk) {
+		if (clk_set_rate(opp_info->scmi_clk, POWER_DOWN_FREQ))
+			LOG_DEV_ERROR(dev, "failed to restore clk rate\n");
+	}
+	opp_info->current_rm = UINT_MAX;
+
+	return 0;
+}
+EXPORT_SYMBOL(rknpu_devfreq_runtime_suspend);
+
+int rknpu_devfreq_runtime_resume(struct device *dev)
+{
+	struct rknpu_device *rknpu_dev = dev_get_drvdata(dev);
+	struct rockchip_opp_info *opp_info = &rknpu_dev->opp_info;
+	int ret = 0;
+
+	if (!rknpu_dev->current_freq || !rknpu_dev->current_volt)
+		return 0;
+
+	ret = clk_bulk_prepare_enable(opp_info->num_clks, opp_info->clks);
+	if (ret) {
+		LOG_DEV_INFO(dev, "failed to enable opp clks\n");
+		return ret;
+	}
+
+	if (opp_info->data && opp_info->data->set_read_margin)
+		opp_info->data->set_read_margin(dev, opp_info,
+						opp_info->target_rm);
+	if (opp_info->scmi_clk) {
+		if (clk_set_rate(opp_info->scmi_clk, rknpu_dev->current_freq))
+			LOG_DEV_ERROR(dev, "failed to set power down rate\n");
+	}
+
+	clk_bulk_disable_unprepare(opp_info->num_clks, opp_info->clks);
+
+	return ret;
+}
+EXPORT_SYMBOL(rknpu_devfreq_runtime_resume);
+#endif
+
+void rknpu_devfreq_remove(struct rknpu_device *rknpu_dev)
+{
+	if (rknpu_dev->mdev_info) {
+		rockchip_system_monitor_unregister(rknpu_dev->mdev_info);
+		rknpu_dev->mdev_info = NULL;
+	}
+	if (rknpu_dev->devfreq)
+		devfreq_remove_governor(&devfreq_rknpu_ondemand);
+	rockchip_uninit_opp_table(rknpu_dev->dev, &rknpu_dev->opp_info);
+}
+EXPORT_SYMBOL(rknpu_devfreq_remove);
diff --git a/drivers/rknpu/rknpu_drv.c b/drivers/rknpu/rknpu_drv.c
new file mode 100644
index 000000000..9bcf61299
--- /dev/null
+++ b/drivers/rknpu/rknpu_drv.c
@@ -0,0 +1,1477 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#include <linux/dma-buf.h>
+#include <linux/dma-mapping.h>
+#include <linux/fs.h>
+#include <linux/interrupt.h>
+#include <linux/irq.h>
+#include <linux/irqdomain.h>
+#include <linux/iopoll.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+#include <linux/of_irq.h>
+#include <linux/of_platform.h>
+#include <linux/of_reserved_mem.h>
+#include <linux/platform_device.h>
+#include <linux/printk.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/time.h>
+#include <linux/uaccess.h>
+#include <linux/ktime.h>
+#include <linux/delay.h>
+#include <linux/wait.h>
+#include <linux/sched.h>
+#include <linux/clk.h>
+#include <linux/clk-provider.h>
+#include <linux/pm_domain.h>
+#include <linux/pm_runtime.h>
+#include <linux/regmap.h>
+#include <linux/of_address.h>
+
+#ifndef FPGA_PLATFORM
+#include <soc/rockchip/rockchip_iommu.h>
+#endif
+
+#include "rknpu_ioctl.h"
+#include "rknpu_reset.h"
+#include "rknpu_fence.h"
+#include "rknpu_drv.h"
+#include "rknpu_gem.h"
+#include "rknpu_devfreq.h"
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_DRM_GEM
+#include <drm/drm_device.h>
+#include <drm/drm_ioctl.h>
+#include <drm/drm_file.h>
+#include <drm/drm_drv.h>
+#include "rknpu_gem.h"
+#endif
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_DMA_HEAP
+#include <linux/rk-dma-heap.h>
+#include "rknpu_mem.h"
+#endif
+
+#define POWER_DOWN_FREQ 200000000
+#define NPU_MMU_DISABLED_POLL_PERIOD_US 1000
+#define NPU_MMU_DISABLED_POLL_TIMEOUT_US 20000
+
+static int bypass_irq_handler;
+module_param(bypass_irq_handler, int, 0644);
+MODULE_PARM_DESC(bypass_irq_handler,
+		 "bypass RKNPU irq handler if set it to 1, disabled by default");
+
+static int bypass_soft_reset;
+module_param(bypass_soft_reset, int, 0644);
+MODULE_PARM_DESC(bypass_soft_reset,
+		 "bypass RKNPU soft reset if set it to 1, disabled by default");
+
+struct rknpu_irqs_data {
+	const char *name;
+	irqreturn_t (*irq_hdl)(int irq, void *ctx);
+};
+
+static const struct rknpu_irqs_data rknpu_irqs[] = {
+	{ "npu_irq", rknpu_core0_irq_handler }
+};
+
+static const struct rknpu_irqs_data rk3588_npu_irqs[] = {
+	{ "npu0_irq", rknpu_core0_irq_handler },
+	{ "npu1_irq", rknpu_core1_irq_handler },
+	{ "npu2_irq", rknpu_core2_irq_handler }
+};
+
+static const struct rknpu_reset_data rknpu_resets[] = { { "srst_a",
+							  "srst_h" } };
+
+static const struct rknpu_reset_data rk3588_npu_resets[] = {
+	{ "srst_a0", "srst_h0" },
+	{ "srst_a1", "srst_h1" },
+	{ "srst_a2", "srst_h2" }
+};
+
+static const struct rknpu_config rk356x_rknpu_config = {
+	.bw_priority_addr = 0xfe180008,
+	.bw_priority_length = 0x10,
+	.dma_mask = DMA_BIT_MASK(32),
+	.pc_data_amount_scale = 1,
+	.pc_task_number_bits = 12,
+	.pc_task_number_mask = 0xfff,
+	.pc_task_status_offset = 0x3c,
+	.pc_dma_ctrl = 0,
+	.bw_enable = 1,
+	.irqs = rknpu_irqs,
+	.resets = rknpu_resets,
+	.num_irqs = ARRAY_SIZE(rknpu_irqs),
+	.num_resets = ARRAY_SIZE(rknpu_resets),
+	.nbuf_phyaddr = 0,
+	.nbuf_size = 0,
+	.max_submit_number = (1 << 12) - 1,
+	.core_mask = 0x1,
+};
+
+static const struct rknpu_config rk3588_rknpu_config = {
+	.bw_priority_addr = 0x0,
+	.bw_priority_length = 0x0,
+	.dma_mask = DMA_BIT_MASK(40),
+	.pc_data_amount_scale = 2,
+	.pc_task_number_bits = 12,
+	.pc_task_number_mask = 0xfff,
+	.pc_task_status_offset = 0x3c,
+	.pc_dma_ctrl = 0,
+	.bw_enable = 0,
+	.irqs = rk3588_npu_irqs,
+	.resets = rk3588_npu_resets,
+	.num_irqs = ARRAY_SIZE(rk3588_npu_irqs),
+	.num_resets = ARRAY_SIZE(rk3588_npu_resets),
+	.nbuf_phyaddr = 0,
+	.nbuf_size = 0,
+	.max_submit_number = (1 << 12) - 1,
+	.core_mask = 0x7,
+};
+
+static const struct rknpu_config rk3583_rknpu_config = {
+	.bw_priority_addr = 0x0,
+	.bw_priority_length = 0x0,
+	.dma_mask = DMA_BIT_MASK(40),
+	.pc_data_amount_scale = 2,
+	.pc_task_number_bits = 12,
+	.pc_task_number_mask = 0xfff,
+	.pc_task_status_offset = 0x3c,
+	.pc_dma_ctrl = 0,
+	.bw_enable = 0,
+	.irqs = rk3588_npu_irqs,
+	.resets = rk3588_npu_resets,
+	.num_irqs = 2,
+	.num_resets = 2,
+	.nbuf_phyaddr = 0,
+	.nbuf_size = 0,
+	.max_submit_number = (1 << 12) - 1,
+	.core_mask = 0x3,
+};
+
+static const struct rknpu_config rv1106_rknpu_config = {
+	.bw_priority_addr = 0x0,
+	.bw_priority_length = 0x0,
+	.dma_mask = DMA_BIT_MASK(32),
+	.pc_data_amount_scale = 2,
+	.pc_task_number_bits = 16,
+	.pc_task_number_mask = 0xffff,
+	.pc_task_status_offset = 0x3c,
+	.pc_dma_ctrl = 0,
+	.bw_enable = 1,
+	.irqs = rknpu_irqs,
+	.resets = rknpu_resets,
+	.num_irqs = ARRAY_SIZE(rknpu_irqs),
+	.num_resets = ARRAY_SIZE(rknpu_resets),
+	.nbuf_phyaddr = 0,
+	.nbuf_size = 0,
+	.max_submit_number = (1 << 16) - 1,
+	.core_mask = 0x1,
+};
+
+static const struct rknpu_config rk3562_rknpu_config = {
+	.bw_priority_addr = 0x0,
+	.bw_priority_length = 0x0,
+	.dma_mask = DMA_BIT_MASK(40),
+	.pc_data_amount_scale = 2,
+	.pc_task_number_bits = 16,
+	.pc_task_number_mask = 0xffff,
+	.pc_task_status_offset = 0x48,
+	.pc_dma_ctrl = 1,
+	.bw_enable = 1,
+	.irqs = rknpu_irqs,
+	.resets = rknpu_resets,
+	.num_irqs = ARRAY_SIZE(rknpu_irqs),
+	.num_resets = ARRAY_SIZE(rknpu_resets),
+	.nbuf_phyaddr = 0xfe400000,
+	.nbuf_size = 256 * 1024,
+	.max_submit_number = (1 << 16) - 1,
+	.core_mask = 0x1,
+};
+
+/* driver probe and init */
+static const struct of_device_id rknpu_of_match[] = {
+	{
+		.compatible = "rockchip,rknpu",
+		.data = &rk356x_rknpu_config,
+	},
+	{
+		.compatible = "rockchip,rk3568-rknpu",
+		.data = &rk356x_rknpu_config,
+	},
+	{
+		.compatible = "rockchip,rk3588-rknpu",
+		.data = &rk3588_rknpu_config,
+	},
+	{
+		.compatible = "rockchip,rv1106-rknpu",
+		.data = &rv1106_rknpu_config,
+	},
+	{
+		.compatible = "rockchip,rk3562-rknpu",
+		.data = &rk3562_rknpu_config,
+	},
+	{},
+};
+
+static int rknpu_get_drv_version(uint32_t *version)
+{
+	*version = RKNPU_GET_DRV_VERSION_CODE(DRIVER_MAJOR, DRIVER_MINOR,
+					      DRIVER_PATCHLEVEL);
+	return 0;
+}
+
+static int rknpu_power_on(struct rknpu_device *rknpu_dev);
+static int rknpu_power_off(struct rknpu_device *rknpu_dev);
+
+static void rknpu_power_off_delay_work(struct work_struct *power_off_work)
+{
+	struct rknpu_device *rknpu_dev =
+		container_of(to_delayed_work(power_off_work),
+			     struct rknpu_device, power_off_work);
+	mutex_lock(&rknpu_dev->power_lock);
+	if (atomic_dec_if_positive(&rknpu_dev->power_refcount) == 0)
+		rknpu_power_off(rknpu_dev);
+	mutex_unlock(&rknpu_dev->power_lock);
+}
+
+int rknpu_power_get(struct rknpu_device *rknpu_dev)
+{
+	int ret = 0;
+
+	mutex_lock(&rknpu_dev->power_lock);
+	if (atomic_inc_return(&rknpu_dev->power_refcount) == 1)
+		ret = rknpu_power_on(rknpu_dev);
+	mutex_unlock(&rknpu_dev->power_lock);
+
+	return ret;
+}
+
+int rknpu_power_put(struct rknpu_device *rknpu_dev)
+{
+	int ret = 0;
+
+	mutex_lock(&rknpu_dev->power_lock);
+	if (atomic_dec_if_positive(&rknpu_dev->power_refcount) == 0)
+		ret = rknpu_power_off(rknpu_dev);
+	mutex_unlock(&rknpu_dev->power_lock);
+
+	return ret;
+}
+
+static int rknpu_power_put_delay(struct rknpu_device *rknpu_dev)
+{
+	if (rknpu_dev->power_put_delay == 0)
+		return rknpu_power_put(rknpu_dev);
+
+	mutex_lock(&rknpu_dev->power_lock);
+	if (atomic_read(&rknpu_dev->power_refcount) == 1)
+		queue_delayed_work(
+			rknpu_dev->power_off_wq, &rknpu_dev->power_off_work,
+			msecs_to_jiffies(rknpu_dev->power_put_delay));
+	else
+		atomic_dec_if_positive(&rknpu_dev->power_refcount);
+	mutex_unlock(&rknpu_dev->power_lock);
+
+	return 0;
+}
+
+static int rknpu_action(struct rknpu_device *rknpu_dev,
+			struct rknpu_action *args)
+{
+	int ret = -EINVAL;
+
+	switch (args->flags) {
+	case RKNPU_GET_HW_VERSION:
+		ret = rknpu_get_hw_version(rknpu_dev, &args->value);
+		break;
+	case RKNPU_GET_DRV_VERSION:
+		ret = rknpu_get_drv_version(&args->value);
+		break;
+	case RKNPU_GET_FREQ:
+#ifndef FPGA_PLATFORM
+		args->value = clk_get_rate(rknpu_dev->clks[0].clk);
+#endif
+		ret = 0;
+		break;
+	case RKNPU_SET_FREQ:
+		break;
+	case RKNPU_GET_VOLT:
+#ifndef FPGA_PLATFORM
+		args->value = regulator_get_voltage(rknpu_dev->vdd);
+#endif
+		ret = 0;
+		break;
+	case RKNPU_SET_VOLT:
+		break;
+	case RKNPU_ACT_RESET:
+		ret = rknpu_soft_reset(rknpu_dev);
+		break;
+	case RKNPU_GET_BW_PRIORITY:
+		ret = rknpu_get_bw_priority(rknpu_dev, &args->value, NULL,
+					    NULL);
+		break;
+	case RKNPU_SET_BW_PRIORITY:
+		ret = rknpu_set_bw_priority(rknpu_dev, args->value, 0, 0);
+		break;
+	case RKNPU_GET_BW_EXPECT:
+		ret = rknpu_get_bw_priority(rknpu_dev, NULL, &args->value,
+					    NULL);
+		break;
+	case RKNPU_SET_BW_EXPECT:
+		ret = rknpu_set_bw_priority(rknpu_dev, 0, args->value, 0);
+		break;
+	case RKNPU_GET_BW_TW:
+		ret = rknpu_get_bw_priority(rknpu_dev, NULL, NULL,
+					    &args->value);
+		break;
+	case RKNPU_SET_BW_TW:
+		ret = rknpu_set_bw_priority(rknpu_dev, 0, 0, args->value);
+		break;
+	case RKNPU_ACT_CLR_TOTAL_RW_AMOUNT:
+		ret = rknpu_clear_rw_amount(rknpu_dev);
+		break;
+	case RKNPU_GET_DT_WR_AMOUNT:
+		ret = rknpu_get_rw_amount(rknpu_dev, &args->value, NULL, NULL);
+		break;
+	case RKNPU_GET_DT_RD_AMOUNT:
+		ret = rknpu_get_rw_amount(rknpu_dev, NULL, &args->value, NULL);
+		break;
+	case RKNPU_GET_WT_RD_AMOUNT:
+		ret = rknpu_get_rw_amount(rknpu_dev, NULL, NULL, &args->value);
+		break;
+	case RKNPU_GET_TOTAL_RW_AMOUNT:
+		ret = rknpu_get_total_rw_amount(rknpu_dev, &args->value);
+		break;
+	case RKNPU_GET_IOMMU_EN:
+		args->value = rknpu_dev->iommu_en;
+		ret = 0;
+		break;
+	case RKNPU_SET_PROC_NICE:
+		set_user_nice(current, *(int32_t *)&args->value);
+		ret = 0;
+		break;
+	case RKNPU_GET_TOTAL_SRAM_SIZE:
+		if (rknpu_dev->sram_mm)
+			args->value = rknpu_dev->sram_mm->total_chunks *
+				      rknpu_dev->sram_mm->chunk_size;
+		else
+			args->value = 0;
+		ret = 0;
+		break;
+	case RKNPU_GET_FREE_SRAM_SIZE:
+		if (rknpu_dev->sram_mm)
+			args->value = rknpu_dev->sram_mm->free_chunks *
+				      rknpu_dev->sram_mm->chunk_size;
+		else
+			args->value = 0;
+		ret = 0;
+		break;
+	default:
+		ret = -EINVAL;
+		break;
+	}
+
+	return ret;
+}
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_DMA_HEAP
+static int rknpu_open(struct inode *inode, struct file *file)
+{
+	struct rknpu_device *rknpu_dev =
+		container_of(file->private_data, struct rknpu_device, miscdev);
+	struct rknpu_session *session = NULL;
+
+	session = kzalloc(sizeof(*session), GFP_KERNEL);
+	if (!session) {
+		LOG_ERROR("rknpu session alloc failed\n");
+		return -ENOMEM;
+	}
+
+	session->rknpu_dev = rknpu_dev;
+	INIT_LIST_HEAD(&session->list);
+
+	file->private_data = (void *)session;
+
+	return nonseekable_open(inode, file);
+}
+
+static int rknpu_release(struct inode *inode, struct file *file)
+{
+	struct rknpu_mem_object *entry;
+	struct rknpu_session *session = file->private_data;
+	struct rknpu_device *rknpu_dev = session->rknpu_dev;
+	LIST_HEAD(local_list);
+
+	spin_lock(&rknpu_dev->lock);
+	list_replace_init(&session->list, &local_list);
+	file->private_data = NULL;
+	spin_unlock(&rknpu_dev->lock);
+
+	while (!list_empty(&local_list)) {
+		entry = list_first_entry(&local_list, struct rknpu_mem_object,
+					 head);
+
+		LOG_DEBUG(
+			"Fd close free rknpu_obj: %#llx, rknpu_obj->dma_addr: %#llx\n",
+			(__u64)(uintptr_t)entry, (__u64)entry->dma_addr);
+
+		vunmap(entry->kv_addr);
+		entry->kv_addr = NULL;
+
+		if (!entry->owner)
+			dma_buf_put(entry->dmabuf);
+
+		list_del(&entry->head);
+		kfree(entry);
+	}
+
+	kfree(session);
+
+	return 0;
+}
+
+static int rknpu_action_ioctl(struct rknpu_device *rknpu_dev,
+			      unsigned long data)
+{
+	struct rknpu_action args;
+	int ret = -EINVAL;
+
+	if (unlikely(copy_from_user(&args, (struct rknpu_action *)data,
+				    sizeof(struct rknpu_action)))) {
+		LOG_ERROR("%s: copy_from_user failed\n", __func__);
+		ret = -EFAULT;
+		return ret;
+	}
+
+	ret = rknpu_action(rknpu_dev, &args);
+
+	if (unlikely(copy_to_user((struct rknpu_action *)data, &args,
+				  sizeof(struct rknpu_action)))) {
+		LOG_ERROR("%s: copy_to_user failed\n", __func__);
+		ret = -EFAULT;
+		return ret;
+	}
+
+	return ret;
+}
+
+static long rknpu_ioctl(struct file *file, uint32_t cmd, unsigned long arg)
+{
+	long ret = -EINVAL;
+	struct rknpu_device *rknpu_dev = NULL;
+
+	if (!file->private_data)
+		return -EINVAL;
+
+	rknpu_dev = ((struct rknpu_session *)file->private_data)->rknpu_dev;
+
+	rknpu_power_get(rknpu_dev);
+
+	switch (cmd) {
+	case IOCTL_RKNPU_ACTION:
+		ret = rknpu_action_ioctl(rknpu_dev, arg);
+		break;
+	case IOCTL_RKNPU_SUBMIT:
+		ret = rknpu_submit_ioctl(rknpu_dev, arg);
+		break;
+	case IOCTL_RKNPU_MEM_CREATE:
+		ret = rknpu_mem_create_ioctl(rknpu_dev, arg, file);
+		break;
+	case RKNPU_MEM_MAP:
+		break;
+	case IOCTL_RKNPU_MEM_DESTROY:
+		ret = rknpu_mem_destroy_ioctl(rknpu_dev, arg, file);
+		break;
+	case IOCTL_RKNPU_MEM_SYNC:
+		ret = rknpu_mem_sync_ioctl(rknpu_dev, arg);
+		break;
+	default:
+		break;
+	}
+
+	rknpu_power_put_delay(rknpu_dev);
+
+	return ret;
+}
+const struct file_operations rknpu_fops = {
+	.owner = THIS_MODULE,
+	.open = rknpu_open,
+	.release = rknpu_release,
+	.unlocked_ioctl = rknpu_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl = rknpu_ioctl,
+#endif
+};
+#endif
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_DRM_GEM
+#if KERNEL_VERSION(6, 1, 0) > LINUX_VERSION_CODE
+static const struct vm_operations_struct rknpu_gem_vm_ops = {
+	.fault = rknpu_gem_fault,
+	.open = drm_gem_vm_open,
+	.close = drm_gem_vm_close,
+};
+#endif
+
+static int rknpu_action_ioctl(struct drm_device *dev, void *data,
+			      struct drm_file *file_priv)
+{
+	struct rknpu_device *rknpu_dev = dev_get_drvdata(dev->dev);
+
+	return rknpu_action(rknpu_dev, (struct rknpu_action *)data);
+}
+
+#define RKNPU_IOCTL(func)                                                      \
+	static int __##func(struct drm_device *dev, void *data,                \
+			    struct drm_file *file_priv)                        \
+	{                                                                      \
+		struct rknpu_device *rknpu_dev = dev_get_drvdata(dev->dev);    \
+		int ret = -EINVAL;                                             \
+		rknpu_power_get(rknpu_dev);                                    \
+		ret = func(dev, data, file_priv);                              \
+		rknpu_power_put_delay(rknpu_dev);                              \
+		return ret;                                                    \
+	}
+
+RKNPU_IOCTL(rknpu_action_ioctl);
+RKNPU_IOCTL(rknpu_submit_ioctl);
+RKNPU_IOCTL(rknpu_gem_create_ioctl);
+RKNPU_IOCTL(rknpu_gem_map_ioctl);
+RKNPU_IOCTL(rknpu_gem_destroy_ioctl);
+RKNPU_IOCTL(rknpu_gem_sync_ioctl);
+
+static const struct drm_ioctl_desc rknpu_ioctls[] = {
+	DRM_IOCTL_DEF_DRV(RKNPU_ACTION, __rknpu_action_ioctl, DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(RKNPU_SUBMIT, __rknpu_submit_ioctl, DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(RKNPU_MEM_CREATE, __rknpu_gem_create_ioctl,
+			  DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(RKNPU_MEM_MAP, __rknpu_gem_map_ioctl,
+			  DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(RKNPU_MEM_DESTROY, __rknpu_gem_destroy_ioctl,
+			  DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(RKNPU_MEM_SYNC, __rknpu_gem_sync_ioctl,
+			  DRM_RENDER_ALLOW),
+};
+
+#if KERNEL_VERSION(6, 1, 0) <= LINUX_VERSION_CODE
+DEFINE_DRM_GEM_FOPS(rknpu_drm_driver_fops);
+#else
+static const struct file_operations rknpu_drm_driver_fops = {
+	.owner = THIS_MODULE,
+	.open = drm_open,
+	.mmap = rknpu_gem_mmap,
+	.poll = drm_poll,
+	.read = drm_read,
+	.unlocked_ioctl = drm_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl = drm_compat_ioctl,
+#endif
+	.release = drm_release,
+	.llseek = noop_llseek,
+};
+#endif
+
+static struct drm_driver rknpu_drm_driver = {
+#if KERNEL_VERSION(5, 4, 0) <= LINUX_VERSION_CODE
+	.driver_features = DRIVER_GEM | DRIVER_RENDER,
+#else
+	.driver_features = DRIVER_GEM | DRIVER_PRIME | DRIVER_RENDER,
+#endif
+#if KERNEL_VERSION(6, 1, 0) > LINUX_VERSION_CODE
+	.gem_free_object_unlocked = rknpu_gem_free_object,
+	.gem_vm_ops = &rknpu_gem_vm_ops,
+	.dumb_destroy = drm_gem_dumb_destroy,
+	.gem_prime_export = drm_gem_prime_export,
+	.gem_prime_get_sg_table = rknpu_gem_prime_get_sg_table,
+	.gem_prime_vmap = rknpu_gem_prime_vmap,
+	.gem_prime_vunmap = rknpu_gem_prime_vunmap,
+#endif
+	.dumb_create = rknpu_gem_dumb_create,
+#if KERNEL_VERSION(4, 19, 0) > LINUX_VERSION_CODE
+	.dumb_map_offset = rknpu_gem_dumb_map_offset,
+#else
+	.dumb_map_offset = drm_gem_dumb_map_offset,
+#endif
+	.prime_handle_to_fd = drm_gem_prime_handle_to_fd,
+	.prime_fd_to_handle = drm_gem_prime_fd_to_handle,
+#if KERNEL_VERSION(4, 13, 0) <= LINUX_VERSION_CODE
+	.gem_prime_import = rknpu_gem_prime_import,
+#else
+	.gem_prime_import = drm_gem_prime_import,
+#endif
+	.gem_prime_import_sg_table = rknpu_gem_prime_import_sg_table,
+#if KERNEL_VERSION(6, 6, 0) <= LINUX_VERSION_CODE
+#elif KERNEL_VERSION(6, 1, 0) <= LINUX_VERSION_CODE
+	.gem_prime_mmap = drm_gem_prime_mmap,
+#else
+	.gem_prime_mmap = rknpu_gem_prime_mmap,
+#endif
+	.ioctls = rknpu_ioctls,
+	.num_ioctls = ARRAY_SIZE(rknpu_ioctls),
+	.fops = &rknpu_drm_driver_fops,
+	.name = DRIVER_NAME,
+	.desc = DRIVER_DESC,
+	.date = DRIVER_DATE,
+	.major = DRIVER_MAJOR,
+	.minor = DRIVER_MINOR,
+	.patchlevel = DRIVER_PATCHLEVEL,
+};
+
+#endif
+
+static enum hrtimer_restart hrtimer_handler(struct hrtimer *timer)
+{
+	struct rknpu_device *rknpu_dev =
+		container_of(timer, struct rknpu_device, timer);
+	struct rknpu_subcore_data *subcore_data = NULL;
+	struct rknpu_job *job = NULL;
+	ktime_t now;
+	unsigned long flags;
+	int i;
+
+	for (i = 0; i < rknpu_dev->config->num_irqs; i++) {
+		subcore_data = &rknpu_dev->subcore_datas[i];
+
+		spin_lock_irqsave(&rknpu_dev->irq_lock, flags);
+
+		job = subcore_data->job;
+		if (job) {
+			now = ktime_get();
+			subcore_data->timer.busy_time +=
+				ktime_sub(now, job->hw_recoder_time);
+			job->hw_recoder_time = now;
+		}
+
+		subcore_data->timer.total_busy_time =
+			subcore_data->timer.busy_time;
+		subcore_data->timer.busy_time = 0;
+
+		spin_unlock_irqrestore(&rknpu_dev->irq_lock, flags);
+	}
+
+	hrtimer_forward_now(timer, rknpu_dev->kt);
+	return HRTIMER_RESTART;
+}
+
+static void rknpu_init_timer(struct rknpu_device *rknpu_dev)
+{
+	rknpu_dev->kt = ktime_set(0, RKNPU_LOAD_INTERVAL);
+	hrtimer_init(&rknpu_dev->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	rknpu_dev->timer.function = hrtimer_handler;
+	hrtimer_start(&rknpu_dev->timer, rknpu_dev->kt, HRTIMER_MODE_REL);
+}
+
+static void rknpu_cancel_timer(struct rknpu_device *rknpu_dev)
+{
+	hrtimer_cancel(&rknpu_dev->timer);
+}
+
+static bool rknpu_is_iommu_enable(struct device *dev)
+{
+	struct device_node *iommu = NULL;
+
+	iommu = of_parse_phandle(dev->of_node, "iommus", 0);
+	if (!iommu) {
+		LOG_DEV_INFO(
+			dev,
+			"rknpu iommu device-tree entry not found!, using non-iommu mode\n");
+		return false;
+	}
+
+	if (!of_device_is_available(iommu)) {
+		LOG_DEV_INFO(dev,
+			     "rknpu iommu is disabled, using non-iommu mode\n");
+		of_node_put(iommu);
+		return false;
+	}
+	of_node_put(iommu);
+
+	LOG_DEV_INFO(dev, "rknpu iommu is enabled, using iommu mode\n");
+
+	return true;
+}
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_DRM_GEM
+static int drm_fake_dev_register(struct rknpu_device *rknpu_dev)
+{
+	const struct platform_device_info rknpu_dev_info = {
+		.name = "rknpu_dev",
+		.id = PLATFORM_DEVID_AUTO,
+		.dma_mask = rknpu_dev->config->dma_mask,
+	};
+	struct platform_device *pdev = NULL;
+	int ret = -EINVAL;
+
+	pdev = platform_device_register_full(&rknpu_dev_info);
+	if (pdev) {
+		ret = of_dma_configure(&pdev->dev, NULL, true);
+		if (ret) {
+			platform_device_unregister(pdev);
+			pdev = NULL;
+		}
+	}
+
+	rknpu_dev->fake_dev = pdev ? &pdev->dev : NULL;
+
+	return ret;
+}
+
+static void drm_fake_dev_unregister(struct rknpu_device *rknpu_dev)
+{
+	struct platform_device *pdev = NULL;
+
+	if (!rknpu_dev->fake_dev)
+		return;
+
+	pdev = to_platform_device(rknpu_dev->fake_dev);
+
+	platform_device_unregister(pdev);
+}
+
+static int rknpu_drm_probe(struct rknpu_device *rknpu_dev)
+{
+	struct device *dev = rknpu_dev->dev;
+	struct drm_device *drm_dev = NULL;
+	int ret = -EINVAL;
+
+	drm_dev = drm_dev_alloc(&rknpu_drm_driver, dev);
+	if (IS_ERR(drm_dev))
+		return PTR_ERR(drm_dev);
+
+	/* register the DRM device */
+	ret = drm_dev_register(drm_dev, 0);
+	if (ret < 0)
+		goto err_free_drm;
+
+	drm_dev->dev_private = rknpu_dev;
+	rknpu_dev->drm_dev = drm_dev;
+
+	drm_fake_dev_register(rknpu_dev);
+
+	return 0;
+
+err_free_drm:
+#if KERNEL_VERSION(4, 15, 0) <= LINUX_VERSION_CODE
+	drm_dev_put(drm_dev);
+#else
+	drm_dev_unref(drm_dev);
+#endif
+
+	return ret;
+}
+
+static void rknpu_drm_remove(struct rknpu_device *rknpu_dev)
+{
+	struct drm_device *drm_dev = rknpu_dev->drm_dev;
+
+	drm_fake_dev_unregister(rknpu_dev);
+
+	drm_dev_unregister(drm_dev);
+
+#if KERNEL_VERSION(4, 15, 0) <= LINUX_VERSION_CODE
+	drm_dev_put(drm_dev);
+#else
+	drm_dev_unref(drm_dev);
+#endif
+}
+#endif
+
+static int rknpu_power_on(struct rknpu_device *rknpu_dev)
+{
+	struct device *dev = rknpu_dev->dev;
+	int ret = -EINVAL;
+
+#ifndef FPGA_PLATFORM
+	if (rknpu_dev->vdd) {
+		ret = regulator_enable(rknpu_dev->vdd);
+		if (ret) {
+			LOG_DEV_ERROR(
+				dev,
+				"failed to enable vdd reg for rknpu, ret: %d\n",
+				ret);
+			return ret;
+		}
+	}
+
+	if (rknpu_dev->mem) {
+		ret = regulator_enable(rknpu_dev->mem);
+		if (ret) {
+			LOG_DEV_ERROR(
+				dev,
+				"failed to enable mem reg for rknpu, ret: %d\n",
+				ret);
+			return ret;
+		}
+	}
+#endif
+
+	ret = clk_bulk_prepare_enable(rknpu_dev->num_clks, rknpu_dev->clks);
+	if (ret) {
+		LOG_DEV_ERROR(dev, "failed to enable clk for rknpu, ret: %d\n",
+			      ret);
+		return ret;
+	}
+
+#ifndef FPGA_PLATFORM
+	rknpu_devfreq_lock(rknpu_dev);
+#endif
+
+	if (rknpu_dev->multiple_domains) {
+		if (rknpu_dev->genpd_dev_npu0) {
+#if KERNEL_VERSION(5, 5, 0) < LINUX_VERSION_CODE
+			ret = pm_runtime_resume_and_get(
+				rknpu_dev->genpd_dev_npu0);
+#else
+			ret = pm_runtime_get_sync(rknpu_dev->genpd_dev_npu0);
+#endif
+			if (ret < 0) {
+				LOG_DEV_ERROR(
+					dev,
+					"failed to get pm runtime for npu0, ret: %d\n",
+					ret);
+				goto out;
+			}
+		}
+		if (rknpu_dev->genpd_dev_npu1) {
+#if KERNEL_VERSION(5, 5, 0) < LINUX_VERSION_CODE
+			ret = pm_runtime_resume_and_get(
+				rknpu_dev->genpd_dev_npu1);
+#else
+			ret = pm_runtime_get_sync(rknpu_dev->genpd_dev_npu1);
+#endif
+			if (ret < 0) {
+				LOG_DEV_ERROR(
+					dev,
+					"failed to get pm runtime for npu1, ret: %d\n",
+					ret);
+				goto out;
+			}
+		}
+		if (rknpu_dev->genpd_dev_npu2) {
+#if KERNEL_VERSION(5, 5, 0) < LINUX_VERSION_CODE
+			ret = pm_runtime_resume_and_get(
+				rknpu_dev->genpd_dev_npu2);
+#else
+			ret = pm_runtime_get_sync(rknpu_dev->genpd_dev_npu2);
+#endif
+			if (ret < 0) {
+				LOG_DEV_ERROR(
+					dev,
+					"failed to get pm runtime for npu2, ret: %d\n",
+					ret);
+				goto out;
+			}
+		}
+	}
+	ret = pm_runtime_get_sync(dev);
+	if (ret < 0) {
+		LOG_DEV_ERROR(dev,
+			      "failed to get pm runtime for rknpu, ret: %d\n",
+			      ret);
+	}
+
+out:
+#ifndef FPGA_PLATFORM
+	rknpu_devfreq_unlock(rknpu_dev);
+#endif
+
+	return ret;
+}
+
+static int rknpu_power_off(struct rknpu_device *rknpu_dev)
+{
+	struct device *dev = rknpu_dev->dev;
+
+#ifndef FPGA_PLATFORM
+	int ret;
+	// bool val;
+
+	rknpu_devfreq_lock(rknpu_dev);
+#endif
+
+	pm_runtime_put_sync(dev);
+
+	if (rknpu_dev->multiple_domains) {
+#ifndef FPGA_PLATFORM
+		/*
+		 * Because IOMMU's runtime suspend callback is asynchronous,
+		 * So it may be executed after the NPU is turned off after PD/CLK/VD,
+		 * and the runtime suspend callback has a register access.
+		 * If the PD/VD/CLK is closed, the register access will crash.
+		 * As a workaround, it's safe to close pd stuff until iommu disabled.
+		 * If pm runtime framework can handle this issue in the future, remove
+		 * this.
+		 */
+		// ret = readx_poll_timeout(rockchip_iommu_is_enabled, dev, val,
+		// 			 !val, NPU_MMU_DISABLED_POLL_PERIOD_US,
+		// 			 NPU_MMU_DISABLED_POLL_TIMEOUT_US);
+
+		// TODO: implement the rockchip_iommu_is_enabled
+		msleep(20);
+		ret = 0;
+		if (ret) {
+			LOG_DEV_ERROR(dev, "iommu still enabled\n");
+			pm_runtime_get_sync(dev);
+			rknpu_devfreq_unlock(rknpu_dev);
+			return ret;
+		}
+#else
+		if (rknpu_dev->iommu_en)
+			msleep(20);
+#endif
+		if (rknpu_dev->genpd_dev_npu2)
+			pm_runtime_put_sync(rknpu_dev->genpd_dev_npu2);
+		if (rknpu_dev->genpd_dev_npu1)
+			pm_runtime_put_sync(rknpu_dev->genpd_dev_npu1);
+		if (rknpu_dev->genpd_dev_npu0)
+			pm_runtime_put_sync(rknpu_dev->genpd_dev_npu0);
+	}
+
+#ifndef FPGA_PLATFORM
+	rknpu_devfreq_unlock(rknpu_dev);
+#endif
+
+	clk_bulk_disable_unprepare(rknpu_dev->num_clks, rknpu_dev->clks);
+
+#ifndef FPGA_PLATFORM
+	if (rknpu_dev->vdd)
+		regulator_disable(rknpu_dev->vdd);
+
+	if (rknpu_dev->mem)
+		regulator_disable(rknpu_dev->mem);
+#endif
+
+	return 0;
+}
+
+static int rknpu_register_irq(struct platform_device *pdev,
+			      struct rknpu_device *rknpu_dev)
+{
+	const struct rknpu_config *config = rknpu_dev->config;
+	struct device *dev = &pdev->dev;
+	struct resource *res;
+	int i, ret, irq;
+
+	res = platform_get_resource_byname(pdev, IORESOURCE_IRQ,
+					   config->irqs[0].name);
+	if (res) {
+		/* there are irq names in dts */
+		for (i = 0; i < config->num_irqs; i++) {
+			irq = platform_get_irq_byname(pdev,
+						      config->irqs[i].name);
+			if (irq < 0) {
+				LOG_DEV_ERROR(dev, "no npu %s in dts\n",
+					      config->irqs[i].name);
+				return irq;
+			}
+
+			ret = devm_request_irq(dev, irq,
+					       config->irqs[i].irq_hdl,
+					       IRQF_SHARED, dev_name(dev),
+					       rknpu_dev);
+			if (ret < 0) {
+				LOG_DEV_ERROR(dev, "request %s failed: %d\n",
+					      config->irqs[i].name, ret);
+				return ret;
+			}
+		}
+	} else {
+		/* no irq names in dts */
+		irq = platform_get_irq(pdev, 0);
+		if (irq < 0) {
+			LOG_DEV_ERROR(dev, "no npu irq in dts\n");
+			return irq;
+		}
+
+		ret = devm_request_irq(dev, irq, rknpu_core0_irq_handler,
+				       IRQF_SHARED, dev_name(dev), rknpu_dev);
+		if (ret < 0) {
+			LOG_DEV_ERROR(dev, "request irq failed: %d\n", ret);
+			return ret;
+		}
+	}
+
+	return 0;
+}
+
+static int rknpu_find_sram_resource(struct rknpu_device *rknpu_dev)
+{
+	struct device *dev = rknpu_dev->dev;
+	struct device_node *sram_node = NULL;
+	struct resource sram_res;
+	uint32_t sram_size = 0;
+	int ret = -EINVAL;
+
+	/* get sram device node */
+	sram_node = of_parse_phandle(dev->of_node, "rockchip,sram", 0);
+	rknpu_dev->sram_size = 0;
+	if (!sram_node)
+		return -EINVAL;
+
+	/* get sram start and size */
+	ret = of_address_to_resource(sram_node, 0, &sram_res);
+	of_node_put(sram_node);
+	if (ret)
+		return ret;
+
+	/* check sram start and size is PAGE_SIZE align */
+	rknpu_dev->sram_start = round_up(sram_res.start, PAGE_SIZE);
+	rknpu_dev->sram_end = round_down(
+		sram_res.start + resource_size(&sram_res), PAGE_SIZE);
+	if (rknpu_dev->sram_end <= rknpu_dev->sram_start) {
+		LOG_DEV_WARN(
+			dev,
+			"invalid sram resource, sram start %pa, sram end %pa\n",
+			&rknpu_dev->sram_start, &rknpu_dev->sram_end);
+		return -EINVAL;
+	}
+
+	sram_size = rknpu_dev->sram_end - rknpu_dev->sram_start;
+
+	rknpu_dev->sram_base_io =
+		devm_ioremap(dev, rknpu_dev->sram_start, sram_size);
+	if (IS_ERR(rknpu_dev->sram_base_io)) {
+		LOG_DEV_ERROR(dev, "failed to remap sram base io!\n");
+		rknpu_dev->sram_base_io = NULL;
+	}
+
+	rknpu_dev->sram_size = sram_size;
+
+	LOG_DEV_INFO(dev, "sram region: [%pa, %pa), sram size: %#x\n",
+		     &rknpu_dev->sram_start, &rknpu_dev->sram_end,
+		     rknpu_dev->sram_size);
+
+	return 0;
+}
+
+static int rknpu_find_nbuf_resource(struct rknpu_device *rknpu_dev)
+{
+	struct device *dev = rknpu_dev->dev;
+
+	if (rknpu_dev->config->nbuf_size == 0)
+		return -EINVAL;
+
+	rknpu_dev->nbuf_start = rknpu_dev->config->nbuf_phyaddr;
+	rknpu_dev->nbuf_size = rknpu_dev->config->nbuf_size;
+	rknpu_dev->nbuf_base_io =
+		devm_ioremap(dev, rknpu_dev->nbuf_start, rknpu_dev->nbuf_size);
+	if (IS_ERR(rknpu_dev->nbuf_base_io)) {
+		LOG_DEV_ERROR(dev, "failed to remap nbuf base io!\n");
+		rknpu_dev->nbuf_base_io = NULL;
+	}
+
+	rknpu_dev->nbuf_end = rknpu_dev->nbuf_start + rknpu_dev->nbuf_size;
+
+	LOG_DEV_INFO(dev, "nbuf region: [%pa, %pa), nbuf size: %#x\n",
+		     &rknpu_dev->nbuf_start, &rknpu_dev->nbuf_end,
+		     rknpu_dev->nbuf_size);
+
+	return 0;
+}
+
+static int rknpu_get_invalid_core_mask(struct device *dev)
+{
+	int ret = 0;
+	u8 invalid_core_mask = 0;
+
+	if (of_property_match_string(dev->of_node, "nvmem-cell-names",
+				     "cores") >= 0) {
+		ret = rockchip_nvmem_cell_read_u8(dev->of_node, "cores",
+						  &invalid_core_mask);
+		/* The default valid npu cores for RK3583 are core0 and core1 */
+		invalid_core_mask |= RKNPU_CORE2_MASK;
+		if (ret) {
+			LOG_DEV_ERROR(
+				dev,
+				"failed to get specification_serial_number\n");
+			return invalid_core_mask;
+		}
+	}
+
+	return (int)invalid_core_mask;
+}
+
+static int rknpu_probe(struct platform_device *pdev)
+{
+	struct resource *res = NULL;
+	struct rknpu_device *rknpu_dev = NULL;
+	struct device *dev = &pdev->dev;
+	struct device *virt_dev = NULL;
+	const struct of_device_id *match = NULL;
+	const struct rknpu_config *config = NULL;
+	int ret = -EINVAL, i = 0;
+
+	if (!pdev->dev.of_node) {
+		LOG_DEV_ERROR(dev, "rknpu device-tree data is missing!\n");
+		return -ENODEV;
+	}
+
+	match = of_match_device(rknpu_of_match, dev);
+	if (!match) {
+		LOG_DEV_ERROR(dev, "rknpu device-tree entry is missing!\n");
+		return -ENODEV;
+	}
+
+	rknpu_dev = devm_kzalloc(dev, sizeof(*rknpu_dev), GFP_KERNEL);
+	if (!rknpu_dev) {
+		LOG_DEV_ERROR(dev, "failed to allocate rknpu device!\n");
+		return -ENOMEM;
+	}
+
+	config = of_device_get_match_data(dev);
+	if (!config)
+		return -EINVAL;
+
+	if (match->data == (void *)&rk3588_rknpu_config) {
+		int invalid_core_mask = rknpu_get_invalid_core_mask(dev);
+		/* The default valid npu cores for RK3583 are core0 and core1 */
+		if (invalid_core_mask & RKNPU_CORE2_MASK) {
+			if ((invalid_core_mask & RKNPU_CORE0_MASK) ||
+			    (invalid_core_mask & RKNPU_CORE1_MASK)) {
+				LOG_DEV_ERROR(
+					dev,
+					"rknpu core invalid, invalid core mask: %#x\n",
+					invalid_core_mask);
+				return -ENODEV;
+			}
+			config = &rk3583_rknpu_config;
+		}
+	}
+
+	rknpu_dev->config = config;
+	rknpu_dev->dev = dev;
+
+	rknpu_dev->iommu_en = rknpu_is_iommu_enable(dev);
+	if (!rknpu_dev->iommu_en) {
+		/* Initialize reserved memory resources */
+		ret = of_reserved_mem_device_init(dev);
+		if (!ret) {
+			LOG_DEV_INFO(
+				dev,
+				"initialize reserved memory for rknpu device!\n");
+		}
+	}
+
+	rknpu_dev->bypass_irq_handler = bypass_irq_handler;
+	rknpu_dev->bypass_soft_reset = bypass_soft_reset;
+
+	rknpu_reset_get(rknpu_dev);
+
+	rknpu_dev->num_clks = devm_clk_bulk_get_all(dev, &rknpu_dev->clks);
+	if (rknpu_dev->num_clks < 1) {
+		LOG_DEV_ERROR(dev, "failed to get clk source for rknpu\n");
+#ifndef FPGA_PLATFORM
+		return -ENODEV;
+#endif
+	}
+
+#ifndef FPGA_PLATFORM
+	rknpu_dev->vdd = devm_regulator_get_optional(dev, "rknpu");
+	if (IS_ERR(rknpu_dev->vdd)) {
+		if (PTR_ERR(rknpu_dev->vdd) != -ENODEV) {
+			ret = PTR_ERR(rknpu_dev->vdd);
+			LOG_DEV_ERROR(
+				dev,
+				"failed to get vdd regulator for rknpu: %d\n",
+				ret);
+			return ret;
+		}
+		rknpu_dev->vdd = NULL;
+	}
+
+	rknpu_dev->mem = devm_regulator_get_optional(dev, "mem");
+	if (IS_ERR(rknpu_dev->mem)) {
+		if (PTR_ERR(rknpu_dev->mem) != -ENODEV) {
+			ret = PTR_ERR(rknpu_dev->mem);
+			LOG_DEV_ERROR(
+				dev,
+				"failed to get mem regulator for rknpu: %d\n",
+				ret);
+			return ret;
+		}
+		rknpu_dev->mem = NULL;
+	}
+#endif
+
+	spin_lock_init(&rknpu_dev->lock);
+	spin_lock_init(&rknpu_dev->irq_lock);
+	mutex_init(&rknpu_dev->power_lock);
+	mutex_init(&rknpu_dev->reset_lock);
+	for (i = 0; i < config->num_irqs; i++) {
+		INIT_LIST_HEAD(&rknpu_dev->subcore_datas[i].todo_list);
+		init_waitqueue_head(&rknpu_dev->subcore_datas[i].job_done_wq);
+		rknpu_dev->subcore_datas[i].task_num = 0;
+		res = platform_get_resource(pdev, IORESOURCE_MEM, i);
+		if (!res) {
+			LOG_DEV_ERROR(
+				dev,
+				"failed to get memory resource for rknpu\n");
+			return -ENXIO;
+		}
+
+		rknpu_dev->base[i] = devm_ioremap_resource(dev, res);
+		if (PTR_ERR(rknpu_dev->base[i]) == -EBUSY) {
+			rknpu_dev->base[i] = devm_ioremap(dev, res->start,
+							  resource_size(res));
+		}
+
+		if (IS_ERR(rknpu_dev->base[i])) {
+			LOG_DEV_ERROR(dev,
+				      "failed to remap register for rknpu\n");
+			return PTR_ERR(rknpu_dev->base[i]);
+		}
+	}
+
+	if (config->bw_priority_length > 0) {
+		rknpu_dev->bw_priority_base =
+			devm_ioremap(dev, config->bw_priority_addr,
+				     config->bw_priority_length);
+		if (IS_ERR(rknpu_dev->bw_priority_base)) {
+			LOG_DEV_ERROR(
+				rknpu_dev->dev,
+				"failed to remap bw priority register for rknpu\n");
+			rknpu_dev->bw_priority_base = NULL;
+		}
+	}
+
+	if (!rknpu_dev->bypass_irq_handler) {
+		ret = rknpu_register_irq(pdev, rknpu_dev);
+		if (ret)
+			return ret;
+	} else {
+		LOG_DEV_WARN(dev, "bypass irq handler!\n");
+	}
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_DRM_GEM
+	ret = rknpu_drm_probe(rknpu_dev);
+	if (ret) {
+		LOG_DEV_ERROR(dev, "failed to probe device for rknpu\n");
+		return ret;
+	}
+#endif
+#ifdef CONFIG_ROCKCHIP_RKNPU_DMA_HEAP
+	rknpu_dev->miscdev.minor = MISC_DYNAMIC_MINOR;
+	rknpu_dev->miscdev.name = "rknpu";
+	rknpu_dev->miscdev.fops = &rknpu_fops;
+
+	ret = misc_register(&rknpu_dev->miscdev);
+	if (ret) {
+		LOG_DEV_ERROR(dev, "cannot register miscdev (%d)\n", ret);
+		return ret;
+	}
+
+	rknpu_dev->heap = rk_dma_heap_find("rk-dma-heap-cma");
+	if (!rknpu_dev->heap) {
+		LOG_DEV_ERROR(dev, "failed to find cma heap\n");
+		return -ENOMEM;
+	}
+	rk_dma_heap_set_dev(dev);
+	LOG_DEV_INFO(dev, "Initialized %s: v%d.%d.%d for %s\n", DRIVER_DESC,
+		     DRIVER_MAJOR, DRIVER_MINOR, DRIVER_PATCHLEVEL,
+		     DRIVER_DATE);
+#endif
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_FENCE
+	ret = rknpu_fence_context_alloc(rknpu_dev);
+	if (ret) {
+		LOG_DEV_ERROR(dev,
+			      "failed to allocate fence context for rknpu\n");
+		goto err_remove_drv;
+	}
+#endif
+
+	platform_set_drvdata(pdev, rknpu_dev);
+
+	pm_runtime_enable(dev);
+
+	if (of_count_phandle_with_args(dev->of_node, "power-domains",
+				       "#power-domain-cells") > 1) {
+		virt_dev = dev_pm_domain_attach_by_name(dev, "npu0");
+		if (!IS_ERR(virt_dev))
+			rknpu_dev->genpd_dev_npu0 = virt_dev;
+		virt_dev = dev_pm_domain_attach_by_name(dev, "npu1");
+		if (!IS_ERR(virt_dev))
+			rknpu_dev->genpd_dev_npu1 = virt_dev;
+		if (config->num_irqs > 2) {
+			virt_dev = dev_pm_domain_attach_by_name(dev, "npu2");
+			if (!IS_ERR(virt_dev))
+				rknpu_dev->genpd_dev_npu2 = virt_dev;
+		}
+		rknpu_dev->multiple_domains = true;
+	}
+
+	ret = rknpu_power_on(rknpu_dev);
+	if (ret)
+		goto err_remove_drv;
+
+#ifndef FPGA_PLATFORM
+	rknpu_devfreq_init(rknpu_dev);
+#endif
+
+	// set default power put delay to 3s
+	rknpu_dev->power_put_delay = 3000;
+	rknpu_dev->power_off_wq =
+		create_freezable_workqueue("rknpu_power_off_wq");
+	if (!rknpu_dev->power_off_wq) {
+		LOG_DEV_ERROR(dev, "rknpu couldn't create power_off workqueue");
+		ret = -ENOMEM;
+		goto err_devfreq_remove;
+	}
+	INIT_DEFERRABLE_WORK(&rknpu_dev->power_off_work,
+			     rknpu_power_off_delay_work);
+
+	if (IS_ENABLED(CONFIG_NO_GKI) &&
+	    IS_ENABLED(CONFIG_ROCKCHIP_RKNPU_SRAM) && rknpu_dev->iommu_en) {
+		if (!rknpu_find_sram_resource(rknpu_dev)) {
+			ret = rknpu_mm_create(rknpu_dev->sram_size, PAGE_SIZE,
+					      &rknpu_dev->sram_mm);
+			if (ret != 0)
+				goto err_remove_wq;
+		} else {
+			LOG_DEV_WARN(dev, "could not find sram resource!\n");
+		}
+	}
+
+	if (IS_ENABLED(CONFIG_NO_GKI) && rknpu_dev->iommu_en &&
+	    rknpu_dev->config->nbuf_size > 0)
+		rknpu_find_nbuf_resource(rknpu_dev);
+
+	rknpu_power_off(rknpu_dev);
+	atomic_set(&rknpu_dev->power_refcount, 0);
+	atomic_set(&rknpu_dev->cmdline_power_refcount, 0);
+
+	rknpu_debugger_init(rknpu_dev);
+	rknpu_init_timer(rknpu_dev);
+
+	return 0;
+
+err_remove_wq:
+	destroy_workqueue(rknpu_dev->power_off_wq);
+
+err_devfreq_remove:
+#ifndef FPGA_PLATFORM
+	rknpu_devfreq_remove(rknpu_dev);
+#endif
+
+err_remove_drv:
+#ifdef CONFIG_ROCKCHIP_RKNPU_DRM_GEM
+	rknpu_drm_remove(rknpu_dev);
+#endif
+#ifdef CONFIG_ROCKCHIP_RKNPU_DMA_HEAP
+	misc_deregister(&(rknpu_dev->miscdev));
+#endif
+
+	return ret;
+}
+
+static int rknpu_remove(struct platform_device *pdev)
+{
+	struct rknpu_device *rknpu_dev = platform_get_drvdata(pdev);
+	int i = 0;
+
+	cancel_delayed_work_sync(&rknpu_dev->power_off_work);
+	destroy_workqueue(rknpu_dev->power_off_wq);
+
+	if (IS_ENABLED(CONFIG_ROCKCHIP_RKNPU_SRAM) && rknpu_dev->sram_mm)
+		rknpu_mm_destroy(rknpu_dev->sram_mm);
+
+	rknpu_debugger_remove(rknpu_dev);
+	rknpu_cancel_timer(rknpu_dev);
+
+	for (i = 0; i < rknpu_dev->config->num_irqs; i++) {
+		WARN_ON(rknpu_dev->subcore_datas[i].job);
+		WARN_ON(!list_empty(&rknpu_dev->subcore_datas[i].todo_list));
+	}
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_DRM_GEM
+	rknpu_drm_remove(rknpu_dev);
+#endif
+#ifdef CONFIG_ROCKCHIP_RKNPU_DMA_HEAP
+	misc_deregister(&(rknpu_dev->miscdev));
+#endif
+
+#ifndef FPGA_PLATFORM
+	rknpu_devfreq_remove(rknpu_dev);
+#endif
+
+	mutex_lock(&rknpu_dev->power_lock);
+	if (atomic_read(&rknpu_dev->power_refcount) > 0)
+		rknpu_power_off(rknpu_dev);
+	mutex_unlock(&rknpu_dev->power_lock);
+
+	if (rknpu_dev->multiple_domains) {
+		if (rknpu_dev->genpd_dev_npu0)
+			dev_pm_domain_detach(rknpu_dev->genpd_dev_npu0, true);
+		if (rknpu_dev->genpd_dev_npu1)
+			dev_pm_domain_detach(rknpu_dev->genpd_dev_npu1, true);
+		if (rknpu_dev->genpd_dev_npu2)
+			dev_pm_domain_detach(rknpu_dev->genpd_dev_npu2, true);
+	}
+
+	pm_runtime_disable(&pdev->dev);
+
+	return 0;
+}
+
+#ifndef FPGA_PLATFORM
+static int rknpu_runtime_suspend(struct device *dev)
+{
+	return rknpu_devfreq_runtime_suspend(dev);
+}
+
+static int rknpu_runtime_resume(struct device *dev)
+{
+	return rknpu_devfreq_runtime_resume(dev);
+}
+
+static const struct dev_pm_ops rknpu_pm_ops = {
+	SET_SYSTEM_SLEEP_PM_OPS(pm_runtime_force_suspend,
+				pm_runtime_force_resume)
+		SET_RUNTIME_PM_OPS(rknpu_runtime_suspend, rknpu_runtime_resume,
+				   NULL)
+};
+#endif
+
+static struct platform_driver rknpu_driver = {
+	.probe = rknpu_probe,
+	.remove = rknpu_remove,
+	.driver = {
+		.owner = THIS_MODULE,
+		.name = "RKNPU",
+#ifndef FPGA_PLATFORM
+		.pm = &rknpu_pm_ops,
+#endif
+		.of_match_table = of_match_ptr(rknpu_of_match),
+	},
+};
+
+static int rknpu_init(void)
+{
+	return platform_driver_register(&rknpu_driver);
+}
+
+static void rknpu_exit(void)
+{
+	platform_driver_unregister(&rknpu_driver);
+}
+
+late_initcall(rknpu_init);
+module_exit(rknpu_exit);
+
+MODULE_DESCRIPTION("RKNPU driver");
+MODULE_AUTHOR("Felix Zeng <felix.zeng@rock-chips.com>");
+MODULE_ALIAS("rockchip-rknpu");
+MODULE_LICENSE("GPL v2");
+MODULE_VERSION(RKNPU_GET_DRV_VERSION_STRING(DRIVER_MAJOR, DRIVER_MINOR,
+					    DRIVER_PATCHLEVEL));
+#if KERNEL_VERSION(5, 16, 0) < LINUX_VERSION_CODE
+MODULE_IMPORT_NS(DMA_BUF);
+#endif
diff --git a/drivers/rknpu/rknpu_fence.c b/drivers/rknpu/rknpu_fence.c
new file mode 100644
index 000000000..dc22ea1c4
--- /dev/null
+++ b/drivers/rknpu/rknpu_fence.c
@@ -0,0 +1,80 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#include <linux/slab.h>
+#include <linux/file.h>
+#include <linux/dma-fence.h>
+#include <linux/sync_file.h>
+
+#include "rknpu_drv.h"
+#include "rknpu_job.h"
+
+#include "rknpu_fence.h"
+
+static const char *rknpu_fence_get_name(struct dma_fence *fence)
+{
+	return DRIVER_NAME;
+}
+
+static const struct dma_fence_ops rknpu_fence_ops = {
+	.get_driver_name = rknpu_fence_get_name,
+	.get_timeline_name = rknpu_fence_get_name,
+};
+
+int rknpu_fence_context_alloc(struct rknpu_device *rknpu_dev)
+{
+	struct rknpu_fence_context *fence_ctx = NULL;
+
+	fence_ctx =
+		devm_kzalloc(rknpu_dev->dev, sizeof(*fence_ctx), GFP_KERNEL);
+	if (!fence_ctx)
+		return -ENOMEM;
+
+	fence_ctx->context = dma_fence_context_alloc(1);
+	spin_lock_init(&fence_ctx->spinlock);
+
+	rknpu_dev->fence_ctx = fence_ctx;
+
+	return 0;
+}
+
+int rknpu_fence_alloc(struct rknpu_job *job)
+{
+	struct rknpu_fence_context *fence_ctx = job->rknpu_dev->fence_ctx;
+	struct dma_fence *fence = NULL;
+
+	fence = kzalloc(sizeof(*fence), GFP_KERNEL);
+	if (!fence)
+		return -ENOMEM;
+
+	dma_fence_init(fence, &rknpu_fence_ops, &fence_ctx->spinlock,
+		       fence_ctx->context, ++fence_ctx->seqno);
+
+	job->fence = fence;
+
+	return 0;
+}
+
+int rknpu_fence_get_fd(struct rknpu_job *job)
+{
+	struct sync_file *sync_file = NULL;
+	int fence_fd = -1;
+
+	if (!job->fence)
+		return -EINVAL;
+
+	fence_fd = get_unused_fd_flags(O_CLOEXEC);
+	if (fence_fd < 0)
+		return fence_fd;
+
+	sync_file = sync_file_create(job->fence);
+	if (!sync_file)
+		return -ENOMEM;
+
+	fd_install(fence_fd, sync_file->file);
+
+	return fence_fd;
+}
diff --git a/drivers/rknpu/rknpu_gem.c b/drivers/rknpu/rknpu_gem.c
new file mode 100644
index 000000000..533b31592
--- /dev/null
+++ b/drivers/rknpu/rknpu_gem.c
@@ -0,0 +1,1510 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#include "linux/mm.h"
+#include <drm/drm_device.h>
+#include <drm/drm_vma_manager.h>
+#include <drm/drm_prime.h>
+#include <drm/drm_file.h>
+#include <drm/drm_drv.h>
+
+#include <linux/shmem_fs.h>
+#include <linux/dma-buf.h>
+#include <linux/iommu.h>
+#include <linux/pfn_t.h>
+#include <linux/version.h>
+#include <asm/cacheflush.h>
+
+#if KERNEL_VERSION(5, 10, 0) <= LINUX_VERSION_CODE
+#include <linux/dma-map-ops.h>
+#endif
+
+#include "rknpu_drv.h"
+#include "rknpu_ioctl.h"
+#include "rknpu_gem.h"
+#include "rknpu_iommu.h"
+
+#define RKNPU_GEM_ALLOC_FROM_PAGES 1
+
+#if RKNPU_GEM_ALLOC_FROM_PAGES
+static int rknpu_gem_get_pages(struct rknpu_gem_object *rknpu_obj)
+{
+	struct drm_device *drm = rknpu_obj->base.dev;
+	struct scatterlist *s = NULL;
+	dma_addr_t dma_addr = 0;
+	dma_addr_t phys = 0;
+	int ret = -EINVAL, i = 0;
+	struct iommu_domain *domain = NULL;
+
+	rknpu_obj->pages = drm_gem_get_pages(&rknpu_obj->base);
+	if (IS_ERR(rknpu_obj->pages)) {
+		ret = PTR_ERR(rknpu_obj->pages);
+		LOG_ERROR("failed to get pages: %d\n", ret);
+		return ret;
+	}
+
+	rknpu_obj->num_pages = rknpu_obj->size >> PAGE_SHIFT;
+
+#if KERNEL_VERSION(5, 10, 0) <= LINUX_VERSION_CODE
+	rknpu_obj->sgt = drm_prime_pages_to_sg(drm, rknpu_obj->pages,
+					       rknpu_obj->num_pages);
+#else
+	rknpu_obj->sgt =
+		drm_prime_pages_to_sg(rknpu_obj->pages, rknpu_obj->num_pages);
+#endif
+	if (IS_ERR(rknpu_obj->sgt)) {
+		ret = PTR_ERR(rknpu_obj->sgt);
+		LOG_ERROR("failed to allocate sgt: %d\n", ret);
+		goto put_pages;
+	}
+
+	ret = dma_map_sg(drm->dev, rknpu_obj->sgt->sgl, rknpu_obj->sgt->nents,
+			 DMA_BIDIRECTIONAL);
+	if (ret == 0) {
+		ret = -EFAULT;
+		LOG_DEV_ERROR(drm->dev, "%s: dma map %zu fail\n", __func__,
+			      rknpu_obj->size);
+		goto free_sgt;
+	}
+
+	domain = iommu_get_domain_for_dev(drm->dev);
+	if (domain)
+		iommu_flush_iotlb_all(domain);
+
+	if (rknpu_obj->flags & RKNPU_MEM_KERNEL_MAPPING) {
+		rknpu_obj->cookie = vmap(rknpu_obj->pages, rknpu_obj->num_pages,
+					 VM_MAP, PAGE_KERNEL);
+		if (!rknpu_obj->cookie) {
+			ret = -ENOMEM;
+			LOG_ERROR("failed to vmap: %d\n", ret);
+			goto unmap_sg;
+		}
+		rknpu_obj->kv_addr = rknpu_obj->cookie;
+	}
+
+	dma_addr = sg_dma_address(rknpu_obj->sgt->sgl);
+	rknpu_obj->dma_addr = dma_addr;
+
+	for_each_sg(rknpu_obj->sgt->sgl, s, rknpu_obj->sgt->nents, i) {
+		dma_addr += s->length;
+		phys = sg_phys(s);
+		LOG_DEBUG(
+			"gem pages alloc sgt[%d], dma_address: %pad, length: %#x, phys: %pad, virt: %p\n",
+			i, &dma_addr, s->length, &phys, sg_virt(s));
+	}
+
+	return 0;
+
+unmap_sg:
+	dma_unmap_sg(drm->dev, rknpu_obj->sgt->sgl, rknpu_obj->sgt->nents,
+		     DMA_BIDIRECTIONAL);
+
+free_sgt:
+	sg_free_table(rknpu_obj->sgt);
+	kfree(rknpu_obj->sgt);
+
+put_pages:
+	drm_gem_put_pages(&rknpu_obj->base, rknpu_obj->pages, false, false);
+
+	return ret;
+}
+
+static void rknpu_gem_put_pages(struct rknpu_gem_object *rknpu_obj)
+{
+	struct drm_device *drm = rknpu_obj->base.dev;
+
+	if (rknpu_obj->flags & RKNPU_MEM_KERNEL_MAPPING) {
+		vunmap(rknpu_obj->kv_addr);
+		rknpu_obj->kv_addr = NULL;
+	}
+
+	if (rknpu_obj->sgt != NULL) {
+		dma_unmap_sg(drm->dev, rknpu_obj->sgt->sgl,
+			     rknpu_obj->sgt->nents, DMA_BIDIRECTIONAL);
+		sg_free_table(rknpu_obj->sgt);
+		kfree(rknpu_obj->sgt);
+	}
+
+	drm_gem_put_pages(&rknpu_obj->base, rknpu_obj->pages, true, true);
+}
+#endif
+
+static int rknpu_gem_alloc_buf(struct rknpu_gem_object *rknpu_obj)
+{
+	struct drm_device *drm = rknpu_obj->base.dev;
+	struct rknpu_device *rknpu_dev = drm->dev_private;
+	unsigned int nr_pages = 0;
+	struct sg_table *sgt = NULL;
+	struct scatterlist *s = NULL;
+	gfp_t gfp_mask = GFP_KERNEL;
+	int ret = -EINVAL, i = 0;
+
+	if (rknpu_obj->dma_addr) {
+		LOG_DEBUG("buffer already allocated.\n");
+		return 0;
+	}
+
+	rknpu_obj->dma_attrs = 0;
+
+	/*
+	 * if RKNPU_MEM_CONTIGUOUS, fully physically contiguous memory
+	 * region will be allocated else physically contiguous
+	 * as possible.
+	 */
+	if (!(rknpu_obj->flags & RKNPU_MEM_NON_CONTIGUOUS))
+		rknpu_obj->dma_attrs |= DMA_ATTR_FORCE_CONTIGUOUS;
+
+	// cacheable mapping or writecombine mapping
+	if (rknpu_obj->flags & RKNPU_MEM_CACHEABLE) {
+#ifdef DMA_ATTR_NON_CONSISTENT
+		rknpu_obj->dma_attrs |= DMA_ATTR_NON_CONSISTENT;
+#endif
+#ifdef DMA_ATTR_SYS_CACHE_ONLY
+		rknpu_obj->dma_attrs |= DMA_ATTR_SYS_CACHE_ONLY;
+#endif
+	} else if (rknpu_obj->flags & RKNPU_MEM_WRITE_COMBINE) {
+		rknpu_obj->dma_attrs |= DMA_ATTR_WRITE_COMBINE;
+	}
+
+	if (!(rknpu_obj->flags & RKNPU_MEM_KERNEL_MAPPING))
+		rknpu_obj->dma_attrs |= DMA_ATTR_NO_KERNEL_MAPPING;
+
+#ifdef DMA_ATTR_SKIP_ZEROING
+	if (!(rknpu_obj->flags & RKNPU_MEM_ZEROING))
+		rknpu_obj->dma_attrs |= DMA_ATTR_SKIP_ZEROING;
+#endif
+
+#if RKNPU_GEM_ALLOC_FROM_PAGES
+	if ((rknpu_obj->flags & RKNPU_MEM_NON_CONTIGUOUS) &&
+	    rknpu_dev->iommu_en) {
+		return rknpu_gem_get_pages(rknpu_obj);
+	}
+#endif
+
+	if (rknpu_obj->flags & RKNPU_MEM_ZEROING)
+		gfp_mask |= __GFP_ZERO;
+
+	if (!rknpu_dev->iommu_en ||
+	    rknpu_dev->config->dma_mask <= DMA_BIT_MASK(32) ||
+	    (rknpu_obj->flags & RKNPU_MEM_DMA32)) {
+		gfp_mask &= ~__GFP_HIGHMEM;
+		gfp_mask |= __GFP_DMA32;
+	}
+
+	nr_pages = rknpu_obj->size >> PAGE_SHIFT;
+
+	rknpu_obj->pages = rknpu_gem_alloc_page(nr_pages);
+	if (!rknpu_obj->pages) {
+		LOG_ERROR("failed to allocate pages.\n");
+		return -ENOMEM;
+	}
+
+	rknpu_obj->cookie =
+		dma_alloc_attrs(drm->dev, rknpu_obj->size, &rknpu_obj->dma_addr,
+				gfp_mask, rknpu_obj->dma_attrs);
+	if (!rknpu_obj->cookie) {
+		/*
+		 * when RKNPU_MEM_CONTIGUOUS and IOMMU is available
+		 * try to fallback to allocate non-contiguous buffer
+		 */
+		if (!(rknpu_obj->flags & RKNPU_MEM_NON_CONTIGUOUS) &&
+		    rknpu_dev->iommu_en) {
+			LOG_DEV_WARN(
+				drm->dev,
+				"try to fallback to allocate non-contiguous %lu buffer.\n",
+				rknpu_obj->size);
+			rknpu_obj->dma_attrs &= ~DMA_ATTR_FORCE_CONTIGUOUS;
+			rknpu_obj->flags |= RKNPU_MEM_NON_CONTIGUOUS;
+			rknpu_obj->cookie =
+				dma_alloc_attrs(drm->dev, rknpu_obj->size,
+						&rknpu_obj->dma_addr, gfp_mask,
+						rknpu_obj->dma_attrs);
+			if (!rknpu_obj->cookie) {
+				LOG_DEV_ERROR(
+					drm->dev,
+					"failed to allocate non-contiguous %lu buffer.\n",
+					rknpu_obj->size);
+				goto err_free;
+			}
+		} else {
+			LOG_DEV_ERROR(drm->dev,
+				      "failed to allocate %lu buffer.\n",
+				      rknpu_obj->size);
+			goto err_free;
+		}
+	}
+
+	if (rknpu_obj->flags & RKNPU_MEM_KERNEL_MAPPING)
+		rknpu_obj->kv_addr = rknpu_obj->cookie;
+
+	sgt = kzalloc(sizeof(*sgt), GFP_KERNEL);
+	if (!sgt) {
+		ret = -ENOMEM;
+		goto err_free_dma;
+	}
+
+	ret = dma_get_sgtable_attrs(drm->dev, sgt, rknpu_obj->cookie,
+				    rknpu_obj->dma_addr, rknpu_obj->size,
+				    rknpu_obj->dma_attrs);
+	if (ret < 0) {
+		LOG_DEV_ERROR(drm->dev, "failed to get sgtable.\n");
+		goto err_free_sgt;
+	}
+
+	for_each_sg(sgt->sgl, s, sgt->nents, i) {
+		sg_dma_address(s) = sg_phys(s);
+		LOG_DEBUG("dma alloc sgt[%d], phys_address: %pad, length: %u\n",
+			  i, &s->dma_address, s->length);
+	}
+
+#if KERNEL_VERSION(6, 1, 0) > LINUX_VERSION_CODE
+	ret = drm_prime_sg_to_page_addr_arrays(sgt, rknpu_obj->pages, NULL,
+					       nr_pages);
+#else
+	ret = drm_prime_sg_to_page_array(sgt, rknpu_obj->pages, nr_pages);
+#endif
+
+	if (ret < 0) {
+		LOG_DEV_ERROR(drm->dev, "invalid sgtable, ret: %d\n", ret);
+		goto err_free_sg_table;
+	}
+
+	rknpu_obj->sgt = sgt;
+
+	return ret;
+
+err_free_sg_table:
+	sg_free_table(sgt);
+err_free_sgt:
+	kfree(sgt);
+err_free_dma:
+	dma_free_attrs(drm->dev, rknpu_obj->size, rknpu_obj->cookie,
+		       rknpu_obj->dma_addr, rknpu_obj->dma_attrs);
+err_free:
+	rknpu_gem_free_page(rknpu_obj->pages);
+
+	return ret;
+}
+
+static void rknpu_gem_free_buf(struct rknpu_gem_object *rknpu_obj)
+{
+	struct drm_device *drm = rknpu_obj->base.dev;
+#if RKNPU_GEM_ALLOC_FROM_PAGES
+	struct rknpu_device *rknpu_dev = drm->dev_private;
+#endif
+
+	if (!rknpu_obj->dma_addr) {
+		LOG_DEBUG("dma handle is invalid.\n");
+		return;
+	}
+
+#if RKNPU_GEM_ALLOC_FROM_PAGES
+	if ((rknpu_obj->flags & RKNPU_MEM_NON_CONTIGUOUS) &&
+	    rknpu_dev->iommu_en) {
+		rknpu_gem_put_pages(rknpu_obj);
+		return;
+	}
+#endif
+
+	sg_free_table(rknpu_obj->sgt);
+	kfree(rknpu_obj->sgt);
+
+	dma_free_attrs(drm->dev, rknpu_obj->size, rknpu_obj->cookie,
+		       rknpu_obj->dma_addr, rknpu_obj->dma_attrs);
+
+	rknpu_gem_free_page(rknpu_obj->pages);
+
+	rknpu_obj->dma_addr = 0;
+}
+
+static int rknpu_gem_handle_create(struct drm_gem_object *obj,
+				   struct drm_file *file_priv,
+				   unsigned int *handle)
+{
+	int ret = -EINVAL;
+	/*
+	 * allocate a id of idr table where the obj is registered
+	 * and handle has the id what user can see.
+	 */
+	ret = drm_gem_handle_create(file_priv, obj, handle);
+	if (ret)
+		return ret;
+
+	LOG_DEBUG("gem handle: %#x\n", *handle);
+
+	/* drop reference from allocate - handle holds it now. */
+	rknpu_gem_object_put(obj);
+
+	return 0;
+}
+
+static int rknpu_gem_handle_destroy(struct drm_file *file_priv,
+				    unsigned int handle)
+{
+	return drm_gem_handle_delete(file_priv, handle);
+}
+
+#if KERNEL_VERSION(6, 1, 0) <= LINUX_VERSION_CODE
+static const struct vm_operations_struct vm_ops = {
+	.fault = rknpu_gem_fault,
+	.open = drm_gem_vm_open,
+	.close = drm_gem_vm_close,
+};
+
+static const struct drm_gem_object_funcs rknpu_gem_object_funcs = {
+	.free = rknpu_gem_free_object,
+	.export = drm_gem_prime_export,
+	.get_sg_table = rknpu_gem_prime_get_sg_table,
+	.vmap = rknpu_gem_prime_vmap,
+	.vunmap = rknpu_gem_prime_vunmap,
+	.mmap = rknpu_gem_mmap_obj,
+	.vm_ops = &vm_ops,
+};
+#endif
+
+static struct rknpu_gem_object *rknpu_gem_init(struct drm_device *drm,
+					       unsigned long size)
+{
+	struct rknpu_device *rknpu_dev = drm->dev_private;
+	struct rknpu_gem_object *rknpu_obj = NULL;
+	struct drm_gem_object *obj = NULL;
+	gfp_t gfp_mask;
+	int ret = -EINVAL;
+
+	rknpu_obj = kzalloc(sizeof(*rknpu_obj), GFP_KERNEL);
+	if (!rknpu_obj)
+		return ERR_PTR(-ENOMEM);
+
+	obj = &rknpu_obj->base;
+#if KERNEL_VERSION(6, 1, 0) <= LINUX_VERSION_CODE
+	obj->funcs = &rknpu_gem_object_funcs;
+#endif
+
+	ret = drm_gem_object_init(drm, obj, size);
+	if (ret < 0) {
+		LOG_DEV_ERROR(drm->dev, "failed to initialize gem object\n");
+		kfree(rknpu_obj);
+		return ERR_PTR(ret);
+	}
+
+	rknpu_obj->size = rknpu_obj->base.size;
+
+	gfp_mask = mapping_gfp_mask(obj->filp->f_mapping);
+
+	if (rknpu_obj->flags & RKNPU_MEM_ZEROING)
+		gfp_mask |= __GFP_ZERO;
+
+	if (!rknpu_dev->iommu_en ||
+	    rknpu_dev->config->dma_mask <= DMA_BIT_MASK(32) ||
+	    (rknpu_obj->flags & RKNPU_MEM_DMA32)) {
+		gfp_mask &= ~__GFP_HIGHMEM;
+		gfp_mask |= __GFP_DMA32;
+	}
+
+	mapping_set_gfp_mask(obj->filp->f_mapping, gfp_mask);
+
+	return rknpu_obj;
+}
+
+static void rknpu_gem_release(struct rknpu_gem_object *rknpu_obj)
+{
+	/* release file pointer to gem object. */
+	drm_gem_object_release(&rknpu_obj->base);
+	kfree(rknpu_obj);
+}
+
+static int rknpu_gem_alloc_buf_with_cache(struct rknpu_gem_object *rknpu_obj,
+					  enum rknpu_cache_type cache_type)
+{
+	struct drm_device *drm = rknpu_obj->base.dev;
+	struct rknpu_device *rknpu_dev = drm->dev_private;
+	struct iommu_domain *domain = NULL;
+	struct rknpu_iommu_dma_cookie *cookie = NULL;
+	struct iova_domain *iovad = NULL;
+	struct scatterlist *s = NULL;
+	unsigned long length = 0;
+	unsigned long size = 0;
+	unsigned long offset = 0;
+	int i = 0;
+	int ret = -EINVAL;
+	phys_addr_t cache_start = 0;
+	unsigned long cache_offset = 0;
+	unsigned long cache_size = 0;
+
+	switch (cache_type) {
+	case RKNPU_CACHE_SRAM:
+		cache_start = rknpu_dev->sram_start;
+		cache_offset = rknpu_obj->sram_obj->range_start *
+			       rknpu_dev->sram_mm->chunk_size;
+		cache_size = rknpu_obj->sram_size;
+		break;
+	case RKNPU_CACHE_NBUF:
+		cache_start = rknpu_dev->nbuf_start;
+		cache_offset = 0;
+		cache_size = rknpu_obj->nbuf_size;
+		break;
+	default:
+		LOG_ERROR("Unknown rknpu_cache_type: %d", cache_type);
+		return -EINVAL;
+	}
+
+	/* iova map to cache */
+	domain = iommu_get_domain_for_dev(rknpu_dev->dev);
+	if (!domain) {
+		LOG_ERROR("failed to get iommu domain!");
+		return -EINVAL;
+	}
+
+	cookie = (void *)domain->iova_cookie;
+	iovad = &cookie->iovad;
+	rknpu_obj->iova_size = iova_align(iovad, cache_size + rknpu_obj->size);
+	rknpu_obj->iova_start = rknpu_iommu_dma_alloc_iova(
+		domain, rknpu_obj->iova_size, dma_get_mask(drm->dev), drm->dev);
+	if (!rknpu_obj->iova_start) {
+		LOG_ERROR("iommu_dma_alloc_iova failed\n");
+		return -ENOMEM;
+	}
+
+	LOG_INFO("allocate iova start: %pad, size: %lu\n",
+		 &rknpu_obj->iova_start, rknpu_obj->iova_size);
+
+	/*
+	 * Overview cache + DDR map to IOVA
+	 * --------
+	 * cache_size:
+	 *   - allocate from CACHE, this size value has been page-aligned
+	 * size: rknpu_obj->size
+	 *   - allocate from DDR pages, this size value has been page-aligned
+	 * iova_size: rknpu_obj->iova_size
+	 *   - from iova_align(cache_size + size)
+	 *   - it may be larger than the (cache_size + size), and the larger part is not mapped
+	 * --------
+	 *
+	 * |<- cache_size ->|      |<- - - - size - - - ->|
+	 * +---------------+      +----------------------+
+	 * |     CACHE      |      |         DDR          |
+	 * +---------------+      +----------------------+
+	 *         |                    |
+	 * |       V       |            V          |
+	 * +---------------------------------------+
+	 * |             IOVA range                |
+	 * +---------------------------------------+
+	 * |<- - - - - - - iova_size - - - - - - ->|
+	 *
+	 */
+	ret = iommu_map(domain, rknpu_obj->iova_start,
+			cache_start + cache_offset, cache_size,
+			IOMMU_READ | IOMMU_WRITE, GFP_KERNEL);
+	if (ret) {
+		LOG_ERROR("cache iommu_map error: %d\n", ret);
+		goto free_iova;
+	}
+
+	rknpu_obj->dma_addr = rknpu_obj->iova_start;
+
+	if (rknpu_obj->size == 0) {
+		LOG_INFO("allocate cache size: %lu\n", cache_size);
+		return 0;
+	}
+
+	rknpu_obj->pages = drm_gem_get_pages(&rknpu_obj->base);
+	if (IS_ERR(rknpu_obj->pages)) {
+		ret = PTR_ERR(rknpu_obj->pages);
+		LOG_ERROR("failed to get pages: %d\n", ret);
+		goto cache_unmap;
+	}
+
+	rknpu_obj->num_pages = rknpu_obj->size >> PAGE_SHIFT;
+
+#if KERNEL_VERSION(5, 10, 0) <= LINUX_VERSION_CODE
+	rknpu_obj->sgt = drm_prime_pages_to_sg(drm, rknpu_obj->pages,
+					       rknpu_obj->num_pages);
+#else
+	rknpu_obj->sgt =
+		drm_prime_pages_to_sg(rknpu_obj->pages, rknpu_obj->num_pages);
+#endif
+	if (IS_ERR(rknpu_obj->sgt)) {
+		ret = PTR_ERR(rknpu_obj->sgt);
+		LOG_ERROR("failed to allocate sgt: %d\n", ret);
+		goto put_pages;
+	}
+
+	length = rknpu_obj->size;
+	offset = rknpu_obj->iova_start + cache_size;
+
+	for_each_sg(rknpu_obj->sgt->sgl, s, rknpu_obj->sgt->nents, i) {
+		size = (length < s->length) ? length : s->length;
+
+		ret = iommu_map(domain, offset, sg_phys(s), size,
+				IOMMU_READ | IOMMU_WRITE, GFP_KERNEL);
+		if (ret) {
+			LOG_ERROR("ddr iommu_map error: %d\n", ret);
+			goto sgl_unmap;
+		}
+
+		length -= size;
+		offset += size;
+
+		if (length == 0)
+			break;
+	}
+
+	LOG_INFO("allocate size: %lu with cache size: %lu\n", rknpu_obj->size,
+		 cache_size);
+
+	return 0;
+
+sgl_unmap:
+	iommu_unmap(domain, rknpu_obj->iova_start + cache_size,
+		    rknpu_obj->size - length);
+	sg_free_table(rknpu_obj->sgt);
+	kfree(rknpu_obj->sgt);
+
+put_pages:
+	drm_gem_put_pages(&rknpu_obj->base, rknpu_obj->pages, false, false);
+
+cache_unmap:
+	iommu_unmap(domain, rknpu_obj->iova_start, cache_size);
+
+free_iova:
+	rknpu_iommu_dma_free_iova((void *)domain->iova_cookie,
+				  rknpu_obj->iova_start, rknpu_obj->iova_size);
+
+	return ret;
+}
+
+static void rknpu_gem_free_buf_with_cache(struct rknpu_gem_object *rknpu_obj,
+					  enum rknpu_cache_type cache_type)
+{
+	struct drm_device *drm = rknpu_obj->base.dev;
+	struct rknpu_device *rknpu_dev = drm->dev_private;
+	struct iommu_domain *domain = NULL;
+	unsigned long cache_size = 0;
+
+	switch (cache_type) {
+	case RKNPU_CACHE_SRAM:
+		cache_size = rknpu_obj->sram_size;
+		break;
+	case RKNPU_CACHE_NBUF:
+		cache_size = rknpu_obj->nbuf_size;
+		break;
+	default:
+		LOG_ERROR("Unknown rknpu_cache_type: %d", cache_type);
+		return;
+	}
+
+	domain = iommu_get_domain_for_dev(rknpu_dev->dev);
+	if (domain) {
+		iommu_unmap(domain, rknpu_obj->iova_start, cache_size);
+		if (rknpu_obj->size > 0)
+			iommu_unmap(domain, rknpu_obj->iova_start + cache_size,
+				    rknpu_obj->size);
+		rknpu_iommu_dma_free_iova((void *)domain->iova_cookie,
+					  rknpu_obj->iova_start,
+					  rknpu_obj->iova_size);
+	}
+
+	if (rknpu_obj->pages)
+		drm_gem_put_pages(&rknpu_obj->base, rknpu_obj->pages, true,
+				  true);
+
+	if (rknpu_obj->sgt != NULL) {
+		sg_free_table(rknpu_obj->sgt);
+		kfree(rknpu_obj->sgt);
+	}
+}
+
+struct rknpu_gem_object *rknpu_gem_object_create(struct drm_device *drm,
+						 unsigned int flags,
+						 unsigned long size,
+						 unsigned long sram_size)
+{
+	struct rknpu_device *rknpu_dev = drm->dev_private;
+	struct rknpu_gem_object *rknpu_obj = NULL;
+	size_t remain_ddr_size = 0;
+	int ret = -EINVAL;
+
+	if (!size) {
+		LOG_DEV_ERROR(drm->dev, "invalid buffer size: %lu\n", size);
+		return ERR_PTR(-EINVAL);
+	}
+
+	remain_ddr_size = round_up(size, PAGE_SIZE);
+
+	if (!rknpu_dev->iommu_en && (flags & RKNPU_MEM_NON_CONTIGUOUS)) {
+		/*
+		 * when no IOMMU is available, all allocated buffers are
+		 * contiguous anyway, so drop RKNPU_MEM_NON_CONTIGUOUS flag
+		 */
+		flags &= ~RKNPU_MEM_NON_CONTIGUOUS;
+		LOG_WARN(
+			"non-contiguous allocation is not supported without IOMMU, falling back to contiguous buffer\n");
+	}
+
+	if (IS_ENABLED(CONFIG_ROCKCHIP_RKNPU_SRAM) &&
+	    (flags & RKNPU_MEM_TRY_ALLOC_SRAM) && rknpu_dev->sram_size > 0) {
+		size_t sram_free_size = 0;
+		size_t real_sram_size = 0;
+
+		if (sram_size != 0)
+			sram_size = round_up(sram_size, PAGE_SIZE);
+
+		rknpu_obj = rknpu_gem_init(drm, remain_ddr_size);
+		if (IS_ERR(rknpu_obj))
+			return rknpu_obj;
+
+		/* set memory type and cache attribute from user side. */
+		rknpu_obj->flags = flags;
+
+		sram_free_size = rknpu_dev->sram_mm->free_chunks *
+				 rknpu_dev->sram_mm->chunk_size;
+		if (sram_free_size > 0) {
+			real_sram_size = remain_ddr_size;
+			if (sram_size != 0 && remain_ddr_size > sram_size)
+				real_sram_size = sram_size;
+			if (real_sram_size > sram_free_size)
+				real_sram_size = sram_free_size;
+			ret = rknpu_mm_alloc(rknpu_dev->sram_mm, real_sram_size,
+					     &rknpu_obj->sram_obj);
+			if (ret != 0) {
+				sram_free_size =
+					rknpu_dev->sram_mm->free_chunks *
+					rknpu_dev->sram_mm->chunk_size;
+				LOG_WARN(
+					"mm allocate %zu failed, ret: %d, free size: %zu\n",
+					real_sram_size, ret, sram_free_size);
+				real_sram_size = 0;
+			}
+		}
+
+		if (real_sram_size > 0) {
+			rknpu_obj->sram_size = real_sram_size;
+
+			ret = rknpu_gem_alloc_buf_with_cache(rknpu_obj,
+							     RKNPU_CACHE_SRAM);
+			if (ret < 0)
+				goto mm_free;
+			remain_ddr_size = 0;
+		}
+	} else if (IS_ENABLED(CONFIG_NO_GKI) &&
+		   (flags & RKNPU_MEM_TRY_ALLOC_NBUF) &&
+		   rknpu_dev->nbuf_size > 0) {
+		size_t nbuf_size = 0;
+
+		rknpu_obj = rknpu_gem_init(drm, remain_ddr_size);
+		if (IS_ERR(rknpu_obj))
+			return rknpu_obj;
+
+		nbuf_size = remain_ddr_size <= rknpu_dev->nbuf_size ?
+				    remain_ddr_size :
+				    rknpu_dev->nbuf_size;
+
+		/* set memory type and cache attribute from user side. */
+		rknpu_obj->flags = flags;
+
+		if (nbuf_size > 0) {
+			rknpu_obj->nbuf_size = nbuf_size;
+
+			ret = rknpu_gem_alloc_buf_with_cache(rknpu_obj,
+							     RKNPU_CACHE_NBUF);
+			if (ret < 0)
+				goto gem_release;
+			remain_ddr_size = 0;
+		}
+	}
+
+	if (remain_ddr_size > 0) {
+		rknpu_obj = rknpu_gem_init(drm, remain_ddr_size);
+		if (IS_ERR(rknpu_obj))
+			return rknpu_obj;
+
+		/* set memory type and cache attribute from user side. */
+		rknpu_obj->flags = flags;
+
+		ret = rknpu_gem_alloc_buf(rknpu_obj);
+		if (ret < 0)
+			goto gem_release;
+	}
+
+	if (rknpu_obj)
+		LOG_DEBUG(
+			"created dma addr: %pad, cookie: %p, ddr size: %lu, sram size: %lu, nbuf size: %lu, attrs: %#lx, flags: %#x\n",
+			&rknpu_obj->dma_addr, rknpu_obj->cookie,
+			rknpu_obj->size, rknpu_obj->sram_size,
+			rknpu_obj->nbuf_size, rknpu_obj->dma_attrs,
+			rknpu_obj->flags);
+
+	return rknpu_obj;
+
+mm_free:
+	if (IS_ENABLED(CONFIG_ROCKCHIP_RKNPU_SRAM) &&
+	    rknpu_obj->sram_obj != NULL)
+		rknpu_mm_free(rknpu_dev->sram_mm, rknpu_obj->sram_obj);
+
+gem_release:
+	rknpu_gem_release(rknpu_obj);
+
+	return ERR_PTR(ret);
+}
+
+void rknpu_gem_object_destroy(struct rknpu_gem_object *rknpu_obj)
+{
+	struct drm_gem_object *obj = &rknpu_obj->base;
+
+	LOG_DEBUG(
+		"destroy dma addr: %pad, cookie: %p, size: %lu, attrs: %#lx, flags: %#x, handle count: %d\n",
+		&rknpu_obj->dma_addr, rknpu_obj->cookie, rknpu_obj->size,
+		rknpu_obj->dma_attrs, rknpu_obj->flags, obj->handle_count);
+
+	/*
+	 * do not release memory region from exporter.
+	 *
+	 * the region will be released by exporter
+	 * once dmabuf's refcount becomes 0.
+	 */
+	if (obj->import_attach) {
+		drm_prime_gem_destroy(obj, rknpu_obj->sgt);
+		rknpu_gem_free_page(rknpu_obj->pages);
+	} else {
+		if (IS_ENABLED(CONFIG_ROCKCHIP_RKNPU_SRAM) &&
+		    rknpu_obj->sram_size > 0) {
+			struct rknpu_device *rknpu_dev = obj->dev->dev_private;
+
+			if (rknpu_obj->sram_obj != NULL)
+				rknpu_mm_free(rknpu_dev->sram_mm,
+					      rknpu_obj->sram_obj);
+			rknpu_gem_free_buf_with_cache(rknpu_obj,
+						      RKNPU_CACHE_SRAM);
+		} else if (IS_ENABLED(CONFIG_NO_GKI) &&
+			   rknpu_obj->nbuf_size > 0) {
+			rknpu_gem_free_buf_with_cache(rknpu_obj,
+						      RKNPU_CACHE_NBUF);
+		} else {
+			rknpu_gem_free_buf(rknpu_obj);
+		}
+	}
+
+	rknpu_gem_release(rknpu_obj);
+}
+
+int rknpu_gem_create_ioctl(struct drm_device *dev, void *data,
+			   struct drm_file *file_priv)
+{
+	struct rknpu_mem_create *args = data;
+	struct rknpu_gem_object *rknpu_obj = NULL;
+	int ret = -EINVAL;
+
+	rknpu_obj = rknpu_gem_object_find(file_priv, args->handle);
+	if (!rknpu_obj) {
+		rknpu_obj = rknpu_gem_object_create(
+			dev, args->flags, args->size, args->sram_size);
+		if (IS_ERR(rknpu_obj))
+			return PTR_ERR(rknpu_obj);
+
+		ret = rknpu_gem_handle_create(&rknpu_obj->base, file_priv,
+					      &args->handle);
+		if (ret) {
+			rknpu_gem_object_destroy(rknpu_obj);
+			return ret;
+		}
+	}
+
+	// rknpu_gem_object_get(&rknpu_obj->base);
+
+	args->size = rknpu_obj->size;
+	args->sram_size = rknpu_obj->sram_size;
+	args->obj_addr = (__u64)(uintptr_t)rknpu_obj;
+	args->dma_addr = rknpu_obj->dma_addr;
+
+	return 0;
+}
+
+int rknpu_gem_map_ioctl(struct drm_device *dev, void *data,
+			struct drm_file *file_priv)
+{
+	struct rknpu_mem_map *args = data;
+
+#if KERNEL_VERSION(4, 19, 0) > LINUX_VERSION_CODE
+	return rknpu_gem_dumb_map_offset(file_priv, dev, args->handle,
+					 &args->offset);
+#else
+	return drm_gem_dumb_map_offset(file_priv, dev, args->handle,
+				       &args->offset);
+#endif
+}
+
+int rknpu_gem_destroy_ioctl(struct drm_device *dev, void *data,
+			    struct drm_file *file_priv)
+{
+	struct rknpu_gem_object *rknpu_obj = NULL;
+	struct rknpu_mem_destroy *args = data;
+
+	rknpu_obj = rknpu_gem_object_find(file_priv, args->handle);
+	if (!rknpu_obj)
+		return -EINVAL;
+
+	// rknpu_gem_object_put(&rknpu_obj->base);
+
+	return rknpu_gem_handle_destroy(file_priv, args->handle);
+}
+
+#if RKNPU_GEM_ALLOC_FROM_PAGES
+/*
+ * __vm_map_pages - maps range of kernel pages into user vma
+ * @vma: user vma to map to
+ * @pages: pointer to array of source kernel pages
+ * @num: number of pages in page array
+ * @offset: user's requested vm_pgoff
+ *
+ * This allows drivers to map range of kernel pages into a user vma.
+ *
+ * Return: 0 on success and error code otherwise.
+ */
+static int __vm_map_pages(struct vm_area_struct *vma, struct page **pages,
+			  unsigned long num, unsigned long offset)
+{
+	unsigned long count = vma_pages(vma);
+	unsigned long uaddr = vma->vm_start;
+	int ret = -EINVAL, i = 0;
+
+	/* Fail if the user requested offset is beyond the end of the object */
+	if (offset >= num)
+		return -ENXIO;
+
+	/* Fail if the user requested size exceeds available object size */
+	if (count > num - offset)
+		return -ENXIO;
+
+	for (i = 0; i < count; i++) {
+		ret = vm_insert_page(vma, uaddr, pages[offset + i]);
+		if (ret < 0)
+			return ret;
+		uaddr += PAGE_SIZE;
+	}
+
+	return 0;
+}
+
+static int rknpu_gem_mmap_pages(struct rknpu_gem_object *rknpu_obj,
+				struct vm_area_struct *vma)
+{
+	struct drm_device *drm = rknpu_obj->base.dev;
+	int ret = -EINVAL;
+
+	vm_flags_set(vma, VM_MIXEDMAP);
+
+	ret = __vm_map_pages(vma, rknpu_obj->pages, rknpu_obj->num_pages,
+			     vma->vm_pgoff);
+	if (ret < 0)
+		LOG_DEV_ERROR(drm->dev, "failed to map pages into vma: %d\n",
+			      ret);
+
+	return ret;
+}
+#endif
+
+static int rknpu_gem_mmap_cache(struct rknpu_gem_object *rknpu_obj,
+				struct vm_area_struct *vma,
+				enum rknpu_cache_type cache_type)
+{
+	struct drm_device *drm = rknpu_obj->base.dev;
+#if RKNPU_GEM_ALLOC_FROM_PAGES
+	struct rknpu_device *rknpu_dev = drm->dev_private;
+#endif
+	unsigned long vm_size = 0;
+	int ret = -EINVAL;
+	unsigned long offset = 0;
+	unsigned long num_pages = 0;
+	int i = 0;
+	phys_addr_t cache_start = 0;
+	unsigned long cache_offset = 0;
+	unsigned long cache_size = 0;
+
+	switch (cache_type) {
+	case RKNPU_CACHE_SRAM:
+		cache_start = rknpu_dev->sram_start;
+		cache_offset = rknpu_obj->sram_obj->range_start *
+			       rknpu_dev->sram_mm->chunk_size;
+		cache_size = rknpu_obj->sram_size;
+		break;
+	case RKNPU_CACHE_NBUF:
+		cache_start = rknpu_dev->nbuf_start;
+		cache_offset = 0;
+		cache_size = rknpu_obj->nbuf_size;
+		break;
+	default:
+		LOG_ERROR("Unknown rknpu_cache_type: %d", cache_type);
+		return -EINVAL;
+	}
+
+	vm_flags_set(vma, VM_MIXEDMAP);
+
+	vm_size = vma->vm_end - vma->vm_start;
+
+	/*
+	 * Convert a physical address in a cache area to a page frame number (PFN),
+	 * and store the resulting PFN in the vm_pgoff field of the given VMA.
+	 *
+	 * NOTE: This conversion carries a risk because the resulting PFN is not a true
+	 * page frame number and may not be valid or usable in all contexts.
+	 */
+	vma->vm_pgoff = __phys_to_pfn(cache_start + cache_offset);
+
+	ret = remap_pfn_range(vma, vma->vm_start, vma->vm_pgoff, cache_size,
+			      vma->vm_page_prot);
+	if (ret)
+		return -EAGAIN;
+
+	if (rknpu_obj->size == 0)
+		return 0;
+
+	offset = cache_size;
+
+	num_pages = (vm_size - cache_size) / PAGE_SIZE;
+	for (i = 0; i < num_pages; ++i) {
+		ret = vm_insert_page(vma, vma->vm_start + offset,
+				     rknpu_obj->pages[i]);
+		if (ret < 0)
+			return ret;
+		offset += PAGE_SIZE;
+	}
+
+	return 0;
+}
+
+static int rknpu_gem_mmap_buffer(struct rknpu_gem_object *rknpu_obj,
+				 struct vm_area_struct *vma)
+{
+	struct drm_device *drm = rknpu_obj->base.dev;
+#if RKNPU_GEM_ALLOC_FROM_PAGES
+	struct rknpu_device *rknpu_dev = drm->dev_private;
+#endif
+	unsigned long vm_size = 0;
+	int ret = -EINVAL;
+
+	/*
+	 * clear the VM_PFNMAP flag that was set by drm_gem_mmap(), and set the
+	 * vm_pgoff (used as a fake buffer offset by DRM) to 0 as we want to map
+	 * the whole buffer.
+	 */
+	vm_flags_set(vma, VM_DONTCOPY | VM_DONTEXPAND | VM_DONTDUMP | VM_IO);
+	vm_flags_clear(vma, VM_PFNMAP);
+	vma->vm_pgoff = 0;
+
+	vm_size = vma->vm_end - vma->vm_start;
+
+	/* check if user-requested size is valid. */
+	if (vm_size > rknpu_obj->size)
+		return -EINVAL;
+
+	if (rknpu_obj->sram_size > 0)
+		return rknpu_gem_mmap_cache(rknpu_obj, vma, RKNPU_CACHE_SRAM);
+	else if (rknpu_obj->nbuf_size > 0)
+		return rknpu_gem_mmap_cache(rknpu_obj, vma, RKNPU_CACHE_NBUF);
+
+#if RKNPU_GEM_ALLOC_FROM_PAGES
+	if ((rknpu_obj->flags & RKNPU_MEM_NON_CONTIGUOUS) &&
+	    rknpu_dev->iommu_en) {
+		return rknpu_gem_mmap_pages(rknpu_obj, vma);
+	}
+#endif
+
+	ret = dma_mmap_attrs(drm->dev, vma, rknpu_obj->cookie,
+			     rknpu_obj->dma_addr, rknpu_obj->size,
+			     rknpu_obj->dma_attrs);
+	if (ret < 0) {
+		LOG_DEV_ERROR(drm->dev, "failed to mmap, ret: %d\n", ret);
+		return ret;
+	}
+
+	return 0;
+}
+
+void rknpu_gem_free_object(struct drm_gem_object *obj)
+{
+	rknpu_gem_object_destroy(to_rknpu_obj(obj));
+}
+
+int rknpu_gem_dumb_create(struct drm_file *file_priv, struct drm_device *drm,
+			  struct drm_mode_create_dumb *args)
+{
+	struct rknpu_device *rknpu_dev = drm->dev_private;
+	struct rknpu_gem_object *rknpu_obj = NULL;
+	unsigned int flags = 0;
+	int ret = -EINVAL;
+
+	/*
+	 * allocate memory to be used for framebuffer.
+	 * - this callback would be called by user application
+	 *	with DRM_IOCTL_MODE_CREATE_DUMB command.
+	 */
+	args->pitch = args->width * ((args->bpp + 7) / 8);
+	args->size = args->pitch * args->height;
+
+	if (rknpu_dev->iommu_en)
+		flags = RKNPU_MEM_NON_CONTIGUOUS | RKNPU_MEM_WRITE_COMBINE;
+	else
+		flags = RKNPU_MEM_CONTIGUOUS | RKNPU_MEM_WRITE_COMBINE;
+
+	rknpu_obj = rknpu_gem_object_create(drm, flags, args->size, 0);
+	if (IS_ERR(rknpu_obj)) {
+		LOG_DEV_ERROR(drm->dev, "gem object allocate failed.\n");
+		return PTR_ERR(rknpu_obj);
+	}
+
+	ret = rknpu_gem_handle_create(&rknpu_obj->base, file_priv,
+				      &args->handle);
+	if (ret) {
+		rknpu_gem_object_destroy(rknpu_obj);
+		return ret;
+	}
+
+	return 0;
+}
+
+#if KERNEL_VERSION(4, 19, 0) > LINUX_VERSION_CODE
+int rknpu_gem_dumb_map_offset(struct drm_file *file_priv,
+			      struct drm_device *drm, uint32_t handle,
+			      uint64_t *offset)
+{
+	struct rknpu_gem_object *rknpu_obj = NULL;
+	struct drm_gem_object *obj = NULL;
+	int ret = -EINVAL;
+
+	rknpu_obj = rknpu_gem_object_find(file_priv, handle);
+	if (!rknpu_obj)
+		return 0;
+
+	/* Don't allow imported objects to be mapped */
+	obj = &rknpu_obj->base;
+	if (obj->import_attach)
+		return -EINVAL;
+
+	ret = drm_gem_create_mmap_offset(obj);
+	if (ret)
+		return ret;
+
+	*offset = drm_vma_node_offset_addr(&obj->vma_node);
+
+	return 0;
+}
+#endif
+
+#if KERNEL_VERSION(4, 15, 0) <= LINUX_VERSION_CODE
+vm_fault_t rknpu_gem_fault(struct vm_fault *vmf)
+{
+	struct vm_area_struct *vma = vmf->vma;
+	struct drm_gem_object *obj = vma->vm_private_data;
+	struct rknpu_gem_object *rknpu_obj = to_rknpu_obj(obj);
+	struct drm_device *drm = rknpu_obj->base.dev;
+	unsigned long pfn = 0;
+	pgoff_t page_offset = 0;
+
+	page_offset = (vmf->address - vma->vm_start) >> PAGE_SHIFT;
+
+	if (page_offset >= (rknpu_obj->size >> PAGE_SHIFT)) {
+		LOG_DEV_ERROR(drm->dev, "invalid page offset\n");
+		return VM_FAULT_SIGBUS;
+	}
+
+	pfn = page_to_pfn(rknpu_obj->pages[page_offset]);
+	return vmf_insert_mixed(vma, vmf->address,
+				__pfn_to_pfn_t(pfn, PFN_DEV));
+}
+#elif KERNEL_VERSION(4, 14, 0) <= LINUX_VERSION_CODE
+int rknpu_gem_fault(struct vm_fault *vmf)
+{
+	struct vm_area_struct *vma = vmf->vma;
+	struct drm_gem_object *obj = vma->vm_private_data;
+	struct rknpu_gem_object *rknpu_obj = to_rknpu_obj(obj);
+	struct drm_device *drm = rknpu_obj->base.dev;
+	unsigned long pfn = 0;
+	pgoff_t page_offset = 0;
+	int ret = -EINVAL;
+
+	page_offset = (vmf->address - vma->vm_start) >> PAGE_SHIFT;
+
+	if (page_offset >= (rknpu_obj->size >> PAGE_SHIFT)) {
+		LOG_DEV_ERROR(drm->dev, "invalid page offset\n");
+		ret = -EINVAL;
+		goto out;
+	}
+
+	pfn = page_to_pfn(rknpu_obj->pages[page_offset]);
+	ret = vm_insert_mixed(vma, vmf->address, __pfn_to_pfn_t(pfn, PFN_DEV));
+
+out:
+	switch (ret) {
+	case 0:
+	case -ERESTARTSYS:
+	case -EINTR:
+		return VM_FAULT_NOPAGE;
+	case -ENOMEM:
+		return VM_FAULT_OOM;
+	default:
+		return VM_FAULT_SIGBUS;
+	}
+}
+#else
+int rknpu_gem_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
+{
+	struct drm_gem_object *obj = vma->vm_private_data;
+	struct rknpu_gem_object *rknpu_obj = to_rknpu_obj(obj);
+	struct drm_device *drm = rknpu_obj->base.dev;
+	unsigned long pfn = 0;
+	pgoff_t page_offset = 0;
+	int ret = -EINVAL;
+
+	page_offset = ((unsigned long)vmf->virtual_address - vma->vm_start) >>
+		      PAGE_SHIFT;
+
+	if (page_offset >= (rknpu_obj->size >> PAGE_SHIFT)) {
+		LOG_DEV_ERROR(drm->dev, "invalid page offset\n");
+		ret = -EINVAL;
+		goto out;
+	}
+
+	pfn = page_to_pfn(rknpu_obj->pages[page_offset]);
+	ret = vm_insert_mixed(vma, (unsigned long)vmf->virtual_address,
+			      __pfn_to_pfn_t(pfn, PFN_DEV));
+
+out:
+	switch (ret) {
+	case 0:
+	case -ERESTARTSYS:
+	case -EINTR:
+		return VM_FAULT_NOPAGE;
+	case -ENOMEM:
+		return VM_FAULT_OOM;
+	default:
+		return VM_FAULT_SIGBUS;
+	}
+}
+#endif
+
+int rknpu_gem_mmap_obj(struct drm_gem_object *obj, struct vm_area_struct *vma)
+{
+	struct rknpu_gem_object *rknpu_obj = to_rknpu_obj(obj);
+	int ret = -EINVAL;
+
+	LOG_DEBUG("flags: %#x\n", rknpu_obj->flags);
+
+	/* non-cacheable as default. */
+	if (rknpu_obj->flags & RKNPU_MEM_CACHEABLE) {
+		vma->vm_page_prot = vm_get_page_prot(vma->vm_flags);
+	} else if (rknpu_obj->flags & RKNPU_MEM_WRITE_COMBINE) {
+		vma->vm_page_prot =
+			pgprot_writecombine(vm_get_page_prot(vma->vm_flags));
+	} else {
+		vma->vm_page_prot =
+			pgprot_noncached(vm_get_page_prot(vma->vm_flags));
+	}
+
+	ret = rknpu_gem_mmap_buffer(rknpu_obj, vma);
+	if (ret)
+		goto err_close_vm;
+
+	return 0;
+
+err_close_vm:
+	drm_gem_vm_close(vma);
+
+	return ret;
+}
+
+int rknpu_gem_mmap(struct file *filp, struct vm_area_struct *vma)
+{
+	struct drm_gem_object *obj = NULL;
+	int ret = -EINVAL;
+
+	/* set vm_area_struct. */
+	ret = drm_gem_mmap(filp, vma);
+	if (ret < 0) {
+		LOG_ERROR("failed to mmap, ret: %d\n", ret);
+		return ret;
+	}
+
+	obj = vma->vm_private_data;
+
+	if (obj->import_attach)
+		return dma_buf_mmap(obj->dma_buf, vma, 0);
+
+	return rknpu_gem_mmap_obj(obj, vma);
+}
+
+/* low-level interface prime helpers */
+#if KERNEL_VERSION(4, 13, 0) <= LINUX_VERSION_CODE
+struct drm_gem_object *rknpu_gem_prime_import(struct drm_device *dev,
+					      struct dma_buf *dma_buf)
+{
+	return drm_gem_prime_import_dev(dev, dma_buf, dev->dev);
+}
+#endif
+
+struct sg_table *rknpu_gem_prime_get_sg_table(struct drm_gem_object *obj)
+{
+	struct rknpu_gem_object *rknpu_obj = to_rknpu_obj(obj);
+	int npages = 0;
+
+	npages = rknpu_obj->size >> PAGE_SHIFT;
+
+#if KERNEL_VERSION(5, 10, 0) <= LINUX_VERSION_CODE
+	return drm_prime_pages_to_sg(obj->dev, rknpu_obj->pages, npages);
+#else
+	return drm_prime_pages_to_sg(rknpu_obj->pages, npages);
+#endif
+}
+
+struct drm_gem_object *
+rknpu_gem_prime_import_sg_table(struct drm_device *dev,
+				struct dma_buf_attachment *attach,
+				struct sg_table *sgt)
+{
+	struct rknpu_gem_object *rknpu_obj = NULL;
+	int npages = 0;
+	int ret = -EINVAL;
+
+	rknpu_obj = rknpu_gem_init(dev, attach->dmabuf->size);
+	if (IS_ERR(rknpu_obj)) {
+		ret = PTR_ERR(rknpu_obj);
+		return ERR_PTR(ret);
+	}
+
+	rknpu_obj->dma_addr = sg_dma_address(sgt->sgl);
+
+	npages = rknpu_obj->size >> PAGE_SHIFT;
+	rknpu_obj->pages = rknpu_gem_alloc_page(npages);
+	if (!rknpu_obj->pages) {
+		ret = -ENOMEM;
+		goto err;
+	}
+
+#if KERNEL_VERSION(6, 1, 0) > LINUX_VERSION_CODE
+	ret = drm_prime_sg_to_page_addr_arrays(sgt, rknpu_obj->pages, NULL,
+					       npages);
+#else
+	ret = drm_prime_sg_to_page_array(sgt, rknpu_obj->pages, npages);
+#endif
+	if (ret < 0)
+		goto err_free_large;
+
+	rknpu_obj->sgt = sgt;
+
+	if (sgt->nents == 1) {
+		/* always physically continuous memory if sgt->nents is 1. */
+		rknpu_obj->flags |= RKNPU_MEM_CONTIGUOUS;
+	} else {
+		/*
+		 * this case could be CONTIG or NONCONTIG type but for now
+		 * sets NONCONTIG.
+		 * TODO. we have to find a way that exporter can notify
+		 * the type of its own buffer to importer.
+		 */
+		rknpu_obj->flags |= RKNPU_MEM_NON_CONTIGUOUS;
+	}
+
+	return &rknpu_obj->base;
+
+err_free_large:
+	rknpu_gem_free_page(rknpu_obj->pages);
+err:
+	rknpu_gem_release(rknpu_obj);
+	return ERR_PTR(ret);
+}
+
+#if KERNEL_VERSION(6, 1, 0) > LINUX_VERSION_CODE
+void *rknpu_gem_prime_vmap(struct drm_gem_object *obj)
+{
+	struct rknpu_gem_object *rknpu_obj = to_rknpu_obj(obj);
+
+	if (!rknpu_obj->pages)
+		return NULL;
+
+	return vmap(rknpu_obj->pages, rknpu_obj->num_pages, VM_MAP,
+		    PAGE_KERNEL);
+}
+
+void rknpu_gem_prime_vunmap(struct drm_gem_object *obj, void *vaddr)
+{
+	vunmap(vaddr);
+}
+#else
+int rknpu_gem_prime_vmap(struct drm_gem_object *obj, struct iosys_map *map)
+{
+	struct rknpu_gem_object *rknpu_obj = to_rknpu_obj(obj);
+	void *vaddr = NULL;
+
+	if (!rknpu_obj->pages)
+		return -EINVAL;
+
+	vaddr = vmap(rknpu_obj->pages, rknpu_obj->num_pages, VM_MAP,
+			  PAGE_KERNEL);
+	if (!vaddr)
+		return -ENOMEM;
+
+	iosys_map_set_vaddr(map, vaddr);
+
+	return 0;
+}
+
+void rknpu_gem_prime_vunmap(struct drm_gem_object *obj, struct iosys_map *map)
+{
+	struct rknpu_gem_object *rknpu_obj = to_rknpu_obj(obj);
+
+	if (rknpu_obj->pages) {
+		vunmap(map->vaddr);
+		map->vaddr = NULL;
+	}
+}
+#endif
+
+int rknpu_gem_prime_mmap(struct drm_gem_object *obj, struct vm_area_struct *vma)
+{
+	int ret = -EINVAL;
+
+	ret = drm_gem_mmap_obj(obj, obj->size, vma);
+	if (ret < 0)
+		return ret;
+
+	return rknpu_gem_mmap_obj(obj, vma);
+}
+
+static int rknpu_cache_sync(struct rknpu_gem_object *rknpu_obj,
+			    unsigned long *length, unsigned long *offset,
+			    enum rknpu_cache_type cache_type)
+{
+#if KERNEL_VERSION(6, 1, 0) > LINUX_VERSION_CODE
+	struct drm_gem_object *obj = &rknpu_obj->base;
+	struct rknpu_device *rknpu_dev = obj->dev->dev_private;
+	void __iomem *cache_base_io = NULL;
+	unsigned long cache_offset = 0;
+	unsigned long cache_size = 0;
+
+	switch (cache_type) {
+	case RKNPU_CACHE_SRAM:
+		cache_base_io = rknpu_dev->sram_base_io;
+		cache_offset = rknpu_obj->sram_obj->range_start *
+			       rknpu_dev->sram_mm->chunk_size;
+		cache_size = rknpu_obj->sram_size;
+		break;
+	case RKNPU_CACHE_NBUF:
+		cache_base_io = rknpu_dev->nbuf_base_io;
+		cache_offset = 0;
+		cache_size = rknpu_obj->nbuf_size;
+		break;
+	default:
+		LOG_ERROR("Unknown rknpu_cache_type: %d", cache_type);
+		return -EINVAL;
+	}
+
+	if ((*offset + *length) <= cache_size) {
+		__dma_map_area(cache_base_io + *offset + cache_offset, *length,
+			       DMA_TO_DEVICE);
+		__dma_unmap_area(cache_base_io + *offset + cache_offset,
+				 *length, DMA_FROM_DEVICE);
+		*length = 0;
+		*offset = 0;
+	} else if (*offset >= cache_size) {
+		*offset -= cache_size;
+	} else {
+		unsigned long cache_length = cache_size - *offset;
+
+		__dma_map_area(cache_base_io + *offset + cache_offset,
+			       cache_length, DMA_TO_DEVICE);
+		__dma_unmap_area(cache_base_io + *offset + cache_offset,
+				 cache_length, DMA_FROM_DEVICE);
+		*length -= cache_length;
+		*offset = 0;
+	}
+#endif
+
+	return 0;
+}
+
+int rknpu_gem_sync_ioctl(struct drm_device *dev, void *data,
+			 struct drm_file *file_priv)
+{
+	struct rknpu_gem_object *rknpu_obj = NULL;
+	struct rknpu_device *rknpu_dev = dev->dev_private;
+	struct rknpu_mem_sync *args = data;
+	struct scatterlist *sg;
+	dma_addr_t sg_phys_addr;
+	unsigned long length, offset = 0;
+	unsigned long sg_offset, sg_left, size = 0;
+	unsigned long len = 0;
+	int i;
+
+	rknpu_obj = (struct rknpu_gem_object *)(uintptr_t)args->obj_addr;
+	if (!rknpu_obj)
+		return -EINVAL;
+
+	if (!(rknpu_obj->flags & RKNPU_MEM_CACHEABLE))
+		return -EINVAL;
+
+	if (!(rknpu_obj->flags & RKNPU_MEM_NON_CONTIGUOUS)) {
+		if (args->flags & RKNPU_MEM_SYNC_TO_DEVICE) {
+			dma_sync_single_range_for_device(
+				dev->dev, rknpu_obj->dma_addr, args->offset,
+				args->size, DMA_TO_DEVICE);
+		}
+		if (args->flags & RKNPU_MEM_SYNC_FROM_DEVICE) {
+			dma_sync_single_range_for_cpu(dev->dev,
+						      rknpu_obj->dma_addr,
+						      args->offset, args->size,
+						      DMA_FROM_DEVICE);
+		}
+	} else {
+		WARN_ON(!rknpu_dev->fake_dev);
+
+		length = args->size;
+		offset = args->offset;
+
+		if (IS_ENABLED(CONFIG_NO_GKI) &&
+		    IS_ENABLED(CONFIG_ROCKCHIP_RKNPU_SRAM) &&
+		    rknpu_obj->sram_size > 0) {
+			rknpu_cache_sync(rknpu_obj, &length, &offset,
+					 RKNPU_CACHE_SRAM);
+		} else if (IS_ENABLED(CONFIG_NO_GKI) &&
+			   rknpu_obj->nbuf_size > 0) {
+			rknpu_cache_sync(rknpu_obj, &length, &offset,
+					 RKNPU_CACHE_NBUF);
+		}
+
+		for_each_sg(rknpu_obj->sgt->sgl, sg, rknpu_obj->sgt->nents,
+			     i) {
+			if (length == 0)
+				break;
+
+			len += sg->length;
+			if (len <= offset)
+				continue;
+
+			sg_phys_addr = sg_phys(sg);
+
+			sg_left = len - offset;
+			sg_offset = sg->length - sg_left;
+
+			size = (length < sg_left) ? length : sg_left;
+
+			if (args->flags & RKNPU_MEM_SYNC_TO_DEVICE) {
+				dma_sync_single_range_for_device(
+					rknpu_dev->fake_dev, sg_phys_addr,
+					sg_offset, size, DMA_TO_DEVICE);
+			}
+
+			if (args->flags & RKNPU_MEM_SYNC_FROM_DEVICE) {
+				dma_sync_single_range_for_cpu(
+					rknpu_dev->fake_dev, sg_phys_addr,
+					sg_offset, size, DMA_FROM_DEVICE);
+			}
+
+			offset += size;
+			length -= size;
+		}
+	}
+
+	return 0;
+}
diff --git a/drivers/rknpu/rknpu_iommu.c b/drivers/rknpu/rknpu_iommu.c
new file mode 100644
index 000000000..01620d9c3
--- /dev/null
+++ b/drivers/rknpu/rknpu_iommu.c
@@ -0,0 +1,61 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#include "rknpu_iommu.h"
+
+dma_addr_t rknpu_iommu_dma_alloc_iova(struct iommu_domain *domain, size_t size,
+				      u64 dma_limit, struct device *dev)
+{
+	struct rknpu_iommu_dma_cookie *cookie = (void *)domain->iova_cookie;
+	struct iova_domain *iovad = &cookie->iovad;
+	unsigned long shift, iova_len, iova = 0;
+#if (KERNEL_VERSION(5, 4, 0) > LINUX_VERSION_CODE)
+	dma_addr_t limit;
+#endif
+
+	shift = iova_shift(iovad);
+	iova_len = size >> shift;
+
+#if KERNEL_VERSION(6, 1, 0) > LINUX_VERSION_CODE
+	/*
+	 * Freeing non-power-of-two-sized allocations back into the IOVA caches
+	 * will come back to bite us badly, so we have to waste a bit of space
+	 * rounding up anything cacheable to make sure that can't happen. The
+	 * order of the unadjusted size will still match upon freeing.
+	 */
+	if (iova_len < (1 << (IOVA_RANGE_CACHE_MAX_SIZE - 1)))
+		iova_len = roundup_pow_of_two(iova_len);
+#endif
+
+#if (KERNEL_VERSION(5, 10, 0) <= LINUX_VERSION_CODE)
+	dma_limit = min_not_zero(dma_limit, dev->bus_dma_limit);
+#else
+	if (dev->bus_dma_mask)
+		dma_limit &= dev->bus_dma_mask;
+#endif
+
+	if (domain->geometry.force_aperture)
+		dma_limit =
+			min_t(u64, dma_limit, domain->geometry.aperture_end);
+
+#if (KERNEL_VERSION(5, 4, 0) <= LINUX_VERSION_CODE)
+	iova = alloc_iova_fast(iovad, iova_len, dma_limit >> shift, true);
+#else
+	limit = min_t(dma_addr_t, dma_limit >> shift, iovad->end_pfn);
+
+	iova = alloc_iova_fast(iovad, iova_len, limit, true);
+#endif
+
+	return (dma_addr_t)iova << shift;
+}
+
+void rknpu_iommu_dma_free_iova(struct rknpu_iommu_dma_cookie *cookie,
+			       dma_addr_t iova, size_t size)
+{
+	struct iova_domain *iovad = &cookie->iovad;
+
+	free_iova_fast(iovad, iova_pfn(iovad, iova), size >> iova_shift(iovad));
+}
diff --git a/drivers/rknpu/rknpu_job.c b/drivers/rknpu/rknpu_job.c
new file mode 100644
index 000000000..6dc94b59b
--- /dev/null
+++ b/drivers/rknpu/rknpu_job.c
@@ -0,0 +1,1018 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#include <linux/slab.h>
+#include <linux/delay.h>
+#include <linux/sync_file.h>
+#include <linux/io.h>
+
+#include "rknpu_ioctl.h"
+#include "rknpu_drv.h"
+#include "rknpu_reset.h"
+#include "rknpu_gem.h"
+#include "rknpu_fence.h"
+#include "rknpu_job.h"
+#include "rknpu_mem.h"
+
+#define _REG_READ(base, offset) readl(base + (offset))
+#define _REG_WRITE(base, value, offset) writel(value, base + (offset))
+
+#define REG_READ(offset) _REG_READ(rknpu_core_base, offset)
+#define REG_WRITE(value, offset) _REG_WRITE(rknpu_core_base, value, offset)
+
+static int rknpu_wait_core_index(int core_mask)
+{
+	int index = 0;
+
+	switch (core_mask) {
+	case RKNPU_CORE0_MASK:
+	case RKNPU_CORE0_MASK | RKNPU_CORE1_MASK:
+	case RKNPU_CORE0_MASK | RKNPU_CORE1_MASK | RKNPU_CORE2_MASK:
+		index = 0;
+		break;
+	case RKNPU_CORE1_MASK:
+		index = 1;
+		break;
+	case RKNPU_CORE2_MASK:
+		index = 2;
+		break;
+	default:
+		break;
+	}
+
+	return index;
+}
+
+static int rknpu_core_mask(int core_index)
+{
+	int core_mask = RKNPU_CORE_AUTO_MASK;
+
+	switch (core_index) {
+	case 0:
+		core_mask = RKNPU_CORE0_MASK;
+		break;
+	case 1:
+		core_mask = RKNPU_CORE1_MASK;
+		break;
+	case 2:
+		core_mask = RKNPU_CORE2_MASK;
+		break;
+	default:
+		break;
+	}
+
+	return core_mask;
+}
+
+static int rknpu_get_task_number(struct rknpu_job *job, int core_index)
+{
+	struct rknpu_device *rknpu_dev = job->rknpu_dev;
+	int task_num = job->args->task_number;
+
+	if (core_index >= RKNPU_MAX_CORES || core_index < 0) {
+		LOG_ERROR("invalid rknpu core index: %d", core_index);
+		return 0;
+	}
+
+	if (rknpu_dev->config->num_irqs > 1) {
+		if (job->use_core_num == 1 || job->use_core_num == 2)
+			task_num =
+				job->args->subcore_task[core_index].task_number;
+		else if (job->use_core_num == 3)
+			task_num = job->args->subcore_task[core_index + 2]
+					   .task_number;
+	}
+
+	return task_num;
+}
+
+static void rknpu_job_free(struct rknpu_job *job)
+{
+#ifdef CONFIG_ROCKCHIP_RKNPU_DRM_GEM
+	struct rknpu_gem_object *task_obj = NULL;
+
+	task_obj =
+		(struct rknpu_gem_object *)(uintptr_t)job->args->task_obj_addr;
+	if (task_obj)
+		rknpu_gem_object_put(&task_obj->base);
+#endif
+
+	if (job->fence)
+		dma_fence_put(job->fence);
+
+	if (job->args_owner)
+		kfree(job->args);
+
+	kfree(job);
+}
+
+static int rknpu_job_cleanup(struct rknpu_job *job)
+{
+	rknpu_job_free(job);
+
+	return 0;
+}
+
+static void rknpu_job_cleanup_work(struct work_struct *work)
+{
+	struct rknpu_job *job =
+		container_of(work, struct rknpu_job, cleanup_work);
+
+	rknpu_job_cleanup(job);
+}
+
+static inline struct rknpu_job *rknpu_job_alloc(struct rknpu_device *rknpu_dev,
+						struct rknpu_submit *args)
+{
+	struct rknpu_job *job = NULL;
+#ifdef CONFIG_ROCKCHIP_RKNPU_DRM_GEM
+	struct rknpu_gem_object *task_obj = NULL;
+#endif
+
+	job = kzalloc(sizeof(*job), GFP_KERNEL);
+	if (!job)
+		return NULL;
+
+	job->timestamp = ktime_get();
+	job->rknpu_dev = rknpu_dev;
+	job->use_core_num = (args->core_mask & RKNPU_CORE0_MASK) +
+			    ((args->core_mask & RKNPU_CORE1_MASK) >> 1) +
+			    ((args->core_mask & RKNPU_CORE2_MASK) >> 2);
+	atomic_set(&job->run_count, job->use_core_num);
+	atomic_set(&job->interrupt_count, job->use_core_num);
+#ifdef CONFIG_ROCKCHIP_RKNPU_DRM_GEM
+	task_obj = (struct rknpu_gem_object *)(uintptr_t)args->task_obj_addr;
+	if (task_obj)
+		rknpu_gem_object_get(&task_obj->base);
+#endif
+
+	if (!(args->flags & RKNPU_JOB_NONBLOCK)) {
+		job->args = args;
+		job->args_owner = false;
+		return job;
+	}
+
+	job->args = kzalloc(sizeof(*args), GFP_KERNEL);
+	if (!job->args) {
+		kfree(job);
+		return NULL;
+	}
+	*job->args = *args;
+	job->args_owner = true;
+
+	INIT_WORK(&job->cleanup_work, rknpu_job_cleanup_work);
+
+	return job;
+}
+
+static inline int rknpu_job_wait(struct rknpu_job *job)
+{
+	struct rknpu_device *rknpu_dev = job->rknpu_dev;
+	struct rknpu_submit *args = job->args;
+	struct rknpu_task *last_task = NULL;
+	struct rknpu_subcore_data *subcore_data = NULL;
+	struct rknpu_job *entry, *q;
+	void __iomem *rknpu_core_base = NULL;
+	int core_index = rknpu_wait_core_index(job->args->core_mask);
+	unsigned long flags;
+	int wait_count = 0;
+	bool continue_wait = false;
+	int ret = -EINVAL;
+	int i = 0;
+
+	subcore_data = &rknpu_dev->subcore_datas[core_index];
+
+	do {
+		ret = wait_event_timeout(subcore_data->job_done_wq,
+					 job->flags & RKNPU_JOB_DONE ||
+						 rknpu_dev->soft_reseting,
+					 msecs_to_jiffies(args->timeout));
+
+		if (++wait_count >= 3)
+			break;
+
+		if (ret == 0) {
+			int64_t elapse_time_us = 0;
+			spin_lock_irqsave(&rknpu_dev->irq_lock, flags);
+			elapse_time_us = ktime_us_delta(ktime_get(),
+							job->hw_commit_time);
+			continue_wait =
+				job->hw_commit_time == 0 ?
+					true :
+					(elapse_time_us < args->timeout * 1000);
+			spin_unlock_irqrestore(&rknpu_dev->irq_lock, flags);
+			LOG_ERROR(
+				"job: %p, wait_count: %d, continue wait: %d, commit elapse time: %lldus, wait time: %lldus, timeout: %uus\n",
+				job, wait_count, continue_wait,
+				(job->hw_commit_time == 0 ? 0 : elapse_time_us),
+				ktime_us_delta(ktime_get(), job->timestamp),
+				args->timeout * 1000);
+		}
+	} while (ret == 0 && continue_wait);
+
+	last_task = job->last_task;
+	if (!last_task) {
+		spin_lock_irqsave(&rknpu_dev->irq_lock, flags);
+		for (i = 0; i < job->use_core_num; i++) {
+			subcore_data = &rknpu_dev->subcore_datas[i];
+			list_for_each_entry_safe(
+				entry, q, &subcore_data->todo_list, head[i]) {
+				if (entry == job) {
+					list_del(&job->head[i]);
+					break;
+				}
+			}
+		}
+		spin_unlock_irqrestore(&rknpu_dev->irq_lock, flags);
+
+		LOG_ERROR("job commit failed\n");
+		return ret < 0 ? ret : -EINVAL;
+	}
+
+	last_task->int_status = job->int_status[core_index];
+
+	if (ret <= 0) {
+		args->task_counter = 0;
+		rknpu_core_base = rknpu_dev->base[core_index];
+		if (args->flags & RKNPU_JOB_PC) {
+			uint32_t task_status = REG_READ(
+				rknpu_dev->config->pc_task_status_offset);
+			args->task_counter =
+				(task_status &
+				 rknpu_dev->config->pc_task_number_mask);
+		}
+
+		LOG_ERROR(
+			"failed to wait job, task counter: %d, flags: %#x, ret = %d, elapsed time: %lldus\n",
+			args->task_counter, args->flags, ret,
+			ktime_us_delta(ktime_get(), job->timestamp));
+
+		return ret < 0 ? ret : -ETIMEDOUT;
+	}
+
+	if (!(job->flags & RKNPU_JOB_DONE))
+		return -EINVAL;
+
+	args->task_counter = args->task_number;
+	args->hw_elapse_time = job->hw_elapse_time;
+
+	return 0;
+}
+
+static inline int rknpu_job_subcore_commit_pc(struct rknpu_job *job,
+					      int core_index)
+{
+	struct rknpu_device *rknpu_dev = job->rknpu_dev;
+	struct rknpu_submit *args = job->args;
+#ifdef CONFIG_ROCKCHIP_RKNPU_DRM_GEM
+	struct rknpu_gem_object *task_obj =
+		(struct rknpu_gem_object *)(uintptr_t)args->task_obj_addr;
+#endif
+#ifdef CONFIG_ROCKCHIP_RKNPU_DMA_HEAP
+	struct rknpu_mem_object *task_obj =
+		(struct rknpu_mem_object *)(uintptr_t)args->task_obj_addr;
+#endif
+	struct rknpu_task *task_base = NULL;
+	struct rknpu_task *first_task = NULL;
+	struct rknpu_task *last_task = NULL;
+	void __iomem *rknpu_core_base = rknpu_dev->base[core_index];
+	int task_start = args->task_start;
+	int task_end;
+	int task_number = args->task_number;
+	int task_pp_en = args->flags & RKNPU_JOB_PINGPONG ? 1 : 0;
+	int pc_data_amount_scale = rknpu_dev->config->pc_data_amount_scale;
+	int pc_task_number_bits = rknpu_dev->config->pc_task_number_bits;
+	int i = 0;
+	int submit_index = atomic_read(&job->submit_count[core_index]);
+	int max_submit_number = rknpu_dev->config->max_submit_number;
+	unsigned long flags;
+
+	if (!task_obj) {
+		job->ret = -EINVAL;
+		return job->ret;
+	}
+
+	if (rknpu_dev->config->num_irqs > 1) {
+		for (i = 0; i < rknpu_dev->config->num_irqs; i++) {
+			if (i == core_index) {
+				REG_WRITE((0xe + 0x10000000 * i), 0x1004);
+				REG_WRITE((0xe + 0x10000000 * i), 0x3004);
+			}
+		}
+
+		switch (job->use_core_num) {
+		case 1:
+		case 2:
+			task_start = args->subcore_task[core_index].task_start;
+			task_number =
+				args->subcore_task[core_index].task_number;
+			break;
+		case 3:
+			task_start =
+				args->subcore_task[core_index + 2].task_start;
+			task_number =
+				args->subcore_task[core_index + 2].task_number;
+			break;
+		default:
+			LOG_ERROR("Unknown use core num %d\n",
+				  job->use_core_num);
+			break;
+		}
+	}
+
+	task_start = task_start + submit_index * max_submit_number;
+	task_number = task_number - submit_index * max_submit_number;
+	task_number = task_number > max_submit_number ? max_submit_number :
+							task_number;
+	task_end = task_start + task_number - 1;
+
+	task_base = task_obj->kv_addr;
+
+	first_task = &task_base[task_start];
+	last_task = &task_base[task_end];
+
+	if (rknpu_dev->config->pc_dma_ctrl) {
+		spin_lock_irqsave(&rknpu_dev->irq_lock, flags);
+		REG_WRITE(first_task->regcmd_addr, RKNPU_OFFSET_PC_DATA_ADDR);
+		spin_unlock_irqrestore(&rknpu_dev->irq_lock, flags);
+	} else {
+		REG_WRITE(first_task->regcmd_addr, RKNPU_OFFSET_PC_DATA_ADDR);
+	}
+
+	REG_WRITE((first_task->regcfg_amount + RKNPU_PC_DATA_EXTRA_AMOUNT +
+		   pc_data_amount_scale - 1) /
+				  pc_data_amount_scale -
+			  1,
+		  RKNPU_OFFSET_PC_DATA_AMOUNT);
+
+	REG_WRITE(last_task->int_mask, RKNPU_OFFSET_INT_MASK);
+
+	REG_WRITE(first_task->int_mask, RKNPU_OFFSET_INT_CLEAR);
+
+	REG_WRITE(((0x6 | task_pp_en) << pc_task_number_bits) | task_number,
+		  RKNPU_OFFSET_PC_TASK_CONTROL);
+
+	REG_WRITE(args->task_base_addr, RKNPU_OFFSET_PC_DMA_BASE_ADDR);
+
+	job->first_task = first_task;
+	job->last_task = last_task;
+	job->int_mask[core_index] = last_task->int_mask;
+
+	REG_WRITE(0x1, RKNPU_OFFSET_PC_OP_EN);
+	REG_WRITE(0x0, RKNPU_OFFSET_PC_OP_EN);
+
+	return 0;
+}
+
+static inline int rknpu_job_subcore_commit(struct rknpu_job *job,
+					   int core_index)
+{
+	struct rknpu_device *rknpu_dev = job->rknpu_dev;
+	struct rknpu_submit *args = job->args;
+	void __iomem *rknpu_core_base = rknpu_dev->base[core_index];
+	unsigned long flags;
+
+	// switch to slave mode
+	if (rknpu_dev->config->pc_dma_ctrl) {
+		spin_lock_irqsave(&rknpu_dev->irq_lock, flags);
+		REG_WRITE(0x1, RKNPU_OFFSET_PC_DATA_ADDR);
+		spin_unlock_irqrestore(&rknpu_dev->irq_lock, flags);
+	} else {
+		REG_WRITE(0x1, RKNPU_OFFSET_PC_DATA_ADDR);
+	}
+
+	if (!(args->flags & RKNPU_JOB_PC)) {
+		job->ret = -EINVAL;
+		return job->ret;
+	}
+
+	return rknpu_job_subcore_commit_pc(job, core_index);
+}
+
+static void rknpu_job_commit(struct rknpu_job *job)
+{
+	switch (job->args->core_mask) {
+	case RKNPU_CORE0_MASK:
+		rknpu_job_subcore_commit(job, 0);
+		break;
+	case RKNPU_CORE1_MASK:
+		rknpu_job_subcore_commit(job, 1);
+		break;
+	case RKNPU_CORE2_MASK:
+		rknpu_job_subcore_commit(job, 2);
+		break;
+	case RKNPU_CORE0_MASK | RKNPU_CORE1_MASK:
+		rknpu_job_subcore_commit(job, 0);
+		rknpu_job_subcore_commit(job, 1);
+		break;
+	case RKNPU_CORE0_MASK | RKNPU_CORE1_MASK | RKNPU_CORE2_MASK:
+		rknpu_job_subcore_commit(job, 0);
+		rknpu_job_subcore_commit(job, 1);
+		rknpu_job_subcore_commit(job, 2);
+		break;
+	default:
+		LOG_ERROR("Unknown core mask: %d\n", job->args->core_mask);
+		break;
+	}
+}
+
+static void rknpu_job_next(struct rknpu_device *rknpu_dev, int core_index)
+{
+	struct rknpu_job *job = NULL;
+	struct rknpu_subcore_data *subcore_data = NULL;
+	unsigned long flags;
+
+	if (rknpu_dev->soft_reseting)
+		return;
+
+	subcore_data = &rknpu_dev->subcore_datas[core_index];
+
+	spin_lock_irqsave(&rknpu_dev->irq_lock, flags);
+
+	if (subcore_data->job || list_empty(&subcore_data->todo_list)) {
+		spin_unlock_irqrestore(&rknpu_dev->irq_lock, flags);
+		return;
+	}
+
+	job = list_first_entry(&subcore_data->todo_list, struct rknpu_job,
+			       head[core_index]);
+
+	list_del_init(&job->head[core_index]);
+	subcore_data->job = job;
+	job->hw_commit_time = ktime_get();
+	job->hw_recoder_time = job->hw_commit_time;
+	spin_unlock_irqrestore(&rknpu_dev->irq_lock, flags);
+
+	if (atomic_dec_and_test(&job->run_count)) {
+		rknpu_job_commit(job);
+	}
+}
+
+static void rknpu_job_done(struct rknpu_job *job, int ret, int core_index)
+{
+	struct rknpu_device *rknpu_dev = job->rknpu_dev;
+	struct rknpu_subcore_data *subcore_data = NULL;
+	ktime_t now;
+	unsigned long flags;
+	int max_submit_number = rknpu_dev->config->max_submit_number;
+
+	if (atomic_inc_return(&job->submit_count[core_index]) <
+	    (rknpu_get_task_number(job, core_index) + max_submit_number - 1) /
+		    max_submit_number) {
+		rknpu_job_subcore_commit(job, core_index);
+		return;
+	}
+
+	subcore_data = &rknpu_dev->subcore_datas[core_index];
+
+	spin_lock_irqsave(&rknpu_dev->irq_lock, flags);
+	subcore_data->job = NULL;
+	subcore_data->task_num -= rknpu_get_task_number(job, core_index);
+	now = ktime_get();
+	job->hw_elapse_time = ktime_sub(now, job->hw_commit_time);
+	subcore_data->timer.busy_time += ktime_sub(now, job->hw_recoder_time);
+	spin_unlock_irqrestore(&rknpu_dev->irq_lock, flags);
+
+	if (atomic_dec_and_test(&job->interrupt_count)) {
+		int use_core_num = job->use_core_num;
+
+		job->flags |= RKNPU_JOB_DONE;
+		job->ret = ret;
+
+		if (job->fence)
+			dma_fence_signal(job->fence);
+
+		if (job->flags & RKNPU_JOB_ASYNC)
+			schedule_work(&job->cleanup_work);
+
+		if (use_core_num > 1)
+			wake_up(&(&rknpu_dev->subcore_datas[0])->job_done_wq);
+		else
+			wake_up(&subcore_data->job_done_wq);
+	}
+
+	rknpu_job_next(rknpu_dev, core_index);
+}
+
+static int rknpu_schedule_core_index(struct rknpu_device *rknpu_dev)
+{
+	int core_num = rknpu_dev->config->num_irqs;
+	int task_num = rknpu_dev->subcore_datas[0].task_num;
+	int core_index = 0;
+	int i = 0;
+
+	for (i = 1; i < core_num; i++) {
+		if (task_num > rknpu_dev->subcore_datas[i].task_num) {
+			core_index = i;
+			task_num = rknpu_dev->subcore_datas[i].task_num;
+		}
+	}
+
+	return core_index;
+}
+
+static void rknpu_job_schedule(struct rknpu_job *job)
+{
+	struct rknpu_device *rknpu_dev = job->rknpu_dev;
+	struct rknpu_subcore_data *subcore_data = NULL;
+	int i = 0, core_index = 0;
+	unsigned long flags;
+
+	if (job->args->core_mask == RKNPU_CORE_AUTO_MASK) {
+		core_index = rknpu_schedule_core_index(rknpu_dev);
+		job->args->core_mask = rknpu_core_mask(core_index);
+		job->use_core_num = 1;
+		atomic_set(&job->run_count, job->use_core_num);
+		atomic_set(&job->interrupt_count, job->use_core_num);
+	}
+
+	spin_lock_irqsave(&rknpu_dev->irq_lock, flags);
+	for (i = 0; i < rknpu_dev->config->num_irqs; i++) {
+		if (job->args->core_mask & rknpu_core_mask(i)) {
+			subcore_data = &rknpu_dev->subcore_datas[i];
+			list_add_tail(&job->head[i], &subcore_data->todo_list);
+			subcore_data->task_num += rknpu_get_task_number(job, i);
+		}
+	}
+	spin_unlock_irqrestore(&rknpu_dev->irq_lock, flags);
+
+	for (i = 0; i < rknpu_dev->config->num_irqs; i++) {
+		if (job->args->core_mask & rknpu_core_mask(i))
+			rknpu_job_next(rknpu_dev, i);
+	}
+}
+
+static void rknpu_job_abort(struct rknpu_job *job)
+{
+	struct rknpu_device *rknpu_dev = job->rknpu_dev;
+	struct rknpu_subcore_data *subcore_data = NULL;
+	unsigned long flags;
+	int i = 0;
+
+	msleep(100);
+
+	spin_lock_irqsave(&rknpu_dev->irq_lock, flags);
+	for (i = 0; i < rknpu_dev->config->num_irqs; i++) {
+		if (job->args->core_mask & rknpu_core_mask(i)) {
+			subcore_data = &rknpu_dev->subcore_datas[i];
+			if (job == subcore_data->job && !job->irq_entry[i]) {
+				subcore_data->job = NULL;
+				subcore_data->task_num -=
+					rknpu_get_task_number(job, i);
+			}
+		}
+	}
+	spin_unlock_irqrestore(&rknpu_dev->irq_lock, flags);
+
+	if (job->ret == -ETIMEDOUT) {
+		LOG_ERROR("job timeout, flags: %#x:\n", job->flags);
+		for (i = 0; i < rknpu_dev->config->num_irqs; i++) {
+			if (job->args->core_mask & rknpu_core_mask(i)) {
+				void __iomem *rknpu_core_base =
+					rknpu_dev->base[i];
+				LOG_ERROR(
+					"\tcore %d irq status: %#x, raw status: %#x, require mask: %#x, task counter: %#x, elapsed time: %lldus\n",
+					i, REG_READ(RKNPU_OFFSET_INT_STATUS),
+					REG_READ(RKNPU_OFFSET_INT_RAW_STATUS),
+					job->int_mask[i],
+					(REG_READ(
+						 rknpu_dev->config
+							 ->pc_task_status_offset) &
+					 rknpu_dev->config->pc_task_number_mask),
+					ktime_us_delta(ktime_get(),
+						       job->timestamp));
+			}
+		}
+		rknpu_soft_reset(rknpu_dev);
+	} else {
+		LOG_ERROR(
+			"job abort, flags: %#x, ret: %d, elapsed time: %lldus\n",
+			job->flags, job->ret,
+			ktime_us_delta(ktime_get(), job->timestamp));
+	}
+
+	rknpu_job_cleanup(job);
+}
+
+static inline uint32_t rknpu_fuzz_status(uint32_t status)
+{
+	uint32_t fuzz_status = 0;
+
+	if ((status & 0x3) != 0)
+		fuzz_status |= 0x3;
+
+	if ((status & 0xc) != 0)
+		fuzz_status |= 0xc;
+
+	if ((status & 0x30) != 0)
+		fuzz_status |= 0x30;
+
+	if ((status & 0xc0) != 0)
+		fuzz_status |= 0xc0;
+
+	if ((status & 0x300) != 0)
+		fuzz_status |= 0x300;
+
+	if ((status & 0xc00) != 0)
+		fuzz_status |= 0xc00;
+
+	return fuzz_status;
+}
+
+static inline irqreturn_t rknpu_irq_handler(int irq, void *data, int core_index)
+{
+	struct rknpu_device *rknpu_dev = data;
+	void __iomem *rknpu_core_base = rknpu_dev->base[core_index];
+	struct rknpu_subcore_data *subcore_data = NULL;
+	struct rknpu_job *job = NULL;
+	uint32_t status = 0;
+	unsigned long flags;
+
+	subcore_data = &rknpu_dev->subcore_datas[core_index];
+
+	spin_lock_irqsave(&rknpu_dev->irq_lock, flags);
+	job = subcore_data->job;
+	if (!job) {
+		spin_unlock_irqrestore(&rknpu_dev->irq_lock, flags);
+		REG_WRITE(RKNPU_INT_CLEAR, RKNPU_OFFSET_INT_CLEAR);
+		rknpu_job_next(rknpu_dev, core_index);
+		return IRQ_HANDLED;
+	}
+	job->irq_entry[core_index] = true;
+	spin_unlock_irqrestore(&rknpu_dev->irq_lock, flags);
+
+	status = REG_READ(RKNPU_OFFSET_INT_STATUS);
+
+	job->int_status[core_index] = status;
+
+	if (rknpu_fuzz_status(status) != job->int_mask[core_index]) {
+		LOG_ERROR(
+			"invalid irq status: %#x, raw status: %#x, require mask: %#x, task counter: %#x\n",
+			status, REG_READ(RKNPU_OFFSET_INT_RAW_STATUS),
+			job->int_mask[core_index],
+			(REG_READ(rknpu_dev->config->pc_task_status_offset) &
+			 rknpu_dev->config->pc_task_number_mask));
+		REG_WRITE(RKNPU_INT_CLEAR, RKNPU_OFFSET_INT_CLEAR);
+		return IRQ_HANDLED;
+	}
+
+	REG_WRITE(RKNPU_INT_CLEAR, RKNPU_OFFSET_INT_CLEAR);
+
+	rknpu_job_done(job, 0, core_index);
+
+	return IRQ_HANDLED;
+}
+
+irqreturn_t rknpu_core0_irq_handler(int irq, void *data)
+{
+	return rknpu_irq_handler(irq, data, 0);
+}
+
+irqreturn_t rknpu_core1_irq_handler(int irq, void *data)
+{
+	return rknpu_irq_handler(irq, data, 1);
+}
+
+irqreturn_t rknpu_core2_irq_handler(int irq, void *data)
+{
+	return rknpu_irq_handler(irq, data, 2);
+}
+
+static void rknpu_job_timeout_clean(struct rknpu_device *rknpu_dev,
+				    int core_mask)
+{
+	struct rknpu_job *job = NULL;
+	unsigned long flags;
+	struct rknpu_subcore_data *subcore_data = NULL;
+	int i = 0;
+
+	for (i = 0; i < rknpu_dev->config->num_irqs; i++) {
+		if (core_mask & rknpu_core_mask(i)) {
+			subcore_data = &rknpu_dev->subcore_datas[i];
+			job = subcore_data->job;
+			if (job &&
+			    ktime_us_delta(ktime_get(), job->timestamp) >=
+				    job->args->timeout) {
+				rknpu_soft_reset(rknpu_dev);
+
+				spin_lock_irqsave(&rknpu_dev->irq_lock, flags);
+				subcore_data->job = NULL;
+				spin_unlock_irqrestore(&rknpu_dev->irq_lock,
+						       flags);
+
+				do {
+					schedule_work(&job->cleanup_work);
+
+					spin_lock_irqsave(&rknpu_dev->irq_lock,
+							  flags);
+
+					if (!list_empty(
+						    &subcore_data->todo_list)) {
+						job = list_first_entry(
+							&subcore_data->todo_list,
+							struct rknpu_job,
+							head[i]);
+						list_del_init(&job->head[i]);
+					} else {
+						job = NULL;
+					}
+
+					spin_unlock_irqrestore(
+						&rknpu_dev->irq_lock, flags);
+				} while (job);
+			}
+		}
+	}
+}
+
+static int rknpu_submit(struct rknpu_device *rknpu_dev,
+			struct rknpu_submit *args)
+{
+	struct rknpu_job *job = NULL;
+	int ret = -EINVAL;
+
+	if (args->task_number == 0) {
+		LOG_ERROR("invalid rknpu task number!\n");
+		return -EINVAL;
+	}
+
+	if (args->core_mask > rknpu_dev->config->core_mask) {
+		LOG_ERROR("invalid rknpu core mask: %#x", args->core_mask);
+		return -EINVAL;
+	}
+
+	job = rknpu_job_alloc(rknpu_dev, args);
+	if (!job) {
+		LOG_ERROR("failed to allocate rknpu job!\n");
+		return -ENOMEM;
+	}
+
+	if (args->flags & RKNPU_JOB_FENCE_IN) {
+#ifdef CONFIG_ROCKCHIP_RKNPU_FENCE
+		struct dma_fence *in_fence;
+
+		in_fence = sync_file_get_fence(args->fence_fd);
+
+		if (!in_fence) {
+			LOG_ERROR("invalid fence in fd, fd: %d\n",
+				  args->fence_fd);
+			return -EINVAL;
+		}
+		args->fence_fd = -1;
+
+		/*
+		 * Wait if the fence is from a foreign context, or if the fence
+		 * array contains any fence from a foreign context.
+		 */
+		ret = 0;
+		if (!dma_fence_match_context(in_fence,
+					     rknpu_dev->fence_ctx->context))
+			ret = dma_fence_wait_timeout(in_fence, true,
+						     args->timeout);
+		dma_fence_put(in_fence);
+		if (ret < 0) {
+			if (ret != -ERESTARTSYS)
+				LOG_ERROR("Error (%d) waiting for fence!\n",
+					  ret);
+
+			return ret;
+		}
+#else
+		LOG_ERROR(
+			"failed to use rknpu fence, please enable rknpu fence config!\n");
+		rknpu_job_free(job);
+		return -EINVAL;
+#endif
+	}
+
+	if (args->flags & RKNPU_JOB_FENCE_OUT) {
+#ifdef CONFIG_ROCKCHIP_RKNPU_FENCE
+		ret = rknpu_fence_alloc(job);
+		if (ret) {
+			rknpu_job_free(job);
+			return ret;
+		}
+		job->args->fence_fd = rknpu_fence_get_fd(job);
+		args->fence_fd = job->args->fence_fd;
+#else
+		LOG_ERROR(
+			"failed to use rknpu fence, please enable rknpu fence config!\n");
+		rknpu_job_free(job);
+		return -EINVAL;
+#endif
+	}
+
+	if (args->flags & RKNPU_JOB_NONBLOCK) {
+		job->flags |= RKNPU_JOB_ASYNC;
+		rknpu_job_timeout_clean(rknpu_dev, job->args->core_mask);
+		rknpu_job_schedule(job);
+		ret = job->ret;
+		if (ret) {
+			rknpu_job_abort(job);
+			return ret;
+		}
+	} else {
+		rknpu_job_schedule(job);
+		if (args->flags & RKNPU_JOB_PC)
+			job->ret = rknpu_job_wait(job);
+
+		args->task_counter = job->args->task_counter;
+		ret = job->ret;
+		if (!ret)
+			rknpu_job_cleanup(job);
+		else
+			rknpu_job_abort(job);
+	}
+
+	return ret;
+}
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_DRM_GEM
+int rknpu_submit_ioctl(struct drm_device *dev, void *data,
+		       struct drm_file *file_priv)
+{
+	struct rknpu_device *rknpu_dev = dev_get_drvdata(dev->dev);
+	struct rknpu_submit *args = data;
+
+	return rknpu_submit(rknpu_dev, args);
+}
+#endif
+
+#ifdef CONFIG_ROCKCHIP_RKNPU_DMA_HEAP
+int rknpu_submit_ioctl(struct rknpu_device *rknpu_dev, unsigned long data)
+{
+	struct rknpu_submit args;
+	int ret = -EINVAL;
+
+	if (unlikely(copy_from_user(&args, (struct rknpu_submit *)data,
+				    sizeof(struct rknpu_submit)))) {
+		LOG_ERROR("%s: copy_from_user failed\n", __func__);
+		ret = -EFAULT;
+		return ret;
+	}
+
+	ret = rknpu_submit(rknpu_dev, &args);
+
+	if (unlikely(copy_to_user((struct rknpu_submit *)data, &args,
+				  sizeof(struct rknpu_submit)))) {
+		LOG_ERROR("%s: copy_to_user failed\n", __func__);
+		ret = -EFAULT;
+		return ret;
+	}
+
+	return ret;
+}
+#endif
+
+int rknpu_get_hw_version(struct rknpu_device *rknpu_dev, uint32_t *version)
+{
+	void __iomem *rknpu_core_base = rknpu_dev->base[0];
+
+	if (version == NULL)
+		return -EINVAL;
+
+	*version = REG_READ(RKNPU_OFFSET_VERSION) +
+		   (REG_READ(RKNPU_OFFSET_VERSION_NUM) & 0xffff);
+
+	return 0;
+}
+
+int rknpu_get_bw_priority(struct rknpu_device *rknpu_dev, uint32_t *priority,
+			  uint32_t *expect, uint32_t *tw)
+{
+	void __iomem *base = rknpu_dev->bw_priority_base;
+
+	if (!rknpu_dev->config->bw_enable) {
+		LOG_WARN("Get bw_priority is not supported on this device!\n");
+		return 0;
+	}
+
+	if (!base)
+		return -EINVAL;
+
+	spin_lock(&rknpu_dev->lock);
+
+	if (priority != NULL)
+		*priority = _REG_READ(base, 0x0);
+
+	if (expect != NULL)
+		*expect = _REG_READ(base, 0x8);
+
+	if (tw != NULL)
+		*tw = _REG_READ(base, 0xc);
+
+	spin_unlock(&rknpu_dev->lock);
+
+	return 0;
+}
+
+int rknpu_set_bw_priority(struct rknpu_device *rknpu_dev, uint32_t priority,
+			  uint32_t expect, uint32_t tw)
+{
+	void __iomem *base = rknpu_dev->bw_priority_base;
+
+	if (!rknpu_dev->config->bw_enable) {
+		LOG_WARN("Set bw_priority is not supported on this device!\n");
+		return 0;
+	}
+
+	if (!base)
+		return -EINVAL;
+
+	spin_lock(&rknpu_dev->lock);
+
+	if (priority != 0)
+		_REG_WRITE(base, priority, 0x0);
+
+	if (expect != 0)
+		_REG_WRITE(base, expect, 0x8);
+
+	if (tw != 0)
+		_REG_WRITE(base, tw, 0xc);
+
+	spin_unlock(&rknpu_dev->lock);
+
+	return 0;
+}
+
+int rknpu_clear_rw_amount(struct rknpu_device *rknpu_dev)
+{
+	void __iomem *rknpu_core_base = rknpu_dev->base[0];
+	unsigned long flags;
+
+	if (!rknpu_dev->config->bw_enable) {
+		LOG_WARN("Clear rw_amount is not supported on this device!\n");
+		return 0;
+	}
+
+	if (rknpu_dev->config->pc_dma_ctrl) {
+		uint32_t pc_data_addr = 0;
+
+		spin_lock_irqsave(&rknpu_dev->irq_lock, flags);
+		pc_data_addr = REG_READ(RKNPU_OFFSET_PC_DATA_ADDR);
+
+		REG_WRITE(0x1, RKNPU_OFFSET_PC_DATA_ADDR);
+		REG_WRITE(0x80000101, RKNPU_OFFSET_CLR_ALL_RW_AMOUNT);
+		REG_WRITE(0x00000101, RKNPU_OFFSET_CLR_ALL_RW_AMOUNT);
+		REG_WRITE(pc_data_addr, RKNPU_OFFSET_PC_DATA_ADDR);
+		spin_unlock_irqrestore(&rknpu_dev->irq_lock, flags);
+	} else {
+		spin_lock(&rknpu_dev->lock);
+		REG_WRITE(0x80000101, RKNPU_OFFSET_CLR_ALL_RW_AMOUNT);
+		REG_WRITE(0x00000101, RKNPU_OFFSET_CLR_ALL_RW_AMOUNT);
+		spin_unlock(&rknpu_dev->lock);
+	}
+
+	return 0;
+}
+
+int rknpu_get_rw_amount(struct rknpu_device *rknpu_dev, uint32_t *dt_wr,
+			uint32_t *dt_rd, uint32_t *wd_rd)
+{
+	void __iomem *rknpu_core_base = rknpu_dev->base[0];
+	int amount_scale = rknpu_dev->config->pc_data_amount_scale;
+
+	if (!rknpu_dev->config->bw_enable) {
+		LOG_WARN("Get rw_amount is not supported on this device!\n");
+		return 0;
+	}
+
+	spin_lock(&rknpu_dev->lock);
+
+	if (dt_wr != NULL)
+		*dt_wr = REG_READ(RKNPU_OFFSET_DT_WR_AMOUNT) * amount_scale;
+
+	if (dt_rd != NULL)
+		*dt_rd = REG_READ(RKNPU_OFFSET_DT_RD_AMOUNT) * amount_scale;
+
+	if (wd_rd != NULL)
+		*wd_rd = REG_READ(RKNPU_OFFSET_WT_RD_AMOUNT) * amount_scale;
+
+	spin_unlock(&rknpu_dev->lock);
+
+	return 0;
+}
+
+int rknpu_get_total_rw_amount(struct rknpu_device *rknpu_dev, uint32_t *amount)
+{
+	uint32_t dt_wr = 0;
+	uint32_t dt_rd = 0;
+	uint32_t wd_rd = 0;
+	int ret = -EINVAL;
+
+	if (!rknpu_dev->config->bw_enable) {
+		LOG_WARN(
+			"Get total_rw_amount is not supported on this device!\n");
+		return 0;
+	}
+
+	ret = rknpu_get_rw_amount(rknpu_dev, &dt_wr, &dt_rd, &wd_rd);
+
+	if (amount != NULL)
+		*amount = dt_wr + dt_rd + wd_rd;
+
+	return ret;
+}
diff --git a/drivers/rknpu/rknpu_mm.c b/drivers/rknpu/rknpu_mm.c
new file mode 100644
index 000000000..a21bb6ded
--- /dev/null
+++ b/drivers/rknpu/rknpu_mm.c
@@ -0,0 +1,238 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#include "rknpu_debugger.h"
+#include "rknpu_mm.h"
+
+int rknpu_mm_create(unsigned int mem_size, unsigned int chunk_size,
+		    struct rknpu_mm **mm)
+{
+	unsigned int num_of_longs;
+	int ret = -EINVAL;
+
+	if (WARN_ON(mem_size < chunk_size))
+		return -EINVAL;
+	if (WARN_ON(mem_size == 0))
+		return -EINVAL;
+	if (WARN_ON(chunk_size == 0))
+		return -EINVAL;
+
+	*mm = kzalloc(sizeof(struct rknpu_mm), GFP_KERNEL);
+	if (!(*mm))
+		return -ENOMEM;
+
+	(*mm)->chunk_size = chunk_size;
+	(*mm)->total_chunks = mem_size / chunk_size;
+	(*mm)->free_chunks = (*mm)->total_chunks;
+
+	num_of_longs =
+		((*mm)->total_chunks + BITS_PER_LONG - 1) / BITS_PER_LONG;
+
+	(*mm)->bitmap = kcalloc(num_of_longs, sizeof(long), GFP_KERNEL);
+	if (!(*mm)->bitmap) {
+		ret = -ENOMEM;
+		goto free_mm;
+	}
+
+	mutex_init(&(*mm)->lock);
+
+	LOG_DEBUG("total_chunks: %d, bitmap: %p\n", (*mm)->total_chunks,
+		  (*mm)->bitmap);
+
+	return 0;
+
+free_mm:
+	kfree(mm);
+	return ret;
+}
+
+void rknpu_mm_destroy(struct rknpu_mm *mm)
+{
+	if (mm != NULL) {
+		mutex_destroy(&mm->lock);
+		kfree(mm->bitmap);
+		kfree(mm);
+	}
+}
+
+int rknpu_mm_alloc(struct rknpu_mm *mm, unsigned int size,
+		   struct rknpu_mm_obj **mm_obj)
+{
+	unsigned int found, start_search, cur_size;
+
+	if (size == 0)
+		return -EINVAL;
+
+	if (size > mm->total_chunks * mm->chunk_size)
+		return -ENOMEM;
+
+	*mm_obj = kzalloc(sizeof(struct rknpu_mm_obj), GFP_KERNEL);
+	if (!(*mm_obj))
+		return -ENOMEM;
+
+	start_search = 0;
+
+	mutex_lock(&mm->lock);
+
+mm_restart_search:
+	/* Find the first chunk that is free */
+	found = find_next_zero_bit(mm->bitmap, mm->total_chunks, start_search);
+
+	/* If there wasn't any free chunk, bail out */
+	if (found == mm->total_chunks)
+		goto mm_no_free_chunk;
+
+	/* Update fields of mm_obj */
+	(*mm_obj)->range_start = found;
+	(*mm_obj)->range_end = found;
+
+	/* If we need only one chunk, mark it as allocated and get out */
+	if (size <= mm->chunk_size) {
+		set_bit(found, mm->bitmap);
+		goto mm_out;
+	}
+
+	/* Otherwise, try to see if we have enough contiguous chunks */
+	cur_size = size - mm->chunk_size;
+	do {
+		(*mm_obj)->range_end = find_next_zero_bit(
+			mm->bitmap, mm->total_chunks, ++found);
+		/*
+		 * If next free chunk is not contiguous than we need to
+		 * restart our search from the last free chunk we found (which
+		 * wasn't contiguous to the previous ones
+		 */
+		if ((*mm_obj)->range_end != found) {
+			start_search = found;
+			goto mm_restart_search;
+		}
+
+		/*
+		 * If we reached end of buffer, bail out with error
+		 */
+		if (found == mm->total_chunks)
+			goto mm_no_free_chunk;
+
+		/* Check if we don't need another chunk */
+		if (cur_size <= mm->chunk_size)
+			cur_size = 0;
+		else
+			cur_size -= mm->chunk_size;
+
+	} while (cur_size > 0);
+
+	/* Mark the chunks as allocated */
+	for (found = (*mm_obj)->range_start; found <= (*mm_obj)->range_end;
+	     found++)
+		set_bit(found, mm->bitmap);
+
+mm_out:
+	mm->free_chunks -= ((*mm_obj)->range_end - (*mm_obj)->range_start + 1);
+	mutex_unlock(&mm->lock);
+
+	LOG_DEBUG("mm allocate, mm_obj: %p, range_start: %d, range_end: %d\n",
+		  *mm_obj, (*mm_obj)->range_start, (*mm_obj)->range_end);
+
+	return 0;
+
+mm_no_free_chunk:
+	mutex_unlock(&mm->lock);
+	kfree(*mm_obj);
+
+	return -ENOMEM;
+}
+
+int rknpu_mm_free(struct rknpu_mm *mm, struct rknpu_mm_obj *mm_obj)
+{
+	unsigned int bit;
+
+	/* Act like kfree when trying to free a NULL object */
+	if (!mm_obj)
+		return 0;
+
+	LOG_DEBUG("mm free, mem_obj: %p, range_start: %d, range_end: %d\n",
+		  mm_obj, mm_obj->range_start, mm_obj->range_end);
+
+	mutex_lock(&mm->lock);
+
+	/* Mark the chunks as free */
+	for (bit = mm_obj->range_start; bit <= mm_obj->range_end; bit++)
+		clear_bit(bit, mm->bitmap);
+
+	mm->free_chunks += (mm_obj->range_end - mm_obj->range_start + 1);
+
+	mutex_unlock(&mm->lock);
+
+	kfree(mm_obj);
+
+	return 0;
+}
+
+int rknpu_mm_dump(struct seq_file *m, void *data)
+{
+	struct rknpu_debugger_node *node = m->private;
+	struct rknpu_debugger *debugger = node->debugger;
+	struct rknpu_device *rknpu_dev =
+		container_of(debugger, struct rknpu_device, debugger);
+	struct rknpu_mm *mm = NULL;
+	int cur = 0, rbot = 0, rtop = 0;
+	size_t ret = 0;
+	char buf[64];
+	size_t size = sizeof(buf);
+	int seg_chunks = 32, seg_id = 0;
+	int free_size = 0;
+	int i = 0;
+
+	mm = rknpu_dev->sram_mm;
+	if (mm == NULL)
+		return 0;
+
+	seq_printf(m, "SRAM bitmap: \"*\" - used, \".\" - free (1bit = %dKB)\n",
+		   mm->chunk_size / 1024);
+
+	rbot = cur = find_first_bit(mm->bitmap, mm->total_chunks);
+	for (i = 0; i < cur; ++i) {
+		ret += scnprintf(buf + ret, size - ret, ".");
+		if (ret >= seg_chunks) {
+			seq_printf(m, "[%03d] [%s]\n", seg_id++, buf);
+			ret = 0;
+		}
+	}
+	while (cur < mm->total_chunks) {
+		rtop = cur;
+		cur = find_next_bit(mm->bitmap, mm->total_chunks, cur + 1);
+		if (cur < mm->total_chunks && cur <= rtop + 1)
+			continue;
+
+		for (i = rbot; i <= rtop; ++i) {
+			ret += scnprintf(buf + ret, size - ret, "*");
+			if (ret >= seg_chunks) {
+				seq_printf(m, "[%03d] [%s]\n", seg_id++, buf);
+				ret = 0;
+			}
+		}
+
+		for (i = rtop + 1; i < cur; ++i) {
+			ret += scnprintf(buf + ret, size - ret, ".");
+			if (ret >= seg_chunks) {
+				seq_printf(m, "[%03d] [%s]\n", seg_id++, buf);
+				ret = 0;
+			}
+		}
+
+		rbot = cur;
+	}
+
+	if (ret > 0)
+		seq_printf(m, "[%03d] [%s]\n", seg_id++, buf);
+
+	free_size = mm->free_chunks * mm->chunk_size;
+	seq_printf(m, "SRAM total size: %d, used: %d, free: %d\n",
+		   rknpu_dev->sram_size, rknpu_dev->sram_size - free_size,
+		   free_size);
+
+	return 0;
+}
diff --git a/drivers/rknpu/rknpu_reset.c b/drivers/rknpu/rknpu_reset.c
new file mode 100644
index 000000000..91c9b75d6
--- /dev/null
+++ b/drivers/rknpu/rknpu_reset.c
@@ -0,0 +1,148 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co.Ltd
+ * Author: Felix Zeng <felix.zeng@rock-chips.com>
+ */
+
+#include <linux/delay.h>
+#include <linux/iommu.h>
+
+#include "rknpu_reset.h"
+
+#ifndef FPGA_PLATFORM
+static inline struct reset_control *rknpu_reset_control_get(struct device *dev,
+							    const char *name)
+{
+	struct reset_control *rst = NULL;
+
+	rst = devm_reset_control_get(dev, name);
+	if (IS_ERR(rst))
+		LOG_DEV_ERROR(dev,
+			      "failed to get rknpu reset control: %s, %ld\n",
+			      name, PTR_ERR(rst));
+
+	return rst;
+}
+#endif
+
+int rknpu_reset_get(struct rknpu_device *rknpu_dev)
+{
+#ifndef FPGA_PLATFORM
+	struct reset_control *srst_a = NULL;
+	struct reset_control *srst_h = NULL;
+	int i = 0;
+
+	for (i = 0; i < rknpu_dev->config->num_resets; i++) {
+		srst_a = rknpu_reset_control_get(
+			rknpu_dev->dev,
+			rknpu_dev->config->resets[i].srst_a_name);
+		if (IS_ERR(srst_a))
+			return PTR_ERR(srst_a);
+
+		rknpu_dev->srst_a[i] = srst_a;
+
+		srst_h = rknpu_reset_control_get(
+			rknpu_dev->dev,
+			rknpu_dev->config->resets[i].srst_h_name);
+		if (IS_ERR(srst_h))
+			return PTR_ERR(srst_h);
+
+		rknpu_dev->srst_h[i] = srst_h;
+	}
+#endif
+
+	return 0;
+}
+
+#ifndef FPGA_PLATFORM
+static int rknpu_reset_assert(struct reset_control *rst)
+{
+	int ret = -EINVAL;
+
+	if (!rst)
+		return -EINVAL;
+
+	ret = reset_control_assert(rst);
+	if (ret < 0) {
+		LOG_ERROR("failed to assert rknpu reset: %d\n", ret);
+		return ret;
+	}
+
+	return 0;
+}
+
+static int rknpu_reset_deassert(struct reset_control *rst)
+{
+	int ret = -EINVAL;
+
+	if (!rst)
+		return -EINVAL;
+
+	ret = reset_control_deassert(rst);
+	if (ret < 0) {
+		LOG_ERROR("failed to deassert rknpu reset: %d\n", ret);
+		return ret;
+	}
+
+	return 0;
+}
+#endif
+
+int rknpu_soft_reset(struct rknpu_device *rknpu_dev)
+{
+#ifndef FPGA_PLATFORM
+	struct iommu_domain *domain = NULL;
+	struct rknpu_subcore_data *subcore_data = NULL;
+	int ret = -EINVAL, i = 0;
+
+	if (rknpu_dev->bypass_soft_reset) {
+		LOG_WARN("bypass soft reset\n");
+		return 0;
+	}
+
+	if (!mutex_trylock(&rknpu_dev->reset_lock))
+		return 0;
+
+	rknpu_dev->soft_reseting = true;
+
+	msleep(100);
+
+	for (i = 0; i < rknpu_dev->config->num_irqs; ++i) {
+		subcore_data = &rknpu_dev->subcore_datas[i];
+		wake_up(&subcore_data->job_done_wq);
+	}
+
+	LOG_INFO("soft reset\n");
+
+	for (i = 0; i < rknpu_dev->config->num_resets; i++) {
+		ret = rknpu_reset_assert(rknpu_dev->srst_a[i]);
+		ret |= rknpu_reset_assert(rknpu_dev->srst_h[i]);
+
+		udelay(10);
+
+		ret |= rknpu_reset_deassert(rknpu_dev->srst_a[i]);
+		ret |= rknpu_reset_deassert(rknpu_dev->srst_h[i]);
+	}
+
+	if (ret) {
+		LOG_DEV_ERROR(rknpu_dev->dev,
+			      "failed to soft reset for rknpu: %d\n", ret);
+		mutex_unlock(&rknpu_dev->reset_lock);
+		return ret;
+	}
+
+	if (rknpu_dev->iommu_en)
+		domain = iommu_get_domain_for_dev(rknpu_dev->dev);
+
+	if (domain) {
+		iommu_detach_device(domain, rknpu_dev->dev);
+		iommu_attach_device(domain, rknpu_dev->dev);
+	}
+
+	rknpu_dev->soft_reseting = false;
+
+	mutex_unlock(&rknpu_dev->reset_lock);
+#endif
+
+	return 0;
+}
diff --git a/drivers/soc/rockchip/Kconfig b/drivers/soc/rockchip/Kconfig
index aff2f7e95..9871fe046 100644
--- a/drivers/soc/rockchip/Kconfig
+++ b/drivers/soc/rockchip/Kconfig
@@ -43,3 +43,9 @@ config ROCKCHIP_DTPM
 	  devices.
 
 endif
+
+config ROCKCHIP_OPP
+	tristate "Rockchip OPP select support"
+	depends on PM_DEVFREQ
+	help
+	  Say y here to enable rockchip OPP support.
\ No newline at end of file
diff --git a/drivers/soc/rockchip/Makefile b/drivers/soc/rockchip/Makefile
index 23d414433..8f3c243c1 100644
--- a/drivers/soc/rockchip/Makefile
+++ b/drivers/soc/rockchip/Makefile
@@ -5,3 +5,4 @@
 obj-$(CONFIG_ROCKCHIP_GRF) += grf.o
 obj-$(CONFIG_ROCKCHIP_IODOMAIN) += io-domain.o
 obj-$(CONFIG_ROCKCHIP_DTPM) += dtpm.o
+obj-$(CONFIG_ROCKCHIP_OPP) += rockchip_opp_select.o
\ No newline at end of file
diff --git a/drivers/soc/rockchip/rockchip_opp_select.c b/drivers/soc/rockchip/rockchip_opp_select.c
new file mode 100644
index 000000000..9e3b93fa7
--- /dev/null
+++ b/drivers/soc/rockchip/rockchip_opp_select.c
@@ -0,0 +1,2490 @@
+/*
+ * Copyright (c) 2017 Fuzhou Rockchip Electronics Co., Ltd
+ *
+ * SPDX-License-Identifier: GPL-2.0+
+ */
+//#define DEBUG
+#include <linux/clk.h>
+#include <linux/cpufreq.h>
+#include <linux/devfreq.h>
+#include <linux/mfd/syscon.h>
+#include <linux/module.h>
+#include <linux/nvmem-consumer.h>
+#include <linux/regmap.h>
+#include <linux/regulator/consumer.h>
+#include <linux/rockchip/rockchip_sip.h>
+#include <linux/slab.h>
+#include <linux/soc/rockchip/pvtm.h>
+#include <linux/thermal.h>
+#include <linux/pm_opp.h>
+#include <linux/version.h>
+#include <soc/rockchip/rockchip_opp_select.h>
+
+#include "../../clk/rockchip/clk.h"
+#include "../../opp/opp.h"
+#include "../../devfreq/governor.h"
+
+#define MAX_PROP_NAME_LEN	6
+#define SEL_TABLE_END		~1
+#define AVS_DELETE_OPP		0
+#define AVS_SCALING_RATE	1
+
+#define LEAKAGE_V1		1
+#define LEAKAGE_V2		2
+#define LEAKAGE_V3		3
+
+#define to_thermal_opp_info(nb) container_of(nb, struct thermal_opp_info, \
+					     thermal_nb)
+
+struct sel_table {
+	int min;
+	int max;
+	int sel;
+};
+
+struct bin_sel_table {
+	int bin;
+	int sel;
+};
+
+struct pvtm_config {
+	unsigned int freq;
+	unsigned int volt;
+	unsigned int ch[2];
+	unsigned int sample_time;
+	unsigned int num;
+	unsigned int err;
+	unsigned int ref_temp;
+	unsigned int offset;
+	int temp_prop[2];
+	const char *tz_name;
+	struct thermal_zone_device *tz;
+	struct regmap *grf;
+};
+
+struct lkg_conversion_table {
+	int temp;
+	int conv;
+};
+
+struct otp_opp_info {
+	u16 min_freq;
+	u16 max_freq;
+	u8 volt;
+	u8 length;
+} __packed;
+
+#define PVTM_CH_MAX	8
+#define PVTM_SUB_CH_MAX	8
+
+#define FRAC_BITS 10
+#define int_to_frac(x) ((x) << FRAC_BITS)
+#define frac_to_int(x) ((x) >> FRAC_BITS)
+
+static int pvtm_value[PVTM_CH_MAX][PVTM_SUB_CH_MAX];
+static int lkg_version;
+
+static int rockchip_init_read_margin(struct device *dev,
+				     struct rockchip_opp_info *opp_info,
+				     const char *reg_name);
+
+/*
+ * temp = temp * 10
+ * conv = exp(-ln(1.2) / 5 * (temp - 23)) * 100
+ */
+static const struct lkg_conversion_table conv_table[] = {
+	{ 200, 111 },
+	{ 205, 109 },
+	{ 210, 107 },
+	{ 215, 105 },
+	{ 220, 103 },
+	{ 225, 101 },
+	{ 230, 100 },
+	{ 235, 98 },
+	{ 240, 96 },
+	{ 245, 94 },
+	{ 250, 92 },
+	{ 255, 91 },
+	{ 260, 89 },
+	{ 265, 88 },
+	{ 270, 86 },
+	{ 275, 84 },
+	{ 280, 83 },
+	{ 285, 81 },
+	{ 290, 80 },
+	{ 295, 78 },
+	{ 300, 77 },
+	{ 305, 76 },
+	{ 310, 74 },
+	{ 315, 73 },
+	{ 320, 72 },
+	{ 325, 70 },
+	{ 330, 69 },
+	{ 335, 68 },
+	{ 340, 66 },
+	{ 345, 65 },
+	{ 350, 64 },
+	{ 355, 63 },
+	{ 360, 62 },
+	{ 365, 61 },
+	{ 370, 60 },
+	{ 375, 58 },
+	{ 380, 57 },
+	{ 385, 56 },
+	{ 390, 55 },
+	{ 395, 54 },
+	{ 400, 53 },
+};
+
+static int rockchip_nvmem_cell_read_common(struct device_node *np,
+					   const char *cell_id,
+					   void *val, size_t count)
+{
+	struct nvmem_cell *cell;
+	void *buf;
+	size_t len;
+
+	cell = of_nvmem_cell_get(np, cell_id);
+	if (IS_ERR(cell))
+		return PTR_ERR(cell);
+
+	buf = nvmem_cell_read(cell, &len);
+	if (IS_ERR(buf)) {
+		nvmem_cell_put(cell);
+		return PTR_ERR(buf);
+	}
+	if (len != count) {
+		kfree(buf);
+		nvmem_cell_put(cell);
+		return -EINVAL;
+	}
+	memcpy(val, buf, count);
+	kfree(buf);
+	nvmem_cell_put(cell);
+
+	return 0;
+}
+
+int rockchip_nvmem_cell_read_u8(struct device_node *np, const char *cell_id,
+				u8 *val)
+{
+	return rockchip_nvmem_cell_read_common(np, cell_id, val, sizeof(*val));
+}
+EXPORT_SYMBOL(rockchip_nvmem_cell_read_u8);
+
+int rockchip_nvmem_cell_read_u16(struct device_node *np, const char *cell_id,
+				 u16 *val)
+{
+	return rockchip_nvmem_cell_read_common(np, cell_id, val, sizeof(*val));
+}
+EXPORT_SYMBOL(rockchip_nvmem_cell_read_u16);
+
+static int rockchip_get_sel_table(struct device_node *np, char *porp_name,
+				  struct sel_table **table)
+{
+	struct sel_table *sel_table;
+	const struct property *prop;
+	int count, i;
+
+	prop = of_find_property(np, porp_name, NULL);
+	if (!prop)
+		return -EINVAL;
+
+	if (!prop->value)
+		return -ENODATA;
+
+	count = of_property_count_u32_elems(np, porp_name);
+	if (count < 0)
+		return -EINVAL;
+
+	if (count % 3)
+		return -EINVAL;
+
+	sel_table = kzalloc(sizeof(*sel_table) * (count / 3 + 1), GFP_KERNEL);
+	if (!sel_table)
+		return -ENOMEM;
+
+	for (i = 0; i < count / 3; i++) {
+		of_property_read_u32_index(np, porp_name, 3 * i,
+					   &sel_table[i].min);
+		of_property_read_u32_index(np, porp_name, 3 * i + 1,
+					   &sel_table[i].max);
+		of_property_read_u32_index(np, porp_name, 3 * i + 2,
+					   &sel_table[i].sel);
+	}
+	sel_table[i].min = 0;
+	sel_table[i].max = 0;
+	sel_table[i].sel = SEL_TABLE_END;
+
+	*table = sel_table;
+
+	return 0;
+}
+
+static int rockchip_get_bin_sel_table(struct device_node *np, char *porp_name,
+				      struct bin_sel_table **table)
+{
+	struct bin_sel_table *sel_table;
+	const struct property *prop;
+	int count, i;
+
+	prop = of_find_property(np, porp_name, NULL);
+	if (!prop)
+		return -EINVAL;
+
+	if (!prop->value)
+		return -ENODATA;
+
+	count = of_property_count_u32_elems(np, porp_name);
+	if (count < 0)
+		return -EINVAL;
+
+	if (count % 2)
+		return -EINVAL;
+
+	sel_table = kzalloc(sizeof(*sel_table) * (count / 2 + 1), GFP_KERNEL);
+	if (!sel_table)
+		return -ENOMEM;
+
+	for (i = 0; i < count / 2; i++) {
+		of_property_read_u32_index(np, porp_name, 2 * i,
+					   &sel_table[i].bin);
+		of_property_read_u32_index(np, porp_name, 2 * i + 1,
+					   &sel_table[i].sel);
+	}
+
+	sel_table[i].bin = 0;
+	sel_table[i].sel = SEL_TABLE_END;
+
+	*table = sel_table;
+
+	return 0;
+}
+
+static int rockchip_get_sel(struct device_node *np, char *name,
+			    int value, int *sel)
+{
+	struct sel_table *table = NULL;
+	int i, ret = -EINVAL;
+
+	if (!sel)
+		return -EINVAL;
+
+	if (rockchip_get_sel_table(np, name, &table))
+		return -EINVAL;
+
+	for (i = 0; table[i].sel != SEL_TABLE_END; i++) {
+		if (value >= table[i].min) {
+			*sel = table[i].sel;
+			ret = 0;
+		}
+	}
+	kfree(table);
+
+	return ret;
+}
+
+static int rockchip_get_bin_sel(struct device_node *np, char *name,
+				int value, int *sel)
+{
+	struct bin_sel_table *table = NULL;
+	int i, ret = -EINVAL;
+
+	if (!sel)
+		return -EINVAL;
+
+	if (rockchip_get_bin_sel_table(np, name, &table))
+		return -EINVAL;
+
+	for (i = 0; table[i].sel != SEL_TABLE_END; i++) {
+		if (value == table[i].bin) {
+			*sel = table[i].sel;
+			ret = 0;
+			break;
+		}
+	}
+	kfree(table);
+
+	return ret;
+}
+
+static int rockchip_parse_pvtm_config(struct device_node *np,
+				      struct pvtm_config *pvtm)
+{
+	if (of_property_read_u32(np, "rockchip,pvtm-freq", &pvtm->freq))
+		return -EINVAL;
+	if (of_property_read_u32(np, "rockchip,pvtm-volt", &pvtm->volt))
+		return -EINVAL;
+	if (of_property_read_u32(np, "rockchip,pvtm-sample-time",
+				 &pvtm->sample_time))
+		return -EINVAL;
+	if (of_property_read_u32(np, "rockchip,pvtm-ref-temp", &pvtm->ref_temp))
+		return -EINVAL;
+	if (of_property_read_u32_array(np, "rockchip,pvtm-temp-prop",
+				       pvtm->temp_prop, 2))
+		return -EINVAL;
+	if (of_property_read_string(np, "rockchip,pvtm-thermal-zone",
+				    &pvtm->tz_name)) {
+		if (of_property_read_string(np, "rockchip,thermal-zone",
+					    &pvtm->tz_name))
+			return -EINVAL;
+	}
+	pvtm->tz = thermal_zone_get_zone_by_name(pvtm->tz_name);
+	if (IS_ERR(pvtm->tz))
+		return -EINVAL;
+	if (!pvtm->tz->ops->get_temp)
+		return -EINVAL;
+	if (of_property_read_bool(np, "rockchip,pvtm-pvtpll")) {
+		if (of_property_read_u32(np, "rockchip,pvtm-offset",
+					 &pvtm->offset))
+			return -EINVAL;
+		pvtm->grf = syscon_regmap_lookup_by_phandle(np, "rockchip,grf");
+		if (IS_ERR(pvtm->grf))
+			return -EINVAL;
+		return 0;
+	}
+	if (of_property_read_u32_array(np, "rockchip,pvtm-ch", pvtm->ch, 2))
+		return -EINVAL;
+	if (pvtm->ch[0] >= PVTM_CH_MAX || pvtm->ch[1] >= PVTM_SUB_CH_MAX)
+		return -EINVAL;
+	if (of_property_read_u32(np, "rockchip,pvtm-number", &pvtm->num))
+		return -EINVAL;
+	if (of_property_read_u32(np, "rockchip,pvtm-error", &pvtm->err))
+		return -EINVAL;
+
+	return 0;
+}
+
+static int rockchip_get_pvtm_specific_value(struct device *dev,
+					    struct device_node *np,
+					    struct clk *clk,
+					    struct regulator *reg,
+					    int *target_value)
+{
+	struct pvtm_config *pvtm;
+	unsigned long old_freq;
+	unsigned int old_volt;
+	int cur_temp, diff_temp;
+	int cur_value, total_value, avg_value, diff_value;
+	int min_value, max_value;
+	int ret = 0, i = 0, retry = 2;
+
+	pvtm = kzalloc(sizeof(*pvtm), GFP_KERNEL);
+	if (!pvtm)
+		return -ENOMEM;
+
+	ret = rockchip_parse_pvtm_config(np, pvtm);
+	if (ret)
+		goto pvtm_value_out;
+
+	old_freq = clk_get_rate(clk);
+	old_volt = regulator_get_voltage(reg);
+
+	/*
+	 * Set pvtm_freq to the lowest frequency in dts,
+	 * so change frequency first.
+	 */
+	ret = clk_set_rate(clk, pvtm->freq * 1000);
+	if (ret) {
+		dev_err(dev, "Failed to set pvtm freq\n");
+		goto pvtm_value_out;
+	}
+
+	ret = regulator_set_voltage(reg, pvtm->volt, pvtm->volt);
+	if (ret) {
+		dev_err(dev, "Failed to set pvtm_volt\n");
+		goto restore_clk;
+	}
+
+	/* The first few values may be fluctuant, if error is too big, retry*/
+	while (retry--) {
+		total_value = 0;
+		min_value = INT_MAX;
+		max_value = 0;
+
+		for (i = 0; i < pvtm->num; i++) {
+			cur_value = rockchip_get_pvtm_value(pvtm->ch[0],
+							    pvtm->ch[1],
+							    pvtm->sample_time);
+			if (cur_value <= 0) {
+				ret = -EINVAL;
+				goto resetore_volt;
+			}
+			if (cur_value < min_value)
+				min_value = cur_value;
+			if (cur_value > max_value)
+				max_value = cur_value;
+			total_value += cur_value;
+		}
+		if (max_value - min_value < pvtm->err)
+			break;
+	}
+	if (!total_value || !pvtm->num) {
+		ret = -EINVAL;
+		goto resetore_volt;
+	}
+	avg_value = total_value / pvtm->num;
+
+	/*
+	 * As pvtm is influenced by temperature, compute difference between
+	 * current temperature and reference temperature
+	 */
+	pvtm->tz->ops->get_temp(pvtm->tz, &cur_temp);
+	diff_temp = (cur_temp / 1000 - pvtm->ref_temp);
+	diff_value = diff_temp *
+		(diff_temp < 0 ? pvtm->temp_prop[0] : pvtm->temp_prop[1]);
+	*target_value = avg_value + diff_value;
+
+	pvtm_value[pvtm->ch[0]][pvtm->ch[1]] = *target_value;
+
+	dev_info(dev, "temp=%d, pvtm=%d (%d + %d)\n",
+		 cur_temp, *target_value, avg_value, diff_value);
+
+resetore_volt:
+	regulator_set_voltage(reg, old_volt, INT_MAX);
+restore_clk:
+	clk_set_rate(clk, old_freq);
+pvtm_value_out:
+	kfree(pvtm);
+
+	return ret;
+}
+
+/**
+ * mul_frac() - multiply two fixed-point numbers
+ * @x:	first multiplicand
+ * @y:	second multiplicand
+ *
+ * Return: the result of multiplying two fixed-point numbers.  The
+ * result is also a fixed-point number.
+ */
+static inline s64 mul_frac(s64 x, s64 y)
+{
+	return (x * y) >> FRAC_BITS;
+}
+
+static int temp_to_conversion_rate(int temp)
+{
+	int high, low, mid;
+
+	low = 0;
+	high = ARRAY_SIZE(conv_table) - 1;
+	mid = (high + low) / 2;
+
+	/* No temp available, return max conversion_rate */
+	if (temp <= conv_table[low].temp)
+		return conv_table[low].conv;
+	if (temp >= conv_table[high].temp)
+		return conv_table[high].conv;
+
+	while (low <= high) {
+		if (temp <= conv_table[mid].temp && temp >
+		    conv_table[mid - 1].temp) {
+			return conv_table[mid - 1].conv +
+			    (conv_table[mid].conv - conv_table[mid - 1].conv) *
+			    (temp - conv_table[mid - 1].temp) /
+			    (conv_table[mid].temp - conv_table[mid - 1].temp);
+		} else if (temp > conv_table[mid].temp) {
+			low = mid + 1;
+		} else {
+			high = mid - 1;
+		}
+		mid = (low + high) / 2;
+	}
+
+	return 100;
+}
+
+static int rockchip_adjust_leakage(struct device *dev, struct device_node *np,
+				   int *leakage)
+{
+	struct nvmem_cell *cell;
+	u8 value = 0;
+	u32 temp;
+	int conversion;
+	int ret;
+
+	cell = of_nvmem_cell_get(np, "leakage_temp");
+	if (IS_ERR(cell))
+		goto next;
+	nvmem_cell_put(cell);
+	ret = rockchip_nvmem_cell_read_u8(np, "leakage_temp", &value);
+	if (ret) {
+		dev_err(dev, "Failed to get leakage temp\n");
+		return -EINVAL;
+	}
+	/*
+	 * The ambient temperature range: 20C to 40C
+	 * In order to improve the precision, we do a conversion.
+	 * The temp in efuse : temp_efuse = (temp - 20) / (40 - 20) * 63
+	 * The ambient temp : temp = (temp_efuse / 63) * (40 - 20) + 20
+	 * Reserves a decimal point : temp = temp * 10
+	 */
+	temp = value;
+	temp = mul_frac((int_to_frac(temp) / 63 * 20 + int_to_frac(20)),
+			int_to_frac(10));
+	conversion = temp_to_conversion_rate(frac_to_int(temp));
+	*leakage = *leakage * conversion / 100;
+
+next:
+	cell = of_nvmem_cell_get(np, "leakage_volt");
+	if (IS_ERR(cell))
+		return 0;
+	nvmem_cell_put(cell);
+	ret = rockchip_nvmem_cell_read_u8(np, "leakage_volt", &value);
+	if (ret) {
+		dev_err(dev, "Failed to get leakage volt\n");
+		return -EINVAL;
+	}
+	/*
+	 * if ft write leakage use 1.35v, need convert to 1v.
+	 * leakage(1v) = leakage(1.35v) / 4
+	 */
+	if (value)
+		*leakage = *leakage / 4;
+
+	return 0;
+}
+
+static int rockchip_get_leakage_version(int *version)
+{
+	if (*version)
+		return 0;
+
+	if (of_machine_is_compatible("rockchip,rk3368"))
+		*version = LEAKAGE_V2;
+	else if (of_machine_is_compatible("rockchip,rv1126") ||
+		 of_machine_is_compatible("rockchip,rv1109"))
+		*version = LEAKAGE_V3;
+	else
+		*version = LEAKAGE_V1;
+
+	return 0;
+}
+
+static int rockchip_get_leakage_v1(struct device *dev, struct device_node *np,
+				   char *lkg_name, int *leakage)
+{
+	struct nvmem_cell *cell;
+	int ret = 0;
+	u8 value = 0;
+
+	cell = of_nvmem_cell_get(np, "leakage");
+	if (IS_ERR(cell)) {
+		ret = rockchip_nvmem_cell_read_u8(np, lkg_name, &value);
+	} else {
+		nvmem_cell_put(cell);
+		ret = rockchip_nvmem_cell_read_u8(np, "leakage", &value);
+	}
+	if (ret)
+		dev_err(dev, "Failed to get %s\n", lkg_name);
+	else
+		*leakage = value;
+
+	return ret;
+}
+
+static int rockchip_get_leakage_v2(struct device *dev, struct device_node *np,
+				   char *lkg_name, int *leakage)
+{
+	int lkg = 0, ret = 0;
+
+	if (rockchip_get_leakage_v1(dev, np, lkg_name, &lkg))
+		return -EINVAL;
+
+	ret = rockchip_adjust_leakage(dev, np, &lkg);
+	if (ret)
+		dev_err(dev, "Failed to adjust leakage, value=%d\n", lkg);
+	else
+		*leakage = lkg;
+
+	return ret;
+}
+
+static int rockchip_get_leakage_v3(struct device *dev, struct device_node *np,
+				   char *lkg_name, int *leakage)
+{
+	int lkg = 0;
+
+	if (rockchip_get_leakage_v1(dev, np, lkg_name, &lkg))
+		return -EINVAL;
+
+	*leakage = (((lkg & 0xf8) >> 3) * 1000) + ((lkg & 0x7) * 125);
+
+	return 0;
+}
+
+int rockchip_of_get_leakage(struct device *dev, char *lkg_name, int *leakage)
+{
+	struct device_node *np;
+	int ret = -EINVAL;
+
+	np = of_parse_phandle(dev->of_node, "operating-points-v2", 0);
+	if (!np) {
+		dev_warn(dev, "OPP-v2 not supported\n");
+		return -ENOENT;
+	}
+
+	rockchip_get_leakage_version(&lkg_version);
+
+	switch (lkg_version) {
+	case LEAKAGE_V1:
+		ret = rockchip_get_leakage_v1(dev, np, lkg_name, leakage);
+		break;
+	case LEAKAGE_V2:
+		ret = rockchip_get_leakage_v2(dev, np, lkg_name, leakage);
+		break;
+	case LEAKAGE_V3:
+		ret = rockchip_get_leakage_v3(dev, np, lkg_name, leakage);
+		if (!ret) {
+			/*
+			 * round up to the nearest whole number for calculating
+			 * static power,  it does not need to be precise.
+			 */
+			if (*leakage % 1000 > 500)
+				*leakage = *leakage / 1000 + 1;
+			else
+				*leakage = *leakage / 1000;
+		}
+		break;
+	default:
+		break;
+	}
+
+	of_node_put(np);
+
+	return ret;
+}
+EXPORT_SYMBOL(rockchip_of_get_leakage);
+
+static void rockchip_of_get_lkg_sel(struct device *dev, struct device_node *np,
+				    char *lkg_name, int process,
+				    int *volt_sel, int *scale_sel)
+{
+	struct property *prop = NULL;
+	int leakage = -EINVAL, ret = 0;
+	char name[NAME_MAX];
+
+	rockchip_get_leakage_version(&lkg_version);
+
+	switch (lkg_version) {
+	case LEAKAGE_V1:
+		ret = rockchip_get_leakage_v1(dev, np, lkg_name, &leakage);
+		if (ret)
+			return;
+		dev_info(dev, "leakage=%d\n", leakage);
+		break;
+	case LEAKAGE_V2:
+		ret = rockchip_get_leakage_v2(dev, np, lkg_name, &leakage);
+		if (ret)
+			return;
+		dev_info(dev, "leakage=%d\n", leakage);
+		break;
+	case LEAKAGE_V3:
+		ret = rockchip_get_leakage_v3(dev, np, lkg_name, &leakage);
+		if (ret)
+			return;
+		dev_info(dev, "leakage=%d.%d\n", leakage / 1000,
+			 leakage % 1000);
+		break;
+	default:
+		return;
+	}
+
+	if (!volt_sel)
+		goto next;
+	if (process >= 0) {
+		snprintf(name, sizeof(name),
+			 "rockchip,p%d-leakage-voltage-sel", process);
+		prop = of_find_property(np, name, NULL);
+	}
+	if (!prop)
+		sprintf(name, "rockchip,leakage-voltage-sel");
+	ret = rockchip_get_sel(np, name, leakage, volt_sel);
+	if (!ret)
+		dev_info(dev, "leakage-volt-sel=%d\n", *volt_sel);
+
+next:
+	if (!scale_sel)
+		return;
+	if (process >= 0) {
+		snprintf(name, sizeof(name),
+			 "rockchip,p%d-leakage-scaling-sel", process);
+		prop = of_find_property(np, name, NULL);
+	}
+	if (!prop)
+		sprintf(name, "rockchip,leakage-scaling-sel");
+	ret = rockchip_get_sel(np, name, leakage, scale_sel);
+	if (!ret)
+		dev_info(dev, "leakage-scale=%d\n", *scale_sel);
+}
+
+static unsigned long rockchip_pvtpll_get_rate(struct rockchip_opp_info *info)
+{
+	unsigned int rate0, rate1, delta;
+	int i;
+
+#define MIN_STABLE_DELTA 3
+	regmap_read(info->grf, info->pvtpll_avg_offset, &rate0);
+	/* max delay 2ms */
+	for (i = 0; i < 20; i++) {
+		udelay(100);
+		regmap_read(info->grf, info->pvtpll_avg_offset, &rate1);
+		delta = abs(rate1 - rate0);
+		rate0 = rate1;
+		if (delta <= MIN_STABLE_DELTA)
+			break;
+	}
+
+	if (delta > MIN_STABLE_DELTA) {
+		dev_err(info->dev, "%s: bad delta: %u\n", __func__, delta);
+		return 0;
+	}
+
+	return rate0 * 1000000;
+}
+
+static int rockchip_pvtpll_parse_dt(struct rockchip_opp_info *info)
+{
+	struct device_node *np;
+	int ret;
+
+	np = of_parse_phandle(info->dev->of_node, "operating-points-v2", 0);
+	if (!np) {
+		dev_warn(info->dev, "OPP-v2 not supported\n");
+		return -ENOENT;
+	}
+
+	ret = of_property_read_u32(np, "rockchip,pvtpll-avg-offset", &info->pvtpll_avg_offset);
+	if (ret)
+		goto out;
+
+	ret = of_property_read_u32(np, "rockchip,pvtpll-min-rate", &info->pvtpll_min_rate);
+	if (ret)
+		goto out;
+
+	ret = of_property_read_u32(np, "rockchip,pvtpll-volt-step", &info->pvtpll_volt_step);
+out:
+	of_node_put(np);
+
+	return ret;
+}
+
+static int rockchip_init_pvtpll_info(struct rockchip_opp_info *info)
+{
+	struct opp_table *opp_table;
+	struct dev_pm_opp *opp;
+	int i = 0, max_count, ret;
+
+	ret = rockchip_pvtpll_parse_dt(info);
+	if (ret)
+		return ret;
+
+	max_count = dev_pm_opp_get_opp_count(info->dev);
+	if (max_count <= 0)
+		return max_count ? max_count : -ENODATA;
+
+	info->opp_table = kcalloc(max_count, sizeof(*info->opp_table), GFP_KERNEL);
+	if (!info->opp_table)
+		return -ENOMEM;
+
+	opp_table = dev_pm_opp_get_opp_table(info->dev);
+	if (!opp_table) {
+		kfree(info->opp_table);
+		info->opp_table = NULL;
+		return -ENOMEM;
+	}
+
+	mutex_lock(&opp_table->lock);
+	list_for_each_entry(opp, &opp_table->opp_list, node) {
+		if (!opp->available)
+			continue;
+
+		info->opp_table[i].u_volt = opp->supplies[0].u_volt;
+		info->opp_table[i].u_volt_min = opp->supplies[0].u_volt_min;
+		info->opp_table[i].u_volt_max = opp->supplies[0].u_volt_max;
+		if (opp_table->regulator_count > 1) {
+			info->opp_table[i].u_volt_mem = opp->supplies[1].u_volt;
+			info->opp_table[i].u_volt_mem_min = opp->supplies[1].u_volt_min;
+			info->opp_table[i].u_volt_mem_max = opp->supplies[1].u_volt_max;
+		}
+		info->opp_table[i++].rate = opp->rates[0];
+	}
+	mutex_unlock(&opp_table->lock);
+
+	dev_pm_opp_put_opp_table(opp_table);
+
+	return 0;
+}
+
+static int rockchip_pvtpll_set_volt(struct device *dev, struct regulator *reg,
+				    int target_uV, int max_uV, char *reg_name)
+{
+	int ret = 0;
+
+	ret = regulator_set_voltage(reg, target_uV, max_uV);
+	if (ret)
+		dev_err(dev, "%s: failed to set %s voltage (%d %d uV): %d\n",
+			__func__, reg_name, target_uV, max_uV, ret);
+
+	return ret;
+}
+
+static int rockchip_pvtpll_set_clk(struct device *dev, struct clk *clk,
+				   unsigned long rate)
+{
+	int ret = 0;
+
+	ret = clk_set_rate(clk, rate);
+	if (ret)
+		dev_err(dev, "%s: failed to set rate %lu Hz, ret:%d\n",
+			__func__, rate, ret);
+
+	return ret;
+}
+
+static void rockchip_pvtpll_calibrate_opp(struct rockchip_opp_info *info)
+{
+	struct opp_table *opp_table;
+	struct dev_pm_opp *opp;
+	struct regulator *reg = NULL, *reg_mem = NULL;
+	unsigned long old_volt = 0, old_volt_mem = 0;
+	unsigned long volt = 0, volt_mem = 0;
+	unsigned long volt_min, volt_max, volt_mem_min, volt_mem_max;
+	unsigned long rate, pvtpll_rate, old_rate, cur_rate, delta0, delta1;
+	int i = 0, max_count, step, cur_step, ret;
+
+	if (!info || !info->grf)
+		return;
+
+	dev_dbg(info->dev, "calibrating opp ...\n");
+	ret = rockchip_init_pvtpll_info(info);
+	if (ret)
+		return;
+
+	max_count = dev_pm_opp_get_opp_count(info->dev);
+	if (max_count <= 0)
+		return;
+
+	opp_table = dev_pm_opp_get_opp_table(info->dev);
+	if (!opp_table)
+		return;
+
+	if (info->clocks) {
+		ret = clk_bulk_prepare_enable(info->nclocks, info->clocks);
+		if (ret) {
+			dev_err(info->dev, "failed to enable opp clks\n");
+			return;
+		}
+	}
+
+	if ((!opp_table->regulators) || IS_ERR(opp_table->clk))
+		goto out_put;
+
+	reg = opp_table->regulators[0];
+	old_volt = regulator_get_voltage(reg);
+	if (opp_table->regulator_count > 1) {
+		reg_mem = opp_table->regulators[1];
+		old_volt_mem = regulator_get_voltage(reg_mem);
+		if (IS_ERR_VALUE(old_volt_mem))
+			goto out_put;
+	}
+	old_rate = clk_get_rate(opp_table->clk);
+	if (IS_ERR_VALUE(old_volt) || IS_ERR_VALUE(old_rate))
+		goto out_put;
+	cur_rate = old_rate;
+
+	step = regulator_get_linear_step(reg);
+	if (!step || info->pvtpll_volt_step > step)
+		step = info->pvtpll_volt_step;
+
+	if (old_rate > info->pvtpll_min_rate * 1000) {
+		if (rockchip_pvtpll_set_clk(info->dev, opp_table->clk,
+					    info->pvtpll_min_rate * 1000))
+			goto out_put;
+	}
+
+	for (i = 0; i < max_count; i++) {
+		rate = info->opp_table[i].rate;
+		if (rate < 1000 * info->pvtpll_min_rate)
+			continue;
+
+		volt = max(volt, info->opp_table[i].u_volt);
+		volt_min = info->opp_table[i].u_volt_min;
+		volt_max = info->opp_table[i].u_volt_max;
+
+		if (opp_table->regulator_count > 1) {
+			volt_mem = max(volt_mem, info->opp_table[i].u_volt_mem);
+			volt_mem_min = info->opp_table[i].u_volt_mem_min;
+			volt_mem_max = info->opp_table[i].u_volt_mem_max;
+			if (rockchip_pvtpll_set_volt(info->dev, reg_mem,
+						     volt_mem, volt_mem_max, "mem"))
+				goto out;
+		}
+		if (rockchip_pvtpll_set_volt(info->dev, reg, volt, volt_max, "vdd"))
+			goto out;
+
+		if (rockchip_pvtpll_set_clk(info->dev, opp_table->clk, rate))
+			goto out;
+		cur_rate = rate;
+		pvtpll_rate = rockchip_pvtpll_get_rate(info);
+		if (!pvtpll_rate)
+			goto out;
+		cur_step = (pvtpll_rate < rate) ? step : -step;
+		delta1 = abs(pvtpll_rate - rate);
+		do {
+			delta0 = delta1;
+			volt += cur_step;
+			if ((volt < volt_min) || (volt > volt_max))
+				break;
+			if (opp_table->regulator_count > 1) {
+				if (volt > volt_mem_max)
+					break;
+				else if (volt < volt_mem_min)
+					volt_mem = volt_mem_min;
+				else
+					volt_mem = volt;
+				if (rockchip_pvtpll_set_volt(info->dev, reg_mem,
+							     volt_mem, volt_mem_max,
+							     "mem"))
+					break;
+			}
+			if (rockchip_pvtpll_set_volt(info->dev, reg, volt,
+						     volt_max, "vdd"))
+				break;
+			pvtpll_rate = rockchip_pvtpll_get_rate(info);
+			if (!pvtpll_rate)
+				goto out;
+			delta1 = abs(pvtpll_rate - rate);
+		} while (delta1 < delta0);
+
+		volt -= cur_step;
+		info->opp_table[i].u_volt = volt;
+		if (opp_table->regulator_count > 1) {
+			if (volt < volt_mem_min)
+				volt_mem = volt_mem_min;
+			else
+				volt_mem = volt;
+			info->opp_table[i].u_volt_mem = volt_mem;
+		}
+	}
+
+	i = 0;
+	mutex_lock(&opp_table->lock);
+	list_for_each_entry(opp, &opp_table->opp_list, node) {
+		if (!opp->available)
+			continue;
+
+		opp->supplies[0].u_volt = info->opp_table[i].u_volt;
+		if (opp_table->regulator_count > 1)
+			opp->supplies[1].u_volt = info->opp_table[i].u_volt_mem;
+		i++;
+	}
+	mutex_unlock(&opp_table->lock);
+	dev_info(info->dev, "opp calibration done\n");
+out:
+	if (cur_rate > old_rate)
+		rockchip_pvtpll_set_clk(info->dev, opp_table->clk, old_rate);
+	if (opp_table->regulator_count > 1)
+		rockchip_pvtpll_set_volt(info->dev, reg_mem, old_volt_mem,
+					 INT_MAX, "mem");
+	rockchip_pvtpll_set_volt(info->dev, reg, old_volt, INT_MAX, "vdd");
+	if (cur_rate < old_rate)
+		rockchip_pvtpll_set_clk(info->dev, opp_table->clk, old_rate);
+out_put:
+	if (info->clocks)
+		clk_bulk_disable_unprepare(info->nclocks, info->clocks);
+	dev_pm_opp_put_opp_table(opp_table);
+}
+
+static void rockchip_pvtpll_add_length(struct rockchip_opp_info *info)
+{
+	struct device_node *np;
+	struct opp_table *opp_table;
+	struct dev_pm_opp *opp;
+	unsigned long old_rate;
+	unsigned int min_rate = 0, max_rate = 0, margin = 0;
+	u32 opp_flag = 0;
+	int ret;
+
+	if (!info)
+		return;
+
+	np = of_parse_phandle(info->dev->of_node, "operating-points-v2", 0);
+	if (!np) {
+		dev_warn(info->dev, "OPP-v2 not supported\n");
+		return;
+	}
+
+	if (of_property_read_u32(np, "rockchip,pvtpll-len-min-rate", &min_rate))
+		goto out;
+	if (of_property_read_u32(np, "rockchip,pvtpll-len-max-rate", &max_rate))
+		goto out;
+	if (of_property_read_u32(np, "rockchip,pvtpll-len-margin", &margin))
+		goto out;
+
+	opp_table = dev_pm_opp_get_opp_table(info->dev);
+	if (!opp_table)
+		goto out;
+	old_rate = clk_get_rate(opp_table->clk);
+	opp_flag = OPP_ADD_LENGTH | ((margin & OPP_LENGTH_MASK) << OPP_LENGTH_SHIFT);
+
+	mutex_lock(&opp_table->lock);
+	list_for_each_entry(opp, &opp_table->opp_list, node) {
+		if (opp->rates[0] < min_rate * 1000 || opp->rates[0] > max_rate * 1000)
+			continue;
+		ret = clk_set_rate(opp_table->clk, opp->rates[0] | opp_flag);
+		if (ret) {
+			dev_err(info->dev,
+				"failed to change %lu len margin %d\n",
+				opp->rates[0], margin);
+			break;
+		}
+	}
+	mutex_unlock(&opp_table->lock);
+
+	clk_set_rate(opp_table->clk, old_rate);
+
+	dev_pm_opp_put_opp_table(opp_table);
+out:
+	of_node_put(np);
+}
+
+static int rockchip_get_pvtm_pvtpll(struct device *dev, struct device_node *np,
+				    const char *reg_name)
+{
+	struct regulator *reg;
+	struct clk *clk;
+	struct pvtm_config *pvtm;
+	unsigned long old_freq;
+	unsigned int old_volt;
+	int cur_temp, diff_temp, prop_temp, diff_value;
+	int pvtm_value = 0;
+	int ret = 0;
+
+	if (!rockchip_nvmem_cell_read_u16(np, "pvtm", (u16 *)&pvtm_value) && pvtm_value) {
+		dev_info(dev, "pvtm = %d, get from otp\n", pvtm_value);
+		return pvtm_value;
+	}
+
+	pvtm = kzalloc(sizeof(*pvtm), GFP_KERNEL);
+	if (!pvtm)
+		return -ENOMEM;
+
+	ret = rockchip_parse_pvtm_config(np, pvtm);
+	if (ret)
+		goto out;
+
+	clk = clk_get(dev, NULL);
+	if (IS_ERR_OR_NULL(clk)) {
+		dev_warn(dev, "Failed to get clk\n");
+		goto out;
+	}
+
+	reg = regulator_get_optional(dev, reg_name);
+	if (IS_ERR_OR_NULL(reg)) {
+		dev_warn(dev, "Failed to get reg\n");
+		clk_put(clk);
+		goto out;
+	}
+	old_freq = clk_get_rate(clk);
+	old_volt = regulator_get_voltage(reg);
+
+	ret = clk_set_rate(clk, pvtm->freq * 1000);
+	if (ret) {
+		dev_err(dev, "Failed to set pvtm freq\n");
+		goto put_reg;
+	}
+	ret = regulator_set_voltage(reg, pvtm->volt, INT_MAX);
+	if (ret) {
+		dev_err(dev, "Failed to set pvtm_volt\n");
+		goto restore_clk;
+	}
+	usleep_range(pvtm->sample_time, pvtm->sample_time + 100);
+
+	ret = regmap_read(pvtm->grf, pvtm->offset, &pvtm_value);
+	if (ret < 0) {
+		dev_err(dev, "failed to get pvtm from 0x%x\n", pvtm->offset);
+		goto resetore_volt;
+	}
+	pvtm->tz->ops->get_temp(pvtm->tz, &cur_temp);
+	diff_temp = (cur_temp / 1000 - pvtm->ref_temp);
+	if (diff_temp < 0)
+		prop_temp = pvtm->temp_prop[0];
+	else
+		prop_temp = pvtm->temp_prop[1];
+	diff_value = diff_temp * prop_temp / 1000;
+	pvtm_value += diff_value;
+
+	dev_info(dev, "pvtm=%d\n", pvtm_value);
+
+resetore_volt:
+	regulator_set_voltage(reg, old_volt, INT_MAX);
+restore_clk:
+	clk_set_rate(clk, old_freq);
+put_reg:
+	regulator_put(reg);
+	clk_put(clk);
+out:
+	kfree(pvtm);
+
+	return pvtm_value;
+}
+
+static int rockchip_get_pvtm(struct device *dev, struct device_node *np,
+			     const char *reg_name)
+{
+	struct regulator *reg;
+	struct clk *clk;
+	unsigned int ch[2];
+	int pvtm = 0;
+	u16 tmp = 0;
+
+	if (!rockchip_nvmem_cell_read_u16(np, "pvtm", &tmp) && tmp) {
+		pvtm = 10 * tmp;
+		dev_info(dev, "pvtm = %d, from nvmem\n", pvtm);
+		return pvtm;
+	}
+
+	if (of_property_read_u32_array(np, "rockchip,pvtm-ch", ch, 2))
+		return -EINVAL;
+
+	if (ch[0] >= PVTM_CH_MAX || ch[1] >= PVTM_SUB_CH_MAX)
+		return -EINVAL;
+
+	if (pvtm_value[ch[0]][ch[1]]) {
+		dev_info(dev, "pvtm = %d, form pvtm_value\n", pvtm_value[ch[0]][ch[1]]);
+		return pvtm_value[ch[0]][ch[1]];
+	}
+
+	clk = clk_get(dev, NULL);
+	if (IS_ERR_OR_NULL(clk)) {
+		dev_warn(dev, "Failed to get clk\n");
+		return PTR_ERR_OR_ZERO(clk);
+	}
+
+	reg = regulator_get_optional(dev, reg_name);
+	if (IS_ERR_OR_NULL(reg)) {
+		dev_warn(dev, "Failed to get reg\n");
+		clk_put(clk);
+		return PTR_ERR_OR_ZERO(reg);
+	}
+
+	rockchip_get_pvtm_specific_value(dev, np, clk, reg, &pvtm);
+
+	regulator_put(reg);
+	clk_put(clk);
+
+	return pvtm;
+}
+
+static void rockchip_of_get_pvtm_sel(struct device *dev, struct device_node *np,
+				     const char *reg_name, int bin, int process,
+				     int *volt_sel, int *scale_sel)
+{
+	struct property *prop = NULL;
+	char name[NAME_MAX];
+	int pvtm, ret;
+	u32 hw = 0;
+
+	if (of_property_read_bool(np, "rockchip,pvtm-pvtpll"))
+		pvtm = rockchip_get_pvtm_pvtpll(dev, np, reg_name);
+	else
+		pvtm = rockchip_get_pvtm(dev, np, reg_name);
+	if (pvtm <= 0)
+		return;
+
+	if (!volt_sel)
+		goto next;
+	if (process >= 0) {
+		snprintf(name, sizeof(name),
+			 "rockchip,p%d-pvtm-voltage-sel", process);
+		prop = of_find_property(np, name, NULL);
+	} else if (bin > 0) {
+		of_property_read_u32(np, "rockchip,pvtm-hw", &hw);
+		if (hw && (hw & BIT(bin))) {
+			sprintf(name, "rockchip,pvtm-voltage-sel-hw");
+			prop = of_find_property(np, name, NULL);
+		}
+		if (!prop) {
+			snprintf(name, sizeof(name),
+				 "rockchip,pvtm-voltage-sel-B%d", bin);
+			prop = of_find_property(np, name, NULL);
+		}
+	}
+	if (!prop)
+		sprintf(name, "rockchip,pvtm-voltage-sel");
+	ret = rockchip_get_sel(np, name, pvtm, volt_sel);
+	if (!ret && volt_sel)
+		dev_info(dev, "pvtm-volt-sel=%d\n", *volt_sel);
+
+next:
+	if (!scale_sel)
+		return;
+	prop = NULL;
+	if (process >= 0) {
+		snprintf(name, sizeof(name),
+			 "rockchip,p%d-pvtm-scaling-sel", process);
+		prop = of_find_property(np, name, NULL);
+	}
+	if (!prop)
+		sprintf(name, "rockchip,pvtm-scaling-sel");
+	ret = rockchip_get_sel(np, name, pvtm, scale_sel);
+	if (!ret)
+		dev_info(dev, "pvtm-scale=%d\n", *scale_sel);
+}
+
+static void rockchip_of_get_bin_sel(struct device *dev, struct device_node *np,
+				    int bin, int *scale_sel)
+{
+	int ret = 0;
+
+	if (!scale_sel || bin < 0)
+		return;
+
+	ret = rockchip_get_bin_sel(np, "rockchip,bin-scaling-sel",
+				   bin, scale_sel);
+	if (!ret)
+		dev_info(dev, "bin-scale=%d\n", *scale_sel);
+}
+
+static void rockchip_of_get_bin_volt_sel(struct device *dev, struct device_node *np,
+					 int bin, int *bin_volt_sel)
+{
+	int ret = 0;
+
+	if (!bin_volt_sel || bin < 0)
+		return;
+
+	ret = rockchip_get_bin_sel(np, "rockchip,bin-voltage-sel",
+				   bin, bin_volt_sel);
+	if (!ret)
+		dev_info(dev, "bin-volt-sel=%d\n", *bin_volt_sel);
+}
+
+void rockchip_get_opp_data(const struct of_device_id *matches,
+			   struct rockchip_opp_info *info)
+{
+	const struct of_device_id *match;
+	struct device_node *node;
+
+	node = of_find_node_by_path("/");
+	match = of_match_node(matches, node);
+	if (match && match->data)
+		info->data = match->data;
+	of_node_put(node);
+}
+EXPORT_SYMBOL(rockchip_get_opp_data);
+
+static int rockchip_get_volt_rm_table(struct device *dev, struct device_node *np,
+				      char *porp_name, struct volt_rm_table **table)
+{
+	struct volt_rm_table *rm_table;
+	const struct property *prop;
+	int count, i;
+
+	prop = of_find_property(np, porp_name, NULL);
+	if (!prop)
+		return -EINVAL;
+
+	if (!prop->value)
+		return -ENODATA;
+
+	count = of_property_count_u32_elems(np, porp_name);
+	if (count < 0)
+		return -EINVAL;
+
+	if (count % 2)
+		return -EINVAL;
+
+	rm_table = devm_kzalloc(dev, sizeof(*rm_table) * (count / 2 + 1),
+				GFP_KERNEL);
+	if (!rm_table)
+		return -ENOMEM;
+
+	for (i = 0; i < count / 2; i++) {
+		of_property_read_u32_index(np, porp_name, 2 * i,
+					   &rm_table[i].volt);
+		of_property_read_u32_index(np, porp_name, 2 * i + 1,
+					   &rm_table[i].rm);
+	}
+
+	rm_table[i].volt = 0;
+	rm_table[i].rm = VOLT_RM_TABLE_END;
+
+	*table = rm_table;
+
+	return 0;
+}
+
+static int rockchip_get_soc_info(struct device *dev, struct device_node *np,
+				 int *bin, int *process)
+{
+	u8 value = 0;
+	int ret = 0;
+
+	if (*bin >= 0 || *process >= 0)
+		return 0;
+
+	if (of_property_match_string(np, "nvmem-cell-names",
+				     "remark_spec_serial_number") >= 0)
+		rockchip_nvmem_cell_read_u8(np, "remark_spec_serial_number", &value);
+
+	if (!value && of_property_match_string(np, "nvmem-cell-names",
+					       "specification_serial_number") >= 0) {
+		ret = rockchip_nvmem_cell_read_u8(np,
+						  "specification_serial_number",
+						  &value);
+		if (ret) {
+			dev_err(dev,
+				"Failed to get specification_serial_number\n");
+			return ret;
+		}
+	}
+
+	/* M */
+	if (value == 0xd)
+		*bin = 1;
+	/* J */
+	else if (value == 0xa)
+		*bin = 2;
+
+	if (*bin < 0)
+		*bin = 0;
+	dev_info(dev, "bin=%d\n", *bin);
+
+	return 0;
+}
+
+static void rockchip_init_pvtpll_table(struct device *dev,
+				       struct rockchip_opp_info *info)
+{
+	struct device_node *np = NULL;
+	struct property *prop = NULL;
+	struct of_phandle_args clkspec = { 0 };
+	struct arm_smccc_res res;
+	char prop_name[NAME_MAX];
+	u32 *value;
+	int count;
+	int ret, i;
+
+	if (!info)
+		return;
+
+	np = of_parse_phandle(dev->of_node, "operating-points-v2", 0);
+	if (!np) {
+		dev_warn(dev, "OPP-v2 not supported\n");
+		return;
+	}
+
+	ret = of_parse_phandle_with_args(dev->of_node, "clocks", "#clock-cells",
+					 0, &clkspec);
+	if (ret)
+		goto out;
+	info->pvtpll_clk_id = clkspec.args[0];
+	of_node_put(clkspec.np);
+
+	res = sip_smc_get_pvtpll_info(PVTPLL_GET_INFO, info->pvtpll_clk_id);
+	if (res.a0)
+		goto out;
+	if (!res.a1)
+		info->pvtpll_low_temp = true;
+
+	if (info->bin > 0) {
+		snprintf(prop_name, sizeof(prop_name),
+			 "rockchip,pvtpll-table-B%d", info->bin);
+		prop = of_find_property(np, prop_name, NULL);
+	}
+	if (!prop)
+		sprintf(prop_name, "rockchip,pvtpll-table");
+
+	prop = of_find_property(np, prop_name, NULL);
+	if (!prop)
+		goto out;
+
+	count = of_property_count_u32_elems(np, prop_name);
+	if (count < 0) {
+		dev_err(dev, "%s: Invalid %s property (%d)\n", __func__,
+			prop_name, count);
+		goto out;
+	} else if (count % 5) {
+		dev_err(dev, "Invalid count of %s\n", prop_name);
+		goto out;
+	}
+
+	value = kmalloc_array(count, sizeof(*value), GFP_KERNEL);
+	if (!value)
+		goto out;
+	ret = of_property_read_u32_array(np, prop_name, value, count);
+	if (ret) {
+		dev_err(dev, "%s: error parsing %s: %d\n", __func__,
+			prop_name, ret);
+		goto free_value;
+	}
+
+	for (i = 0; i < count; i += 5) {
+		res = sip_smc_pvtpll_config(PVTPLL_ADJUST_TABLE,
+					    info->pvtpll_clk_id, value[i],
+					    value[i + 1], value[i + 2],
+					    value[i + 3], value[i + 4]);
+		if (res.a0) {
+			dev_err(dev,
+				"%s: error cfg clk_id=%u %u %u %u %u %u (%d)\n",
+				__func__, info->pvtpll_clk_id, value[i],
+				value[i + 1], value[i + 2], value[i + 3],
+				value[i + 4], (int)res.a0);
+			goto free_value;
+		}
+	}
+
+free_value:
+	kfree(value);
+out:
+	of_node_put(np);
+}
+
+static int rockchip_set_opp_supported_hw(struct device *dev,
+					 struct device_node *np,
+					 struct rockchip_opp_info *info)
+{
+	u32 version = 0, speed = 0;
+
+	if (!of_property_read_bool(np, "rockchip,supported-hw"))
+		return 0;
+	if (info->supported_hw[0] || info->supported_hw[1])
+		return 0;
+
+	if (info->bin >= 0)
+		version = info->bin;
+	if (info->volt_sel >= 0)
+		speed = info->volt_sel;
+	/* SoC Version */
+	info->supported_hw[0] = BIT(version);
+	/* Speed Grade */
+	info->supported_hw[1] = BIT(speed);
+
+	dev_info(dev, "soc version=%d, speed=%d\n", version, speed);
+
+	return 0;
+}
+
+static void rockchip_get_scale_volt_sel(struct device *dev, char *lkg_name,
+					const char *reg_name,
+					struct rockchip_opp_info *info)
+{
+	struct device_node *np;
+	int lkg_scale = 0, pvtm_scale = 0, bin_scale = 0;
+	int lkg_volt_sel = -EINVAL, pvtm_volt_sel = -EINVAL;
+	int bin_volt_sel = -EINVAL;
+
+	np = of_parse_phandle(dev->of_node, "operating-points-v2", 0);
+	if (!np) {
+		dev_warn(dev, "OPP-v2 not supported\n");
+		return;
+	}
+
+	rockchip_of_get_lkg_sel(dev, np, lkg_name, info->process,
+				&lkg_volt_sel, &lkg_scale);
+	rockchip_of_get_pvtm_sel(dev, np, reg_name, info->bin, info->process,
+				 &pvtm_volt_sel, &pvtm_scale);
+	rockchip_of_get_bin_sel(dev, np, info->bin, &bin_scale);
+	rockchip_of_get_bin_volt_sel(dev, np, info->bin, &bin_volt_sel);
+	info->scale = max3(lkg_scale, pvtm_scale, bin_scale);
+	if (bin_volt_sel >= 0)
+		info->volt_sel = bin_volt_sel;
+	else
+		info->volt_sel = max(lkg_volt_sel, pvtm_volt_sel);
+
+	of_node_put(np);
+}
+
+static int rockchip_opp_set_config(struct device *dev, struct rockchip_opp_info *info,
+				   const char *clk_name, const char *reg_name)
+{
+	char name[MAX_PROP_NAME_LEN];
+	struct dev_pm_opp_config config = {0};
+	struct clk *clk = NULL;
+	const char *reg_names[] = {NULL, NULL, NULL};
+	const char *clk_names[] = {NULL, NULL, NULL};
+
+	if (clk_name) {
+		clk = clk_get(dev, clk_name);
+		if (IS_ERR_OR_NULL(clk)) {
+			if (!of_property_read_string_index(dev->of_node,
+							   "clock-names",
+							   0, &clk_name))
+				clk = clk_get(dev, clk_name);
+		}
+		if (!IS_ERR_OR_NULL(clk)) {
+			if (strstr(__clk_get_name(clk), "scmi"))
+				info->is_scmi_clk = true;
+			clk_names[0] = clk_name;
+			config.clk_names = clk_names;
+			clk_put(clk);
+			if (info->data && info->data->config_clks)
+				config.config_clks = info->data->config_clks;
+		}
+	}
+
+	if (info->process >= 0) {
+		if (info->volt_sel >= 0)
+			snprintf(name, MAX_PROP_NAME_LEN, "P%d-L%d",
+				 info->process, info->volt_sel);
+		else
+			snprintf(name, MAX_PROP_NAME_LEN, "P%d", info->process);
+		config.prop_name = name;
+	} else if (info->volt_sel >= 0) {
+		snprintf(name, MAX_PROP_NAME_LEN, "L%d", info->volt_sel);
+		config.prop_name = name;
+	}
+
+	if (info->data && info->data->config_regulators)
+		config.config_regulators = info->data->config_regulators;
+
+	if (info->supported_hw[0] || info->supported_hw[1]) {
+		config.supported_hw = kmemdup(info->supported_hw, 2 * sizeof(u32),
+					      GFP_KERNEL);
+		config.supported_hw_count = 2;
+	}
+
+	if (reg_name) {
+		reg_names[0] = reg_name;
+		if (of_find_property(dev->of_node, "mem-supply", NULL))
+			reg_names[1] = "mem";
+		config.regulator_names = reg_names;
+	}
+
+	info->opp_token = dev_pm_opp_set_config(dev, &config);
+	if (info->opp_token < 0) {
+		dev_err(dev, "failed to set opp config\n");
+		return info->opp_token;
+	}
+
+	return 0;
+}
+
+void rockchip_opp_dvfs_lock(struct rockchip_opp_info *info)
+{
+	if (info)
+		mutex_lock(&info->dvfs_mutex);
+}
+EXPORT_SYMBOL(rockchip_opp_dvfs_lock);
+
+void rockchip_opp_dvfs_unlock(struct rockchip_opp_info *info)
+{
+	if (info)
+		mutex_unlock(&info->dvfs_mutex);
+}
+EXPORT_SYMBOL(rockchip_opp_dvfs_unlock);
+
+static int rockchip_get_opp_clk(struct device *dev, struct device_node *np,
+				struct rockchip_opp_info *info)
+{
+	struct clk_bulk_data *clocks;
+	struct of_phandle_args clkspec;
+	int ret = 0, nclocks = 0, i;
+
+	if (of_find_property(np, "rockchip,opp-clocks", NULL)) {
+		nclocks = of_count_phandle_with_args(np, "rockchip,opp-clocks",
+						     "#clock-cells");
+		if (nclocks <= 0)
+			return 0;
+		clocks = devm_kcalloc(dev, nclocks, sizeof(*clocks), GFP_KERNEL);
+		if (!clocks)
+			return -ENOMEM;
+		for (i = 0; i < nclocks; i++) {
+			ret = of_parse_phandle_with_args(np,
+							 "rockchip,opp-clocks",
+							 "#clock-cells", i,
+							 &clkspec);
+			if (ret < 0) {
+				dev_err(dev, "%s: failed to parse opp clk %d\n",
+					np->name, i);
+				goto error;
+			}
+			clocks[i].clk = of_clk_get_from_provider(&clkspec);
+			of_node_put(clkspec.np);
+			if (IS_ERR(clocks[i].clk)) {
+				ret = PTR_ERR(clocks[i].clk);
+				clocks[i].clk = NULL;
+				dev_err(dev, "%s: failed to get opp clk %d\n",
+					np->name, i);
+				goto error;
+			}
+		}
+	} else {
+		nclocks = of_clk_get_parent_count(np);
+		if (nclocks <= 0)
+			return 0;
+		clocks = devm_kcalloc(dev, nclocks, sizeof(*clocks), GFP_KERNEL);
+		if (!clocks)
+			return -ENOMEM;
+		for (i = 0; i < nclocks; i++) {
+			clocks[i].clk = of_clk_get(np, i);
+			if (IS_ERR(clocks[i].clk)) {
+				ret = PTR_ERR(clocks[i].clk);
+				clocks[i].clk = NULL;
+				dev_err(dev, "%s: failed to get clk %d\n",
+					np->name, i);
+				goto error;
+			}
+		}
+	}
+	info->clocks = clocks;
+	info->nclocks = nclocks;
+
+	return 0;
+error:
+	while (--i >= 0)
+		clk_put(clocks[i].clk);
+	devm_kfree(dev, clocks);
+
+	return ret;
+}
+
+int rockchip_init_opp_info(struct device *dev, struct rockchip_opp_info *info,
+			   char *clk_name, char *reg_name)
+{
+	struct device_node *np;
+	int ret = 0;
+	u32 freq;
+
+	/* Get OPP descriptor node */
+	np = of_parse_phandle(dev->of_node, "operating-points-v2", 0);
+	if (!np) {
+		dev_dbg(dev, "Failed to find operating-points-v2\n");
+		return -ENOENT;
+	}
+	if (!info)
+		return -ENOMEM;
+
+	info->dev = dev;
+	info->bin = -EINVAL;
+	info->process = -EINVAL;
+	info->volt_sel = -EINVAL;
+	info->is_runtime_active = true;
+	mutex_init(&info->dvfs_mutex);
+
+	of_property_read_u32(np, "rockchip,init-freq", &info->init_freq);
+
+	info->grf = syscon_regmap_lookup_by_phandle(np, "rockchip,grf");
+	if (IS_ERR(info->grf))
+		info->grf = NULL;
+	info->dsu_grf = syscon_regmap_lookup_by_phandle(np, "rockchip,dsu-grf");
+	if (IS_ERR(info->dsu_grf))
+		info->dsu_grf = NULL;
+
+	ret = rockchip_get_opp_clk(dev, np, info);
+	if (ret)
+		goto out;
+	if (info->clocks) {
+		ret = clk_bulk_prepare_enable(info->nclocks, info->clocks);
+		if (ret) {
+			dev_err(dev, "failed to enable opp clks\n");
+			goto out;
+		}
+	}
+
+	if (info->data && info->data->set_read_margin) {
+		info->current_rm = UINT_MAX;
+		info->target_rm = UINT_MAX;
+		rockchip_get_volt_rm_table(dev, np, "volt-mem-read-margin",
+					   &info->volt_rm_tbl);
+		of_property_read_u32(np, "low-volt-mem-read-margin",
+				     &info->low_rm);
+		if (!of_property_read_u32(np, "intermediate-threshold-freq",
+					  &freq))
+			info->intermediate_threshold_freq = freq * 1000;
+		rockchip_init_read_margin(dev, info, reg_name);
+	}
+
+	if (info->data && info->data->get_soc_info)
+		info->data->get_soc_info(dev, np, &info->bin, &info->process);
+	rockchip_get_soc_info(dev, np, &info->bin, &info->process);
+
+	rockchip_init_pvtpll_table(dev, info);
+	rockchip_get_scale_volt_sel(dev, "leakage", reg_name, info);
+
+	if (info && info->data && info->data->set_soc_info)
+		info->data->set_soc_info(dev, np, info);
+	rockchip_set_opp_supported_hw(dev, np, info);
+
+	ret = rockchip_opp_set_config(dev, info, clk_name, reg_name);
+
+	if (info->clocks)
+		clk_bulk_disable_unprepare(info->nclocks, info->clocks);
+out:
+	of_node_put(np);
+
+	return ret;
+}
+EXPORT_SYMBOL(rockchip_init_opp_info);
+
+void rockchip_uninit_opp_info(struct device *dev, struct rockchip_opp_info *info)
+{
+	dev_pm_opp_clear_config(info->opp_token);
+}
+EXPORT_SYMBOL(rockchip_uninit_opp_info);
+
+static int rockchip_adjust_opp_by_irdrop(struct device *dev,
+					 struct device_node *np,
+					 unsigned long *safe_rate,
+					 unsigned long *max_rate)
+{
+	struct sel_table *irdrop_table = NULL;
+	struct opp_table *opp_table;
+	struct dev_pm_opp *opp;
+	unsigned long tmp_safe_rate = 0;
+	int evb_irdrop = 0, board_irdrop, delta_irdrop;
+	int opp_rate, i, ret = 0;
+	u32 max_volt = UINT_MAX;
+	bool reach_max_volt = false;
+
+	of_property_read_u32_index(np, "rockchip,max-volt", 0, &max_volt);
+	of_property_read_u32_index(np, "rockchip,evb-irdrop", 0, &evb_irdrop);
+	rockchip_get_sel_table(np, "rockchip,board-irdrop", &irdrop_table);
+
+	opp_table = dev_pm_opp_get_opp_table(dev);
+	if (!opp_table) {
+		ret =  -ENOMEM;
+		goto out;
+	}
+
+	mutex_lock(&opp_table->lock);
+	list_for_each_entry(opp, &opp_table->opp_list, node) {
+		if (!opp->available)
+			continue;
+		if (!irdrop_table) {
+			delta_irdrop = 0;
+		} else {
+			opp_rate = opp->rates[0] / 1000000;
+			board_irdrop = -EINVAL;
+			for (i = 0; irdrop_table[i].sel != SEL_TABLE_END; i++) {
+				if (opp_rate >= irdrop_table[i].min)
+					board_irdrop = irdrop_table[i].sel;
+			}
+			if (board_irdrop == -EINVAL)
+				delta_irdrop = 0;
+			else
+				delta_irdrop = board_irdrop - evb_irdrop;
+		}
+		if ((opp->supplies[0].u_volt + delta_irdrop) <= max_volt) {
+			opp->supplies[0].u_volt += delta_irdrop;
+			opp->supplies[0].u_volt_min += delta_irdrop;
+			if (opp->supplies[0].u_volt_max + delta_irdrop <=
+			    max_volt)
+				opp->supplies[0].u_volt_max += delta_irdrop;
+			else
+				opp->supplies[0].u_volt_max = max_volt;
+			if (!reach_max_volt)
+				tmp_safe_rate = opp->rates[0];
+			if (opp->supplies[0].u_volt == max_volt)
+				reach_max_volt = true;
+		} else {
+			opp->supplies[0].u_volt = max_volt;
+			opp->supplies[0].u_volt_min = max_volt;
+			opp->supplies[0].u_volt_max = max_volt;
+		}
+		if (max_rate)
+			*max_rate = opp->rates[0];
+		if (safe_rate && tmp_safe_rate != opp->rates[0])
+			*safe_rate = tmp_safe_rate;
+	}
+	mutex_unlock(&opp_table->lock);
+
+	dev_pm_opp_put_opp_table(opp_table);
+out:
+	kfree(irdrop_table);
+
+	return ret;
+}
+
+static void rockchip_adjust_opp_by_mbist_vmin(struct device *dev,
+					      struct device_node *np)
+{
+	struct opp_table *opp_table;
+	struct dev_pm_opp *opp;
+	u32 vmin = 0;
+	u8 index = 0;
+
+	if (rockchip_nvmem_cell_read_u8(np, "mbist-vmin", &index))
+		return;
+
+	if (!index)
+		return;
+
+	if (of_property_read_u32_index(np, "mbist-vmin", index-1, &vmin))
+		return;
+
+	opp_table = dev_pm_opp_get_opp_table(dev);
+	if (!opp_table)
+		return;
+
+	mutex_lock(&opp_table->lock);
+	list_for_each_entry(opp, &opp_table->opp_list, node) {
+		if (!opp->available)
+			continue;
+		if (opp->supplies->u_volt < vmin) {
+			opp->supplies->u_volt = vmin;
+			opp->supplies->u_volt_min = vmin;
+		}
+	}
+	mutex_unlock(&opp_table->lock);
+}
+
+static void rockchip_adjust_opp_by_otp(struct device *dev,
+				       struct device_node *np)
+{
+	struct dev_pm_opp *opp;
+	struct opp_table *opp_table;
+	struct otp_opp_info opp_info = {};
+	int ret;
+
+	ret = rockchip_nvmem_cell_read_common(np, "opp-info", &opp_info,
+					      sizeof(opp_info));
+	if (ret || !opp_info.volt)
+		return;
+
+	dev_info(dev, "adjust opp-table by otp: min=%uM, max=%uM, volt=%umV\n",
+		 opp_info.min_freq, opp_info.max_freq, opp_info.volt);
+
+	opp_table = dev_pm_opp_get_opp_table(dev);
+	if (!opp_table)
+		return;
+
+	mutex_lock(&opp_table->lock);
+	list_for_each_entry(opp, &opp_table->opp_list, node) {
+		if (!opp->available)
+			continue;
+		if (opp->rates[0] < opp_info.min_freq * 1000000)
+			continue;
+		if (opp->rates[0] > opp_info.max_freq * 1000000)
+			continue;
+
+		opp->supplies[0].u_volt += opp_info.volt * 1000;
+		if (opp->supplies[0].u_volt > opp->supplies[0].u_volt_max)
+			opp->supplies[0].u_volt = opp->supplies[0].u_volt_max;
+		if (opp_table->regulator_count > 1) {
+			opp->supplies[1].u_volt += opp_info.volt * 1000;
+			if (opp->supplies[1].u_volt > opp->supplies[1].u_volt_max)
+				opp->supplies[1].u_volt = opp->supplies[1].u_volt_max;
+		}
+	}
+	mutex_unlock(&opp_table->lock);
+
+	dev_pm_opp_put_opp_table(opp_table);
+}
+
+static int rockchip_adjust_opp_by_scale(struct device *dev,
+					unsigned long scale_rate)
+{
+	struct dev_pm_opp *opp;
+	unsigned long rate;
+	int i, count, ret = 0;
+
+	count = dev_pm_opp_get_opp_count(dev);
+	if (count <= 0) {
+		ret = count ? count : -ENODATA;
+		goto out;
+	}
+
+	for (i = 0, rate = 0; i < count; i++, rate++) {
+		/* find next rate */
+		opp = dev_pm_opp_find_freq_ceil(dev, &rate);
+		if (IS_ERR(opp)) {
+			ret = PTR_ERR(opp);
+			goto out;
+		}
+		if (opp->rates[0] > scale_rate)
+			dev_pm_opp_disable(dev, opp->rates[0]);
+		dev_pm_opp_put(opp);
+	}
+out:
+	return ret;
+}
+
+static int rockchip_adjust_power_scale(struct device *dev, struct rockchip_opp_info *info)
+{
+	struct device_node *np;
+	struct clk *clk;
+	unsigned long safe_rate = 0, max_rate = 0;
+	int irdrop_scale = 0, opp_scale = 0;
+	u32 target_scale, avs = 0, avs_scale = 0;
+	long scale_rate = 0;
+	int ret = 0;
+
+	np = of_parse_phandle(dev->of_node, "operating-points-v2", 0);
+	if (!np) {
+		dev_warn(dev, "OPP-v2 not supported\n");
+		return -ENOENT;
+	}
+	of_property_read_u32(np, "rockchip,avs-enable", &avs);
+	of_property_read_u32(np, "rockchip,avs", &avs);
+	of_property_read_u32(np, "rockchip,avs-scale", &avs_scale);
+	rockchip_adjust_opp_by_otp(dev, np);
+	rockchip_adjust_opp_by_mbist_vmin(dev, np);
+	rockchip_adjust_opp_by_irdrop(dev, np, &safe_rate, &max_rate);
+
+	dev_info(dev, "avs=%d\n", avs);
+
+	if (!safe_rate && !info->scale)
+		goto out_np;
+
+	clk = of_clk_get_by_name(np, NULL);
+	if (IS_ERR(clk)) {
+		if (!safe_rate)
+			goto out_np;
+		dev_dbg(dev, "Failed to get clk, safe_rate=%lu\n", safe_rate);
+		ret = rockchip_adjust_opp_by_scale(dev, safe_rate);
+		if (ret)
+			dev_err(dev, "Failed to adjust opp table\n");
+		goto out_np;
+	}
+
+	if (safe_rate)
+		irdrop_scale = rockchip_pll_clk_rate_to_scale(clk, safe_rate);
+	target_scale = max(irdrop_scale, info->scale);
+	if (target_scale <= 0)
+		goto out_clk;
+	dev_dbg(dev, "target_scale=%d, irdrop_scale=%d, scale=%d\n",
+		target_scale, irdrop_scale, info->scale);
+
+	if (max_rate)
+		opp_scale = rockchip_pll_clk_rate_to_scale(clk, max_rate);
+	if (avs == AVS_SCALING_RATE) {
+		ret = rockchip_pll_clk_adaptive_scaling(clk, target_scale);
+		if (ret)
+			dev_err(dev, "Failed to adaptive scaling\n");
+		if (opp_scale >= avs_scale)
+			goto out_clk;
+		dev_info(dev, "avs-scale=%d, opp-scale=%d\n", avs_scale,
+			 opp_scale);
+		scale_rate = rockchip_pll_clk_scale_to_rate(clk, avs_scale);
+		if (scale_rate <= 0) {
+			dev_err(dev, "Failed to get avs scale rate, %d\n",
+				avs_scale);
+			goto out_clk;
+		}
+		dev_dbg(dev, "scale_rate=%lu\n", scale_rate);
+		ret = rockchip_adjust_opp_by_scale(dev, scale_rate);
+		if (ret)
+			dev_err(dev, "Failed to adjust opp table\n");
+	} else if (avs == AVS_DELETE_OPP) {
+		if (opp_scale >= target_scale)
+			goto out_clk;
+		dev_info(dev, "target_scale=%d, opp-scale=%d\n", target_scale,
+			 opp_scale);
+		scale_rate = rockchip_pll_clk_scale_to_rate(clk, target_scale);
+		if (scale_rate <= 0) {
+			dev_err(dev, "Failed to get scale rate, %d\n",
+				target_scale);
+			goto out_clk;
+		}
+		dev_dbg(dev, "scale_rate=%lu\n", scale_rate);
+		ret = rockchip_adjust_opp_by_scale(dev, scale_rate);
+		if (ret)
+			dev_err(dev, "Failed to adjust opp table\n");
+	}
+
+out_clk:
+	clk_put(clk);
+out_np:
+	of_node_put(np);
+
+	return ret;
+}
+
+static int rockchip_opp_parse_supplies(struct device *dev,
+				       struct rockchip_opp_info *info)
+{
+	struct opp_table *opp_table;
+
+	opp_table = dev_pm_opp_get_opp_table(dev);
+	if (IS_ERR(opp_table))
+		return PTR_ERR(opp_table);
+
+	if (opp_table->clk)
+		info->clk = opp_table->clk;
+	if (opp_table->regulators)
+		info->regulators = opp_table->regulators;
+	info->regulator_count = opp_table->regulator_count;
+
+	dev_pm_opp_put_opp_table(opp_table);
+
+	return 0;
+}
+
+int rockchip_adjust_opp_table(struct device *dev, struct rockchip_opp_info *info)
+{
+	rockchip_opp_parse_supplies(dev, info);
+	rockchip_adjust_power_scale(dev, info);
+	rockchip_pvtpll_calibrate_opp(info);
+	rockchip_pvtpll_add_length(info);
+
+	return 0;
+}
+EXPORT_SYMBOL(rockchip_adjust_opp_table);
+
+int rockchip_get_read_margin(struct device *dev,
+			     struct rockchip_opp_info *opp_info,
+			     unsigned long volt, u32 *target_rm)
+{
+	int i;
+
+	if (!opp_info || !opp_info->volt_rm_tbl)
+		return 0;
+
+	for (i = 0; opp_info->volt_rm_tbl[i].rm != VOLT_RM_TABLE_END; i++) {
+		if (volt >= opp_info->volt_rm_tbl[i].volt) {
+			opp_info->target_rm = opp_info->volt_rm_tbl[i].rm;
+			break;
+		}
+	}
+	*target_rm = opp_info->target_rm;
+
+	return 0;
+}
+EXPORT_SYMBOL(rockchip_get_read_margin);
+
+int rockchip_set_read_margin(struct device *dev,
+			     struct rockchip_opp_info *opp_info, u32 rm,
+			     bool is_set_rm)
+{
+	if (!is_set_rm || !opp_info)
+		return 0;
+	if (!opp_info || !opp_info->volt_rm_tbl)
+		return 0;
+	if (!opp_info->data || !opp_info->data->set_read_margin)
+		return 0;
+	if (rm == opp_info->current_rm)
+		return 0;
+
+	return opp_info->data->set_read_margin(dev, opp_info, rm);
+}
+EXPORT_SYMBOL(rockchip_set_read_margin);
+
+static int rockchip_init_read_margin(struct device *dev,
+				     struct rockchip_opp_info *opp_info,
+				     const char *reg_name)
+{
+	struct clk *clk;
+	struct regulator *reg;
+	unsigned long cur_rate;
+	int cur_volt, ret = 0;
+	u32 target_rm = UINT_MAX;
+
+	reg = regulator_get_optional(dev, reg_name);
+	if (IS_ERR(reg)) {
+		ret = PTR_ERR(reg);
+		if (ret != -EPROBE_DEFER)
+			dev_err(dev, "%s: no regulator (%s) found: %d\n",
+				__func__, reg_name, ret);
+		return ret;
+	}
+	cur_volt = regulator_get_voltage(reg);
+	if (cur_volt < 0) {
+		ret = cur_volt;
+		if (ret != -EPROBE_DEFER)
+			dev_err(dev, "%s: failed to get (%s) volt: %d\n",
+				__func__, reg_name, ret);
+		goto out;
+	}
+
+	clk = clk_get(dev, NULL);
+	if (IS_ERR(clk)) {
+		ret = PTR_ERR(clk);
+		dev_err(dev, "%s: failed to get clk: %d\n", __func__, ret);
+		goto out;
+	}
+	cur_rate = clk_get_rate(clk);
+
+	rockchip_get_read_margin(dev, opp_info, cur_volt, &target_rm);
+	dev_dbg(dev, "cur_rate=%lu, threshold=%lu, cur_volt=%d, target_rm=%d\n",
+		cur_rate, opp_info->intermediate_threshold_freq,
+		cur_volt, target_rm);
+	if (opp_info->intermediate_threshold_freq &&
+	    cur_rate > opp_info->intermediate_threshold_freq) {
+		clk_set_rate(clk, opp_info->intermediate_threshold_freq);
+		rockchip_set_read_margin(dev, opp_info, target_rm, true);
+		clk_set_rate(clk, cur_rate);
+	} else {
+		rockchip_set_read_margin(dev, opp_info, target_rm, true);
+	}
+
+	clk_put(clk);
+out:
+	regulator_put(reg);
+
+	return ret;
+}
+
+int rockchip_set_intermediate_rate(struct device *dev,
+				   struct rockchip_opp_info *opp_info,
+				   struct clk *clk, unsigned long old_freq,
+				   unsigned long new_freq, bool is_scaling_up,
+				   bool is_set_clk)
+{
+	if (!is_set_clk)
+		return 0;
+	if (!opp_info || !opp_info->volt_rm_tbl)
+		return 0;
+	if (!opp_info->data || !opp_info->data->set_read_margin)
+		return 0;
+	if (opp_info->target_rm == opp_info->current_rm)
+		return 0;
+	/*
+	 * There is no need to set intermediate rate if the new voltage
+	 * and the current voltage are high voltage.
+	 */
+	if ((opp_info->target_rm < opp_info->low_rm) &&
+	    (opp_info->current_rm < opp_info->low_rm))
+		return 0;
+
+	if (is_scaling_up) {
+		/*
+		 * If scaling up and the current frequency is less than
+		 * or equal to intermediate threshold frequency, there is
+		 * no need to set intermediate rate.
+		 */
+		if (opp_info->intermediate_threshold_freq &&
+		    old_freq <= opp_info->intermediate_threshold_freq)
+			return 0;
+		return clk_set_rate(clk, new_freq | OPP_SCALING_UP_INTER);
+	}
+	/*
+	 * If scaling down and the new frequency is less than or equal to
+	 * intermediate threshold frequency , there is no need to set
+	 * intermediate rate and set the new frequency directly.
+	 */
+	if (opp_info->intermediate_threshold_freq &&
+	    new_freq <= opp_info->intermediate_threshold_freq)
+		return clk_set_rate(clk, new_freq);
+
+	return clk_set_rate(clk, new_freq | OPP_SCALING_DOWN_INTER);
+}
+EXPORT_SYMBOL(rockchip_set_intermediate_rate);
+
+static int rockchip_opp_set_volt(struct device *dev, struct regulator *reg,
+				 struct dev_pm_opp_supply *supply, char *reg_name)
+{
+	int ret = 0;
+
+	ret = regulator_set_voltage_triplet(reg, supply->u_volt_min,
+					    supply->u_volt, supply->u_volt_max);
+	if (ret)
+		dev_err(dev, "%s: failed to set voltage (%lu %lu %lu uV): %d\n",
+			reg_name, supply->u_volt_min, supply->u_volt,
+			supply->u_volt_max, ret);
+
+	return ret;
+}
+
+int rockchip_opp_config_regulators(struct device *dev,
+				   struct dev_pm_opp *old_opp,
+				   struct dev_pm_opp *new_opp,
+				   struct regulator **regulators,
+				   unsigned int count,
+				   struct rockchip_opp_info *info)
+{
+	struct regulator *vdd_reg = regulators[0];
+	struct regulator *mem_reg;
+	struct dev_pm_opp_supply old_supplies[2] = { 0 };
+	struct dev_pm_opp_supply new_supplies[2] = { 0 };
+	unsigned long old_freq, freq;
+	u32 target_rm = UINT_MAX;
+	int ret = 0;
+
+	if (count > 1)
+		mem_reg = regulators[1];
+
+	ret = dev_pm_opp_get_supplies(new_opp, new_supplies);
+	if (ret)
+		return ret;
+	ret = dev_pm_opp_get_supplies(old_opp, old_supplies);
+	if (ret)
+		return ret;
+
+	ret = clk_bulk_prepare_enable(info->nclocks, info->clocks);
+	if (ret) {
+		dev_err(dev, "failed to enable opp clks\n");
+		return ret;
+	}
+
+	rockchip_get_read_margin(dev, info, new_supplies[0].u_volt, &target_rm);
+
+	old_freq = dev_pm_opp_get_freq(old_opp);
+	freq = dev_pm_opp_get_freq(new_opp);
+
+	if (count > 1)
+		dev_dbg(dev, "%lu %lu -> %lu %lu (uV)\n",
+			old_supplies[0].u_volt, old_supplies[1].u_volt,
+			new_supplies[0].u_volt, new_supplies[1].u_volt);
+	else
+		dev_dbg(dev, "%lu -> %lu (uV)\n", old_supplies[0].u_volt,
+			new_supplies[0].u_volt);
+
+	if (freq > old_freq) {
+		if (count > 1) {
+			ret = rockchip_opp_set_volt(dev, mem_reg, &new_supplies[1], "mem");
+			if (ret)
+				goto restore_voltage;
+		}
+		ret = rockchip_opp_set_volt(dev, vdd_reg, &new_supplies[0], "vdd");
+		if (ret)
+			goto restore_voltage;
+		rockchip_set_read_margin(dev, info, target_rm, info->is_runtime_active);
+	} else {
+		rockchip_set_read_margin(dev, info, target_rm, info->is_runtime_active);
+		ret = rockchip_opp_set_volt(dev, vdd_reg, &new_supplies[0], "vdd");
+		if (ret)
+			goto restore_voltage;
+		if (count > 1) {
+			ret = rockchip_opp_set_volt(dev, mem_reg, &new_supplies[1], "mem");
+			if (ret)
+				goto restore_voltage;
+		}
+	}
+
+	clk_bulk_disable_unprepare(info->nclocks, info->clocks);
+
+	return 0;
+
+restore_voltage:
+	rockchip_get_read_margin(dev, info, old_supplies[0].u_volt, &target_rm);
+	rockchip_set_read_margin(dev, info, target_rm, info->is_runtime_active);
+
+	if (old_supplies[0].u_volt) {
+		if (count > 1 && old_supplies[1].u_volt) {
+			ret = rockchip_opp_set_volt(dev, mem_reg, &old_supplies[1], "mem");
+			if (ret)
+				goto dis_clks;
+		}
+		ret = rockchip_opp_set_volt(dev, vdd_reg, &old_supplies[0], "vdd");
+		if (ret)
+			goto dis_clks;
+	}
+
+dis_clks:
+	clk_bulk_disable_unprepare(info->nclocks, info->clocks);
+
+	return ret;
+}
+EXPORT_SYMBOL(rockchip_opp_config_regulators);
+
+int rockchip_opp_config_clks(struct device *dev, struct opp_table *opp_table,
+			     struct dev_pm_opp *opp, void *data,
+			     bool scaling_down, struct rockchip_opp_info *info)
+{
+	unsigned long *target = data;
+	int ret;
+
+	if (!info->is_runtime_active)
+		return 0;
+
+	ret = clk_bulk_prepare_enable(info->nclocks, info->clocks);
+	if (ret) {
+		dev_err(dev, "failed to enable opp clks\n");
+		return ret;
+	}
+
+	dev_dbg(dev, "%lu -> %lu (Hz)\n", opp_table->rate_clk_single, *target);
+	ret = clk_set_rate(opp_table->clk, *target);
+	if (ret)
+		dev_err(dev, "failed to set clock rate: %lu\n", *target);
+	else
+		opp_table->rate_clk_single = *target;
+
+	clk_bulk_disable_unprepare(info->nclocks, info->clocks);
+
+	return ret;
+}
+EXPORT_SYMBOL(rockchip_opp_config_clks);
+
+int rockchip_opp_check_rate_volt(struct device *dev, struct rockchip_opp_info *info)
+{
+	struct regulator *vdd_reg = NULL;
+	struct regulator *mem_reg = NULL;
+	struct dev_pm_opp *opp;
+	unsigned long old_rate = 0, new_rate = 0;
+	unsigned long new_volt = 0, new_volt_mem = 0;
+	int old_volt = 0, old_volt_mem = 0;
+	u32 target_rm = UINT_MAX;
+	bool is_set_clk = true;
+	bool is_set_rm = false;
+	int ret = 0;
+
+	if (!info->regulators || !info->clk)
+		return 0;
+
+	vdd_reg = info->regulators[0];
+	old_rate = clk_get_rate(info->clk);
+	old_volt = regulator_get_voltage(vdd_reg);
+	if (info->regulator_count > 1) {
+		mem_reg = info->regulators[1];
+		old_volt_mem = regulator_get_voltage(mem_reg);
+	}
+
+	if (info->init_freq) {
+		new_rate = info->init_freq * 1000;
+		info->init_freq = 0;
+	} else {
+		new_rate = old_rate;
+	}
+	opp = dev_pm_opp_find_freq_ceil(dev, &new_rate);
+	if (IS_ERR(opp)) {
+		opp = dev_pm_opp_find_freq_floor(dev, &new_rate);
+		if (IS_ERR(opp))
+			return PTR_ERR(opp);
+	}
+	new_volt = opp->supplies[0].u_volt;
+	if (info->regulator_count > 1)
+		new_volt_mem = opp->supplies[1].u_volt;
+	dev_pm_opp_put(opp);
+
+	if (old_rate == new_rate && info->is_rate_volt_checked) {
+		if (info->regulator_count > 1) {
+			if (old_volt == new_volt &&
+			    new_volt_mem == old_volt_mem)
+				return 0;
+		} else if (old_volt == new_volt) {
+			return 0;
+		}
+	}
+	if (!new_volt || (info->regulator_count > 1 && !new_volt_mem))
+		return 0;
+
+	ret = clk_bulk_prepare_enable(info->nclocks,  info->clocks);
+	if (ret) {
+		dev_err(dev, "failed to enable opp clks\n");
+		return ret;
+	}
+
+	if (info->is_scmi_clk && !info->is_runtime_active)
+		is_set_clk = false;
+	if (info->data && info->data->set_read_margin && info->is_runtime_active)
+		is_set_rm = true;
+
+	rockchip_get_read_margin(dev, info, new_volt, &target_rm);
+
+	dev_dbg(dev, "%s: %lu Hz --> %lu Hz, %lu %lu uV\n", __func__,
+		old_rate, new_rate, new_volt, new_volt_mem);
+	if (new_rate >= old_rate) {
+		if (old_volt > new_volt) {
+			ret = regulator_set_voltage(vdd_reg, new_volt, INT_MAX);
+			if (ret) {
+				dev_err(dev, "%s: failed to set volt: %lu\n",
+					__func__, new_volt);
+				goto restore_voltage;
+			}
+		}
+		if (info->regulator_count > 1) {
+			ret = regulator_set_voltage(mem_reg, new_volt_mem,
+						    INT_MAX);
+			if (ret) {
+				dev_err(dev, "%s: failed to set volt: %lu\n",
+					__func__, new_volt_mem);
+				goto disable_clk;
+			}
+		}
+		if (old_volt <= new_volt) {
+			ret = regulator_set_voltage(vdd_reg, new_volt, INT_MAX);
+			if (ret) {
+				dev_err(dev, "%s: failed to set volt: %lu\n",
+					__func__, new_volt);
+				goto restore_voltage;
+			}
+		}
+		rockchip_set_read_margin(dev, info, target_rm, is_set_rm);
+		if (new_rate == old_rate)
+			goto out;
+	}
+
+	if (is_set_clk) {
+		ret = clk_set_rate(info->clk, new_rate);
+		if (ret) {
+			dev_err(dev, "%s: failed to set clock rate: %lu\n",
+				__func__, new_rate);
+			goto restore_rm;
+		}
+	}
+
+	if (new_rate < old_rate) {
+		rockchip_set_read_margin(dev, info, target_rm, is_set_rm);
+		ret = regulator_set_voltage(vdd_reg, new_volt,
+					    INT_MAX);
+		if (ret) {
+			dev_err(dev, "%s: failed to set volt: %lu\n",
+				__func__, new_volt);
+			goto restore_freq;
+		}
+		if (info->regulator_count > 1) {
+			ret = regulator_set_voltage(mem_reg, new_volt_mem,
+						    INT_MAX);
+			if (ret) {
+				dev_err(dev, "%s: failed to set volt: %lu\n",
+					__func__, new_volt_mem);
+				goto restore_freq;
+			}
+		}
+	}
+
+out:
+	clk_bulk_disable_unprepare(info->nclocks, info->clocks);
+
+	return 0;
+
+restore_freq:
+	if (is_set_clk && clk_set_rate(info->clk, old_rate))
+		dev_err(dev, "%s: failed to restore old-freq (%lu Hz)\n",
+			__func__, old_rate);
+restore_rm:
+	rockchip_get_read_margin(dev, info, old_volt, &target_rm);
+	rockchip_set_read_margin(dev, info, target_rm, is_set_rm);
+restore_voltage:
+	if (old_volt <= new_volt)
+		regulator_set_voltage(vdd_reg, old_volt, INT_MAX);
+	if (info->regulator_count > 1)
+		regulator_set_voltage(mem_reg, old_volt_mem, INT_MAX);
+	if (old_volt > new_volt)
+		regulator_set_voltage(vdd_reg, old_volt, INT_MAX);
+disable_clk:
+	clk_bulk_disable_unprepare(info->nclocks, info->clocks);
+
+	return ret;
+}
+EXPORT_SYMBOL(rockchip_opp_check_rate_volt);
+
+int rockchip_init_opp_table(struct device *dev, struct rockchip_opp_info *info,
+			    char *clk_name, char *reg_name)
+{
+	int ret = 0;
+
+	ret = rockchip_init_opp_info(dev, info, clk_name, reg_name);
+	if (ret) {
+		dev_err(dev, "failed to init opp info\n");
+		return ret;
+	}
+
+	ret = dev_pm_opp_of_add_table(dev);
+	if (ret) {
+		dev_err(dev, "failed to add opp table\n");
+		rockchip_uninit_opp_info(dev, info);
+		return ret;
+	}
+
+	rockchip_adjust_opp_table(dev, info);
+
+	return 0;
+}
+EXPORT_SYMBOL(rockchip_init_opp_table);
+
+void rockchip_uninit_opp_table(struct device *dev, struct rockchip_opp_info *info)
+{
+	dev_pm_opp_of_remove_table(dev);
+	rockchip_uninit_opp_info(dev, info);
+}
+EXPORT_SYMBOL(rockchip_uninit_opp_table);
+
+MODULE_DESCRIPTION("ROCKCHIP OPP Select");
+MODULE_AUTHOR("Finley Xiao <finley.xiao@rock-chips.com>, Liang Chen <cl@rock-chips.com>");
+MODULE_LICENSE("GPL");
diff --git a/include/linux/clk/rockchip.h b/include/linux/clk/rockchip.h
new file mode 100644
index 000000000..07c563a42
--- /dev/null
+++ b/include/linux/clk/rockchip.h
@@ -0,0 +1,18 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ * Copyright (c) 2020 Rockchip Electronics Co. Ltd.
+ */
+
+#ifndef __LINUX_CLK_ROCKCHIP_H_
+#define __LINUX_CLK_ROCKCHIP_H_
+
+#ifdef CONFIG_ROCKCHIP_CLK_COMPENSATION
+int rockchip_pll_clk_compensation(struct clk *clk, int ppm);
+#else
+static inline int rockchip_pll_clk_compensation(struct clk *clk, int ppm)
+{
+	return -ENOSYS;
+}
+#endif
+
+#endif /* __LINUX_CLK_ROCKCHIP_H_ */
diff --git a/include/linux/rockchip/rockchip_sip.h b/include/linux/rockchip/rockchip_sip.h
new file mode 100644
index 000000000..70b1493ef
--- /dev/null
+++ b/include/linux/rockchip/rockchip_sip.h
@@ -0,0 +1,549 @@
+/* Copyright (c) 2016, Fuzhou Rockchip Electronics Co., Ltd
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+#ifndef __ROCKCHIP_SIP_H
+#define __ROCKCHIP_SIP_H
+
+#include <linux/arm-smccc.h>
+#include <linux/io.h>
+
+/* SMC function IDs for SiP Service queries, compatible with kernel-3.10 */
+#define SIP_ATF_VERSION			0x82000001
+#define SIP_ACCESS_REG			0x82000002
+#define SIP_SUSPEND_MODE		0x82000003
+#define SIP_PENDING_CPUS		0x82000004
+#define SIP_UARTDBG_CFG			0x82000005
+#define SIP_UARTDBG_CFG64		0xc2000005
+#define SIP_MCU_EL3FIQ_CFG		0x82000006
+#define SIP_ACCESS_CHIP_STATE64		0xc2000006
+#define SIP_SECURE_MEM_CONFIG		0x82000007
+#define SIP_ACCESS_CHIP_EXTRA_STATE64	0xc2000007
+#define SIP_DRAM_CONFIG			0x82000008
+#define SIP_SHARE_MEM			0x82000009
+#define SIP_SIP_VERSION			0x8200000a
+#define SIP_REMOTECTL_CFG		0x8200000b
+#define PSCI_SIP_VPU_RESET		0x8200000c
+#define SIP_BUS_CFG			0x8200000d
+#define SIP_LAST_LOG			0x8200000e
+#define SIP_SCMI_AGENT0			0x82000010
+#define SIP_SCMI_AGENT1			0x82000011
+#define SIP_SCMI_AGENT2			0x82000012
+#define SIP_SCMI_AGENT3			0x82000013
+#define SIP_SCMI_AGENT4			0x82000014
+#define SIP_SCMI_AGENT5			0x82000015
+#define SIP_SCMI_AGENT6			0x82000016
+#define SIP_SCMI_AGENT7			0x82000017
+#define SIP_SCMI_AGENT8			0x82000018
+#define SIP_SCMI_AGENT9			0x82000019
+#define SIP_SCMI_AGENT10		0x8200001a
+#define SIP_SCMI_AGENT11		0x8200001b
+#define SIP_SCMI_AGENT12		0x8200001c
+#define SIP_SCMI_AGENT13		0x8200001d
+#define SIP_SCMI_AGENT14		0x8200001e
+#define SIP_SCMI_AGENT15		0x8200001f
+#define SIP_SDEI_FIQ_DBG_SWITCH_CPU	0x82000020
+#define SIP_SDEI_FIQ_DBG_GET_EVENT_ID	0x82000021
+#define RK_SIP_AMP_CFG			0x82000022
+#define RK_SIP_FIQ_CTRL			0x82000024
+#define SIP_HDCP_CONFIG			0x82000025
+#define SIP_WDT_CFG			0x82000026
+#define SIP_HDMIRX_CFG			0x82000027
+#define SIP_MCU_CFG			0x82000028
+#define SIP_PVTPLL_CFG			0x82000029
+
+#define TRUSTED_OS_HDCPKEY_INIT		0xB7000003
+
+/* Rockchip Sip version */
+#define SIP_IMPLEMENT_V1                (1)
+#define SIP_IMPLEMENT_V2                (2)
+
+/* Trust firmware version */
+#define ATF_VER_MAJOR(ver)		(((ver) >> 16) & 0xffff)
+#define ATF_VER_MINOR(ver)		(((ver) >> 0) & 0xffff)
+
+/* SIP_ACCESS_REG: read or write */
+#define SECURE_REG_RD			0x0
+#define SECURE_REG_WR			0x1
+
+/* Fiq debugger share memory: 8KB enough */
+#define FIQ_UARTDBG_PAGE_NUMS		2
+#define FIQ_UARTDBG_SHARE_MEM_SIZE	((FIQ_UARTDBG_PAGE_NUMS) * 4096)
+
+/* Error return code */
+#define IS_SIP_ERROR(x)			(!!(x))
+
+#define SIP_RET_SUCCESS			0
+#define SIP_RET_SMC_UNKNOWN		-1
+#define SIP_RET_NOT_SUPPORTED		-2
+#define SIP_RET_INVALID_PARAMS		-3
+#define SIP_RET_INVALID_ADDRESS		-4
+#define SIP_RET_DENIED			-5
+#define SIP_RET_SET_RATE_TIMEOUT	-6
+
+/* SIP_UARTDBG_CFG64 call types */
+#define UARTDBG_CFG_INIT		0xf0
+#define UARTDBG_CFG_OSHDL_TO_OS		0xf1
+#define UARTDBG_CFG_OSHDL_CPUSW		0xf3
+#define UARTDBG_CFG_OSHDL_DEBUG_ENABLE	0xf4
+#define UARTDBG_CFG_OSHDL_DEBUG_DISABLE	0xf5
+#define UARTDBG_CFG_PRINT_PORT		0xf7
+#define UARTDBG_CFG_FIQ_ENABEL		0xf8
+#define UARTDBG_CFG_FIQ_DISABEL		0xf9
+
+/* SIP_SUSPEND_MODE32 call types */
+#define SUSPEND_MODE_CONFIG		0x01
+#define WKUP_SOURCE_CONFIG		0x02
+#define PWM_REGULATOR_CONFIG		0x03
+#define GPIO_POWER_CONFIG		0x04
+#define SUSPEND_DEBUG_ENABLE		0x05
+#define APIOS_SUSPEND_CONFIG		0x06
+#define VIRTUAL_POWEROFF		0x07
+#define SUSPEND_WFI_TIME_MS		0x08
+#define LINUX_PM_STATE			0x09
+#define SUSPEND_IO_RET_CONFIG		0x0a
+#define SLEEP_PIN_CONFIG		0x0b
+
+/* SIP_REMOTECTL_CFG call types */
+#define	REMOTECTL_SET_IRQ		0xf0
+#define REMOTECTL_SET_PWM_CH		0xf1
+#define REMOTECTL_SET_PWRKEY		0xf2
+#define REMOTECTL_GET_WAKEUP_STATE	0xf3
+#define REMOTECTL_ENABLE		0xf4
+/* wakeup state */
+#define REMOTECTL_PWRKEY_WAKEUP		0xdeadbeaf
+
+/* SIP_MCU_CFG child configs, MCU ID */
+enum {
+	RK_BUS_MCU,
+	RK_PMU_MCU,
+	RK_DDR_MCU,
+	RK_NPU_MCU,
+};
+
+#define RK_SIP_MCU_ID(type, id)		((type) << 8 | id)
+
+#define RK_SIP_CFG_BUSMCU_0_ID		RK_SIP_MCU_ID(RK_BUS_MCU, 0)
+#define RK_SIP_CFG_BUSMCU_1_ID		RK_SIP_MCU_ID(RK_BUS_MCU, 1)
+#define RK_SIP_CFG_PMUMCU_0_ID		RK_SIP_MCU_ID(RK_PMU_MCU, 0)
+#define RK_SIP_CFG_DDRMCU_0_ID		RK_SIP_MCU_ID(RK_DDR_MCU, 0)
+#define RK_SIP_CFG_NPUMCU_0_ID		RK_SIP_MCU_ID(RK_NPU_MCU, 0)
+
+/* SIP_MCU_CFG child configs */
+#define CONFIG_MCU_CODE_START_ADDR	0x01
+#define CONFIG_MCU_EXPERI_START_ADDR	0x02
+#define CONFIG_MCU_SRAM_START_ADDR	0x03
+#define CONFIG_MCU_EXSRAM_START_ADDR	0x04
+
+struct dram_addrmap_info {
+	u64 ch_mask[2];
+	u64 bk_mask[4];
+	u64 bg_mask[2];
+	u64 cs_mask[2];
+	u32 reserved[20];
+	u32 bank_bit_first;
+	u32 bank_bit_mask;
+};
+
+/* AMP Ctrl */
+enum {
+	RK_AMP_SUB_FUNC_CFG_MODE = 0,
+	RK_AMP_SUB_FUNC_BOOT_ARG01,
+	RK_AMP_SUB_FUNC_BOOT_ARG23,
+	RK_AMP_SUB_FUNC_REQ_CPU_OFF,
+	RK_AMP_SUB_FUNC_GET_CPU_STATUS,
+	RK_AMP_SUB_FUNC_RSV, /* for RTOS */
+	RK_AMP_SUB_FUNC_CPU_ON,
+	RK_AMP_SUB_FUNC_END,
+};
+
+enum {
+	FIRMWARE_NONE,
+	FIRMWARE_TEE_32BIT,
+	FIRMWARE_ATF_32BIT,
+	FIRMWARE_ATF_64BIT,
+	FIRMWARE_END,
+};
+
+/* Share mem page types */
+typedef enum {
+	SHARE_PAGE_TYPE_INVALID = 0,
+	SHARE_PAGE_TYPE_UARTDBG,
+	SHARE_PAGE_TYPE_DDR,
+	SHARE_PAGE_TYPE_DDRDBG,
+	SHARE_PAGE_TYPE_DDRECC,
+	SHARE_PAGE_TYPE_DDRFSP,
+	SHARE_PAGE_TYPE_DDR_ADDRMAP,
+	SHARE_PAGE_TYPE_LAST_LOG,
+	SHARE_PAGE_TYPE_HDCP,
+	SHARE_PAGE_TYPE_SLEEP,
+	SHARE_PAGE_TYPE_MAX,
+} share_page_type_t;
+
+/* fiq control sub func */
+enum {
+	RK_SIP_FIQ_CTRL_FIQ_EN = 1,
+	RK_SIP_FIQ_CTRL_FIQ_DIS,
+	RK_SIP_FIQ_CTRL_SET_AFF
+};
+
+/* hdcp function types */
+enum {
+	HDCP_FUNC_STORAGE_INCRYPT = 1,
+	HDCP_FUNC_KEY_LOAD,
+	HDCP_FUNC_ENCRYPT_MODE
+};
+
+/* support hdcp device list */
+enum {
+	DP_TX0,
+	DP_TX1,
+	EDP_TX0,
+	EDP_TX1,
+	HDMI_TX0,
+	HDMI_TX1,
+	HDMI_RX,
+	MAX_DEVICE,
+};
+
+/* SIP_WDT_CONFIG call types  */
+enum {
+	WDT_START = 0,
+	WDT_STOP = 1,
+	WDT_PING = 2,
+};
+
+/* SIP_HDMIRX_CONFIG child configs */
+enum {
+	HDMIRX_AUTO_TOUCH_EN = 0,
+	HDMIRX_REG_PRE_FETCH = 1,
+	HDMIRX_INFO_NOTIFY = 2,
+};
+
+/* SIP_PVTPLL_CFG child configs */
+enum {
+	PVTPLL_GET_INFO = 0,
+	PVTPLL_ADJUST_TABLE = 1,
+	PVTPLL_LOW_TEMP = 2,
+};
+
+struct pt_regs;
+typedef void (*sip_fiq_debugger_uart_irq_tf_cb_t)(struct pt_regs *_pt_regs, unsigned long cpu);
+
+/*
+ * Rules: struct arm_smccc_res contains result and data, details:
+ *
+ * a0: error code(0: success, !0: error);
+ * a1~a3: data
+ */
+#if IS_REACHABLE(CONFIG_ROCKCHIP_SIP)
+struct arm_smccc_res sip_smc_get_atf_version(void);
+struct arm_smccc_res sip_smc_get_sip_version(void);
+struct arm_smccc_res sip_smc_dram(u32 arg0, u32 arg1, u32 arg2);
+struct arm_smccc_res sip_smc_request_share_mem(u32 page_num,
+					       share_page_type_t page_type);
+struct arm_smccc_res sip_smc_mcu_el3fiq(u32 arg0, u32 arg1, u32 arg2);
+struct arm_smccc_res sip_smc_vpu_reset(u32 arg0, u32 arg1, u32 arg2);
+struct arm_smccc_res sip_smc_get_suspend_info(u32 info);
+struct arm_smccc_res sip_smc_lastlog_request(void);
+
+int sip_smc_set_suspend_mode(u32 ctrl, u32 config1, u32 config2);
+int sip_smc_virtual_poweroff(void);
+int sip_smc_remotectl_config(u32 func, u32 data);
+
+int sip_smc_secure_reg_write(u32 addr_phy, u32 val);
+u32 sip_smc_secure_reg_read(u32 addr_phy);
+struct arm_smccc_res sip_smc_bus_config(u32 arg0, u32 arg1, u32 arg2);
+struct dram_addrmap_info *sip_smc_get_dram_map(void);
+int sip_smc_amp_config(u32 sub_func_id, u32 arg1, u32 arg2, u32 arg3);
+struct arm_smccc_res sip_smc_get_amp_info(u32 sub_func_id, u32 arg1);
+struct arm_smccc_res sip_smc_get_pvtpll_info(u32 sub_func_id, u32 arg1);
+struct arm_smccc_res sip_smc_pvtpll_config(u32 sub_func_id, u32 arg1, u32 arg2,
+					   u32 arg3, u32 arg4, u32 arg5, u32 arg6);
+
+void __iomem *sip_hdcp_request_share_memory(int id);
+struct arm_smccc_res sip_hdcp_config(u32 arg0, u32 arg1, u32 arg2);
+ulong sip_cpu_logical_map_mpidr(u32 cpu);
+/***************************fiq debugger **************************************/
+void sip_fiq_debugger_enable_fiq(bool enable, uint32_t tgt_cpu);
+void sip_fiq_debugger_enable_debug(bool enable);
+int sip_fiq_debugger_uart_irq_tf_init(u32 irq_id, sip_fiq_debugger_uart_irq_tf_cb_t callback_fn);
+int sip_fiq_debugger_set_print_port(u32 port_phyaddr, u32 baudrate);
+int sip_fiq_debugger_request_share_memory(void);
+int sip_fiq_debugger_get_target_cpu(void);
+int sip_fiq_debugger_switch_cpu(u32 cpu);
+int sip_fiq_debugger_sdei_switch_cpu(u32 cur_cpu, u32 target_cpu, u32 flag);
+int sip_fiq_debugger_is_enabled(void);
+int sip_fiq_debugger_sdei_get_event_id(u32 *fiq, u32 *sw_cpu, u32 *flag);
+int sip_fiq_control(u32 sub_func, u32 irq, unsigned long data);
+int sip_wdt_config(u32 sub_func, u32 arg1, u32 arg2, u32 arg3);
+int sip_hdmirx_config(u32 sub_func, u32 arg1, u32 arg2, u32 arg3);
+int sip_hdcpkey_init(u32 hdcp_id);
+int sip_smc_mcu_config(unsigned long mcu_id, unsigned long func, unsigned long arg2);
+#else
+static inline struct arm_smccc_res sip_smc_get_atf_version(void)
+{
+	struct arm_smccc_res tmp = { .a0 = SIP_RET_NOT_SUPPORTED };
+	return tmp;
+}
+
+static inline struct arm_smccc_res sip_smc_get_sip_version(void)
+{
+	struct arm_smccc_res tmp = { .a0 = SIP_RET_NOT_SUPPORTED };
+	return tmp;
+}
+
+static inline struct arm_smccc_res sip_smc_dram(u32 arg0, u32 arg1, u32 arg2)
+{
+	struct arm_smccc_res tmp = { .a0 = SIP_RET_NOT_SUPPORTED };
+	return tmp;
+}
+
+static inline struct arm_smccc_res sip_smc_request_share_mem
+			(u32 page_num, share_page_type_t page_type)
+{
+	struct arm_smccc_res tmp = { .a0 = SIP_RET_NOT_SUPPORTED };
+	return tmp;
+}
+
+static inline struct arm_smccc_res sip_smc_mcu_el3fiq
+			(u32 arg0, u32 arg1, u32 arg2)
+{
+	struct arm_smccc_res tmp = { .a0 = SIP_RET_NOT_SUPPORTED };
+	return tmp;
+}
+
+static inline struct arm_smccc_res
+sip_smc_vpu_reset(u32 arg0, u32 arg1, u32 arg2)
+{
+	struct arm_smccc_res tmp = { .a0 = SIP_RET_NOT_SUPPORTED };
+	return tmp;
+}
+
+static inline struct arm_smccc_res sip_smc_get_suspend_info(u32 info)
+{
+	struct arm_smccc_res tmp = { .a0 = SIP_RET_NOT_SUPPORTED };
+	return tmp;
+}
+
+static inline struct arm_smccc_res sip_smc_lastlog_request(void)
+{
+	struct arm_smccc_res tmp = { .a0 = SIP_RET_NOT_SUPPORTED };
+	return tmp;
+}
+
+static inline int sip_smc_set_suspend_mode(u32 ctrl, u32 config1, u32 config2)
+{
+	return 0;
+}
+
+static inline int sip_smc_virtual_poweroff(void) { return 0; }
+static inline int sip_smc_remotectl_config(u32 func, u32 data) { return 0; }
+static inline int sip_smc_secure_reg_write(u32 addr_phy, u32 val) { return 0; }
+static inline u32 sip_smc_secure_reg_read(u32 addr_phy) { return 0; }
+
+static inline struct arm_smccc_res sip_smc_bus_config(u32 arg0, u32 arg1, u32 arg2)
+{
+	struct arm_smccc_res tmp = { .a0 = SIP_RET_NOT_SUPPORTED };
+	return tmp;
+}
+
+static inline struct dram_addrmap_info *sip_smc_get_dram_map(void)
+{
+	return NULL;
+}
+
+static inline int sip_smc_amp_config(u32 sub_func_id,
+				     u32 arg1,
+				     u32 arg2,
+				     u32 arg3)
+{
+	return 0;
+}
+
+static inline struct arm_smccc_res sip_smc_get_amp_info(u32 sub_func_id,
+							u32 arg1)
+{
+	struct arm_smccc_res tmp = { .a0 = SIP_RET_NOT_SUPPORTED, };
+
+	return tmp;
+}
+
+static inline struct arm_smccc_res sip_smc_get_pvtpll_info(u32 sub_func_id,
+							   u32 arg1)
+{
+	struct arm_smccc_res tmp = { .a0 = SIP_RET_NOT_SUPPORTED, };
+
+	return tmp;
+}
+
+static inline struct arm_smccc_res sip_smc_pvtpll_config(u32 sub_func_id,
+							 u32 arg1, u32 arg2,
+							 u32 arg3, u32 arg4,
+							 u32 arg5, u32 arg6)
+{
+	struct arm_smccc_res tmp = { .a0 = SIP_RET_NOT_SUPPORTED, };
+
+	return tmp;
+}
+
+static inline void __iomem *sip_hdcp_request_share_memory(int id)
+{
+	return NULL;
+}
+
+static inline struct arm_smccc_res sip_hdcp_config(u32 arg0, u32 arg1, u32 arg2)
+{
+	struct arm_smccc_res tmp = { .a0 = SIP_RET_NOT_SUPPORTED };
+
+	return tmp;
+}
+
+static inline ulong sip_cpu_logical_map_mpidr(u32 cpu) { return 0; }
+
+/***************************fiq debugger **************************************/
+static inline void sip_fiq_debugger_enable_fiq
+			(bool enable, uint32_t tgt_cpu) { return; }
+
+static inline void sip_fiq_debugger_enable_debug(bool enable) { return; }
+static inline int sip_fiq_debugger_uart_irq_tf_init(u32 irq_id,
+						    sip_fiq_debugger_uart_irq_tf_cb_t callback_fn)
+{
+	return 0;
+}
+
+static inline int sip_fiq_debugger_set_print_port(u32 port_phyaddr,
+						  u32 baudrate)
+{
+	return 0;
+}
+
+static inline int sip_fiq_debugger_request_share_memory(void) { return 0; }
+static inline int sip_fiq_debugger_get_target_cpu(void) { return 0; }
+static inline int sip_fiq_debugger_switch_cpu(u32 cpu) { return 0; }
+static inline int sip_fiq_debugger_sdei_switch_cpu(u32 cur_cpu, u32 target_cpu,
+						   u32 flag) { return 0; }
+static inline int sip_fiq_debugger_is_enabled(void) { return 0; }
+static inline int sip_fiq_debugger_sdei_get_event_id(u32 *fiq, u32 *sw_cpu, u32 *flag)
+{
+	return SIP_RET_NOT_SUPPORTED;
+}
+
+static inline int sip_fiq_control(u32 sub_func, u32 irq, unsigned long data)
+{
+	return 0;
+}
+
+static inline int sip_wdt_config(u32 sub_func,
+				 u32 arg1,
+				 u32 arg2,
+				 u32 arg3)
+{
+	return 0;
+}
+
+static inline int sip_hdmirx_config(u32 sub_func,
+				    u32 arg1,
+				    u32 arg2,
+				    u32 arg3)
+{
+	return SIP_RET_NOT_SUPPORTED;
+}
+
+static inline int sip_hdcpkey_init(u32 hdcp_id)
+{
+	return 0;
+}
+
+static inline int sip_smc_mcu_config(unsigned long mcu_id,
+				     unsigned long func,
+				     unsigned long arg2)
+{
+	return SIP_RET_NOT_SUPPORTED;
+}
+#endif
+
+/* 32-bit OP-TEE context, never change order of members! */
+struct sm_nsec_ctx {
+	u32 usr_sp;
+	u32 usr_lr;
+	u32 irq_spsr;
+	u32 irq_sp;
+	u32 irq_lr;
+	u32 fiq_spsr;
+	u32 fiq_sp;
+	u32 fiq_lr;
+	u32 svc_spsr;
+	u32 svc_sp;
+	u32 svc_lr;
+	u32 abt_spsr;
+	u32 abt_sp;
+	u32 abt_lr;
+	u32 und_spsr;
+	u32 und_sp;
+	u32 und_lr;
+	u32 mon_lr;
+	u32 mon_spsr;
+	u32 r4;
+	u32 r5;
+	u32 r6;
+	u32 r7;
+	u32 r8;
+	u32 r9;
+	u32 r10;
+	u32 r11;
+	u32 r12;
+	u32 r0;
+	u32 r1;
+	u32 r2;
+	u32 r3;
+};
+
+/* 64-bit ATF context, never change order of members! */
+struct gp_regs_ctx {
+	u64 x0;
+	u64 x1;
+	u64 x2;
+	u64 x3;
+	u64 x4;
+	u64 x5;
+	u64 x6;
+	u64 x7;
+	u64 x8;
+	u64 x9;
+	u64 x10;
+	u64 x11;
+	u64 x12;
+	u64 x13;
+	u64 x14;
+	u64 x15;
+	u64 x16;
+	u64 x17;
+	u64 x18;
+	u64 x19;
+	u64 x20;
+	u64 x21;
+	u64 x22;
+	u64 x23;
+	u64 x24;
+	u64 x25;
+	u64 x26;
+	u64 x27;
+	u64 x28;
+	u64 x29;
+	u64 lr;
+	u64 sp_el0;
+	u64 scr_el3;
+	u64 runtime_sp;
+	u64 spsr_el3;
+	u64 elr_el3;
+};
+
+#endif
diff --git a/include/linux/soc/rockchip/pvtm.h b/include/linux/soc/rockchip/pvtm.h
new file mode 100644
index 000000000..3d2495cfd
--- /dev/null
+++ b/include/linux/soc/rockchip/pvtm.h
@@ -0,0 +1,17 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __SOC_ROCKCHIP_PVTM_H
+#define __SOC_ROCKCHIP_PVTM_H
+
+#if IS_ENABLED(CONFIG_ROCKCHIP_PVTM)
+u32 rockchip_get_pvtm_value(unsigned int id, unsigned int ring_sel,
+			    unsigned int time_us);
+#else
+static inline u32 rockchip_get_pvtm_value(unsigned int id,
+					  unsigned int ring_sel,
+					  unsigned int time_us)
+{
+	return 0;
+}
+#endif
+
+#endif /* __SOC_ROCKCHIP_PVTM_H */
diff --git a/include/linux/soc/rockchip/rk_vendor_storage.h b/include/linux/soc/rockchip/rk_vendor_storage.h
new file mode 100644
index 000000000..29cee9bf6
--- /dev/null
+++ b/include/linux/soc/rockchip/rk_vendor_storage.h
@@ -0,0 +1,59 @@
+/*
+ * Copyright (c) 2016, Fuzhou Rockchip Electronics Co., Ltd
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or (at
+ * your option) any later version.
+ */
+
+#ifndef __PLAT_RK_VENDOR_STORAGE_H
+#define __PLAT_RK_VENDOR_STORAGE_H
+
+#define RSV_ID				0
+#define SN_ID				1
+#define WIFI_MAC_ID			2
+#define LAN_MAC_ID			3
+#define BT_MAC_ID			4
+#define HDCP_14_HDMI_ID			5
+#define HDCP_14_DP_ID			6
+#define HDCP_2X_ID			7
+#define DRM_KEY_ID			8
+#define PLAYREADY_CERT_ID		9
+#define ATTENTION_KEY_ID		10
+#define PLAYREADY_ROOT_KEY_0_ID		11
+#define PLAYREADY_ROOT_KEY_1_ID		12
+#define HDCP_14_HDMIRX_ID		13
+#define SENSOR_CALIBRATION_ID		14
+#define IMEI_ID				15
+#define LAN_RGMII_DL_ID			16
+#define EINK_VCOM_ID			17
+
+#if IS_REACHABLE(CONFIG_ROCKCHIP_VENDOR_STORAGE)
+int rk_vendor_read(u32 id, void *pbuf, u32 size);
+int rk_vendor_write(u32 id, void *pbuf, u32 size);
+int rk_vendor_register(void *read, void *write);
+bool is_rk_vendor_ready(void);
+#else
+static inline int rk_vendor_read(u32 id, void *pbuf, u32 size)
+{
+	return -1;
+}
+
+static inline int rk_vendor_write(u32 id, void *pbuf, u32 size)
+{
+	return -1;
+}
+
+static inline int rk_vendor_register(void *read, void *write)
+{
+	return -1;
+}
+
+static inline bool is_rk_vendor_ready(void)
+{
+	return false;
+}
+#endif
+
+#endif
diff --git a/include/soc/rockchip/rockchip_iommu.h b/include/soc/rockchip/rockchip_iommu.h
new file mode 100644
index 000000000..804f0fdc4
--- /dev/null
+++ b/include/soc/rockchip/rockchip_iommu.h
@@ -0,0 +1,39 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2021 Rockchip Electronics Co., Ltd
+ */
+#ifndef __SOC_ROCKCHIP_IOMMU_H
+#define __SOC_ROCKCHIP_IOMMU_H
+
+struct device;
+
+#if IS_ENABLED(CONFIG_ROCKCHIP_IOMMU)
+int rockchip_iommu_enable(struct device *dev);
+int rockchip_iommu_disable(struct device *dev);
+int rockchip_pagefault_done(struct device *master_dev);
+void __iomem *rockchip_get_iommu_base(struct device *master_dev, int idx);
+bool rockchip_iommu_is_enabled(struct device *dev);
+#else
+static inline int rockchip_iommu_enable(struct device *dev)
+{
+	return -ENODEV;
+}
+static inline int rockchip_iommu_disable(struct device *dev)
+{
+	return -ENODEV;
+}
+static inline int rockchip_pagefault_done(struct device *master_dev)
+{
+	return 0;
+}
+static inline void __iomem *rockchip_get_iommu_base(struct device *master_dev, int idx)
+{
+	return NULL;
+}
+static inline bool rockchip_iommu_is_enabled(struct device *dev)
+{
+	return false;
+}
+#endif
+
+#endif
diff --git a/include/soc/rockchip/rockchip_ipa.h b/include/soc/rockchip/rockchip_ipa.h
new file mode 100644
index 000000000..cb333f463
--- /dev/null
+++ b/include/soc/rockchip/rockchip_ipa.h
@@ -0,0 +1,40 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2018 Fuzhou Rockchip Electronics Co., Ltd
+ */
+#ifndef __SOC_ROCKCHIP_IPA_H
+#define __SOC_ROCKCHIP_IPA_H
+
+struct ipa_power_model_data {
+	u32 static_coefficient;
+	u32 dynamic_coefficient;
+	s32 ts[4];			/* temperature scaling factor */
+	struct thermal_zone_device *tz;
+	u32 leakage;
+	u32 ref_leakage;
+	u32 lkg_range[2];		/* min leakage and max leakage */
+	s32 ls[3];			/* leakage scaling factor */
+};
+
+#if IS_ENABLED(CONFIG_ROCKCHIP_IPA)
+struct ipa_power_model_data *rockchip_ipa_power_model_init(struct device *dev,
+							   char *lkg_name);
+unsigned long
+rockchip_ipa_get_static_power(struct ipa_power_model_data *model_data,
+			      unsigned long voltage_mv);
+#else
+static inline struct ipa_power_model_data *
+rockchip_ipa_power_model_init(struct device *dev, char *lkg_name)
+{
+	return ERR_PTR(-ENOTSUPP);
+};
+
+static inline unsigned long
+rockchip_ipa_get_static_power(struct ipa_power_model_data *data,
+			      unsigned long voltage_mv)
+{
+	return 0;
+}
+#endif /* CONFIG_ROCKCHIP_IPA */
+
+#endif
diff --git a/include/soc/rockchip/rockchip_opp_select.h b/include/soc/rockchip/rockchip_opp_select.h
new file mode 100644
index 000000000..f24197854
--- /dev/null
+++ b/include/soc/rockchip/rockchip_opp_select.h
@@ -0,0 +1,291 @@
+/*
+ * Copyright (c) 2017 Fuzhou Rockchip Electronics Co., Ltd
+ *
+ * SPDX-License-Identifier: GPL-2.0+
+ */
+#ifndef __SOC_ROCKCHIP_OPP_SELECT_H
+#define __SOC_ROCKCHIP_OPP_SELECT_H
+
+#include <linux/pm_opp.h>
+
+#define VOLT_RM_TABLE_END	~1
+
+/*
+ * [0]:      set intermediate rate
+ *           [1]: scaling up rate or scaling down rate
+ * [1]:      add length for pvtpll
+ *           [2:5]: length
+ * [2]:      use low length for pvtpll
+ * [3:5]:    reserved
+ */
+#define OPP_RATE_MASK		0x3f
+
+/* Set intermediate rate */
+#define OPP_INTERMEDIATE_RATE	BIT(0)
+#define OPP_SCALING_UP_RATE	BIT(1)
+#define OPP_SCALING_UP_INTER	(OPP_INTERMEDIATE_RATE | OPP_SCALING_UP_RATE)
+#define OPP_SCALING_DOWN_INTER	OPP_INTERMEDIATE_RATE
+
+/* Add length for pvtpll */
+#define OPP_ADD_LENGTH		BIT(1)
+#define OPP_LENGTH_MASK		0xf
+#define OPP_LENGTH_SHIFT	2
+
+/* Use low length for pvtpll */
+#define OPP_LENGTH_LOW		BIT(2)
+
+struct rockchip_opp_info;
+
+struct volt_rm_table {
+	int volt;
+	int rm;
+};
+
+struct rockchip_opp_data {
+	config_clks_t config_clks;
+	config_regulators_t config_regulators;
+
+	int (*get_soc_info)(struct device *dev, struct device_node *np,
+			    int *bin, int *process);
+	int (*set_soc_info)(struct device *dev, struct device_node *np,
+			    struct rockchip_opp_info *info);
+	int (*set_read_margin)(struct device *dev,
+			       struct rockchip_opp_info *info,
+			       u32 rm);
+};
+
+struct pvtpll_opp_table {
+	unsigned long rate;
+	unsigned long u_volt;
+	unsigned long u_volt_min;
+	unsigned long u_volt_max;
+	unsigned long u_volt_mem;
+	unsigned long u_volt_mem_min;
+	unsigned long u_volt_mem_max;
+};
+
+/**
+ * struct rockchip_opp_info - Rockchip opp info structure
+ * @dev:		The device.
+ * @dvfs_mutex:		Mutex to protect changing volage and scmi clock rate.
+ * @data:		Device-specific opp data.
+ * @opp_table:		Temporary opp table only used when enable pvtpll calibration.
+ * @pvtpll_avg_offset:	Register offset of pvtm value.
+ * @pvtpll_min_rate:	Minimum frequency which needs calibration.
+ * @pvtpll_volt_step:	Voltage step of pvtpll calibration.
+ * @volt_rm_tbl:	Pointer to voltage to memory read margin conversion table.
+ * @grf:		General Register Files regmap.
+ * @dsu_grf:		DSU General Register Files regmap.
+ * @clocks:		Pvtpll clocks.
+ * @nclocks:		Number of pvtpll clock.
+ * @intermediate_threshold_freq: The frequency threshold of intermediate rate.
+ * @low_rm:		The read margin threshold of intermediate rate.
+ * @current_rm:		Current memory read margin.
+ * @target_rm:		Target memory read margin.
+ * @is_runtime_active:	Marks if device's pd is power on.
+ * @opp_token:		Integer replacement for opp_table.
+ * @scale:		Frequency scale.
+ * @bin:		Soc version.
+ * @process:		Process version.
+ * @volt_sel:		Speed grade.
+ * @supported_hw:	Array of version number to support.
+ * @clk:		Device's clock handle.
+ * @is_scmi_clk:	Marks if device's clock is scmi clock.
+ * @regulators:		Supply regulators.
+ * @regulator_count:	Number of power supply regulators.
+ * @init_freq:		Set the initial frequency when init opp table.
+ * @is_rate_volt_checked: Marks if device has checked initial rate and voltage.
+ * @pvtpll_clk_id:      Device's clock id.
+ * @pvtpll_low_temp:    Marks if device has low temperature pvtpll config.
+ */
+struct rockchip_opp_info {
+	struct device *dev;
+	struct mutex dvfs_mutex;
+	const struct rockchip_opp_data *data;
+	struct pvtpll_opp_table *opp_table;
+	unsigned int pvtpll_avg_offset;
+	unsigned int pvtpll_min_rate;
+	unsigned int pvtpll_volt_step;
+
+	struct volt_rm_table *volt_rm_tbl;
+	struct regmap *grf;
+	struct regmap *dsu_grf;
+	struct clk_bulk_data *clocks;
+	int nclocks;
+	unsigned long intermediate_threshold_freq;
+	u32 low_rm;
+	u32 current_rm;
+	u32 target_rm;
+	bool is_runtime_active;
+
+	int opp_token;
+	int scale;
+	int bin;
+	int process;
+	int volt_sel;
+	u32 supported_hw[2];
+
+	struct clk *clk;
+	bool is_scmi_clk;
+	struct regulator **regulators;
+	int regulator_count;
+	unsigned int init_freq;
+	bool is_rate_volt_checked;
+
+	u32 pvtpll_clk_id;
+	bool pvtpll_low_temp;
+};
+
+#if IS_ENABLED(CONFIG_ROCKCHIP_OPP)
+int rockchip_of_get_leakage(struct device *dev, char *lkg_name, int *leakage);
+int rockchip_nvmem_cell_read_u8(struct device_node *np, const char *cell_id,
+				u8 *val);
+int rockchip_nvmem_cell_read_u16(struct device_node *np, const char *cell_id,
+				 u16 *val);
+void rockchip_get_opp_data(const struct of_device_id *matches,
+			   struct rockchip_opp_info *info);
+void rockchip_opp_dvfs_lock(struct rockchip_opp_info *info);
+void rockchip_opp_dvfs_unlock(struct rockchip_opp_info *info);
+int rockchip_init_opp_info(struct device *dev, struct rockchip_opp_info *info,
+			   char *clk_name, char *reg_name);
+void rockchip_uninit_opp_info(struct device *dev, struct rockchip_opp_info *info);
+int rockchip_adjust_opp_table(struct device *dev, struct rockchip_opp_info *info);
+int rockchip_get_read_margin(struct device *dev,
+			     struct rockchip_opp_info *info,
+			     unsigned long volt, u32 *target_rm);
+int rockchip_set_read_margin(struct device *dev,
+			     struct rockchip_opp_info *info, u32 rm,
+			     bool is_set_rm);
+int rockchip_set_intermediate_rate(struct device *dev,
+				   struct rockchip_opp_info *info,
+				   struct clk *clk, unsigned long old_freq,
+				   unsigned long new_freq, bool is_scaling_up,
+				   bool is_set_clk);
+int rockchip_opp_config_regulators(struct device *dev,
+				     struct dev_pm_opp *old_opp,
+				     struct dev_pm_opp *new_opp,
+				     struct regulator **regulators,
+				     unsigned int count,
+				     struct rockchip_opp_info *info);
+int rockchip_opp_config_clks(struct device *dev, struct opp_table *opp_table,
+			     struct dev_pm_opp *opp, void *data,
+			     bool scaling_down, struct rockchip_opp_info *info);
+int rockchip_opp_check_rate_volt(struct device *dev, struct rockchip_opp_info *info);
+int rockchip_init_opp_table(struct device *dev, struct rockchip_opp_info *info,
+			    char *clk_name, char *reg_name);
+void rockchip_uninit_opp_table(struct device *dev,
+			       struct rockchip_opp_info *info);
+#else
+static inline int rockchip_of_get_leakage(struct device *dev, char *lkg_name,
+					  int *leakage)
+{
+	return -EOPNOTSUPP;
+}
+
+static inline int rockchip_nvmem_cell_read_u8(struct device_node *np,
+					      const char *cell_id, u8 *val)
+{
+	return -EOPNOTSUPP;
+}
+
+static inline int rockchip_nvmem_cell_read_u16(struct device_node *np,
+					       const char *cell_id, u16 *val)
+{
+	return -EOPNOTSUPP;
+}
+
+static inline void rockchip_get_opp_data(const struct of_device_id *matches,
+					 struct rockchip_opp_info *info)
+{
+}
+
+static inline void rockchip_opp_dvfs_lock(struct rockchip_opp_info *info)
+{
+}
+
+static inline void rockchip_opp_dvfs_unlock(struct rockchip_opp_info *info)
+{
+}
+
+static inline int
+rockchip_init_opp_info(struct device *dev, struct rockchip_opp_info *info,
+		       char *clk_name, char *reg_name)
+{
+	return -EOPNOTSUPP;
+}
+
+static inline void
+rockchip_uninit_opp_info(struct device *dev, struct rockchip_opp_info *info)
+{
+}
+
+static inline int
+rockchip_adjust_opp_table(struct device *dev, struct rockchip_opp_info *info)
+{
+	return -EOPNOTSUPP;
+}
+
+static inline int rockchip_get_read_margin(struct device *dev,
+					   struct rockchip_opp_info *info,
+					   unsigned long volt, u32 *target_rm)
+{
+	return -EOPNOTSUPP;
+}
+static inline int rockchip_set_read_margin(struct device *dev,
+					   struct rockchip_opp_info *info,
+					   u32 rm, bool is_set_rm)
+{
+	return -EOPNOTSUPP;
+}
+
+static inline int
+rockchip_set_intermediate_rate(struct device *dev,
+			       struct rockchip_opp_info *opp_info,
+			       struct clk *clk, unsigned long old_freq,
+			       unsigned long new_freq, bool is_scaling_up,
+			       bool is_set_clk)
+{
+	return -EOPNOTSUPP;
+}
+
+static inline int
+rockchip_opp_config_regulators(struct device *dev,
+			       struct dev_pm_opp *old_opp,
+			       struct dev_pm_opp *new_opp,
+			       struct regulator **regulators,
+			       unsigned int count,
+			       struct rockchip_opp_info *info)
+{
+	return -EOPNOTSUPP;
+}
+
+static inline int rockchip_opp_config_clks(struct device *dev,
+					   struct opp_table *opp_table,
+					   struct dev_pm_opp *opp, void *data,
+					   bool scaling_down,
+					   struct rockchip_opp_info *info)
+{
+	return -EOPNOTSUPP;
+}
+
+static inline int rockchip_opp_check_rate_volt(struct device *dev,
+					       struct rockchip_opp_info *info)
+{
+	return -EOPNOTSUPP;
+}
+
+static inline int
+rockchip_init_opp_table(struct device *dev, struct rockchip_opp_info *info,
+			char *clk_name, char *reg_name)
+{
+	return -EOPNOTSUPP;
+}
+
+static inline void rockchip_uninit_opp_table(struct device *dev,
+					     struct rockchip_opp_info *info)
+{
+}
+
+#endif /* CONFIG_ROCKCHIP_OPP */
+
+#endif
diff --git a/include/soc/rockchip/rockchip_sip.h b/include/soc/rockchip/rockchip_sip.h
index c46a9ae2a..5de684449 100644
--- a/include/soc/rockchip/rockchip_sip.h
+++ b/include/soc/rockchip/rockchip_sip.h
@@ -15,6 +15,16 @@
 #define ROCKCHIP_SIP_CONFIG_DRAM_GET_RATE	0x05
 #define ROCKCHIP_SIP_CONFIG_DRAM_CLR_IRQ	0x06
 #define ROCKCHIP_SIP_CONFIG_DRAM_SET_PARAM	0x07
+#define ROCKCHIP_SIP_CONFIG_DRAM_GET_VERSION	0x08
+#define ROCKCHIP_SIP_CONFIG_DRAM_POST_SET_RATE	0x09
+#define ROCKCHIP_SIP_CONFIG_DRAM_SET_MSCH_RL	0x0a
+#define ROCKCHIP_SIP_CONFIG_DRAM_DEBUG		0x0b
+#define ROCKCHIP_SIP_CONFIG_MCU_START		0x0c
+#define ROCKCHIP_SIP_CONFIG_DRAM_ECC		0x0d
+#define ROCKCHIP_SIP_CONFIG_DRAM_GET_FREQ_INFO	0x0e
+#define ROCKCHIP_SIP_CONFIG_DRAM_ADDRMAP_GET	0x10
+#define ROCKCHIP_SIP_CONFIG_DRAM_GET_STALL_TIME	0x11
+
 #define ROCKCHIP_SIP_CONFIG_DRAM_SET_ODT_PD	0x08
 
 #endif
diff --git a/include/soc/rockchip/rockchip_system_monitor.h b/include/soc/rockchip/rockchip_system_monitor.h
new file mode 100644
index 000000000..4df62aa37
--- /dev/null
+++ b/include/soc/rockchip/rockchip_system_monitor.h
@@ -0,0 +1,215 @@
+/* SPDX-License-Identifier: (GPL-2.0+ OR MIT) */
+/*
+ * Copyright (C) 2019, Fuzhou Rockchip Electronics Co., Ltd
+ */
+
+#ifndef __SOC_ROCKCHIP_SYSTEM_MONITOR_H
+#define __SOC_ROCKCHIP_SYSTEM_MONITOR_H
+
+#include <linux/pm_opp.h>
+#include <linux/pm_qos.h>
+#include <linux/regulator/consumer.h>
+
+enum monitor_dev_type {
+	MONITOR_TYPE_CPU = 0,	/* CPU */
+	MONITOR_TYPE_DEV,	/* GPU, NPU, DMC, and so on */
+};
+
+enum system_monitor_event_type {
+	SYSTEM_MONITOR_CHANGE_TEMP = 0,
+};
+
+struct system_monitor_event_data {
+	int temp;
+};
+
+struct volt_adjust_table {
+	unsigned int min;	/* Minimum frequency in MHz */
+	unsigned int max;	/* Maximum frequency in MHz */
+	int volt;		/* Voltage in microvolt */
+};
+
+struct temp_freq_table {
+	int temp;		/* millicelsius */
+	unsigned int freq;	/* KHz */
+};
+
+/**
+ * struct temp_opp_table - System monitor device OPP description structure
+ * @rate:		Frequency in hertz
+ * @volt:		Target voltage in microvolt
+ * @mem_volt:		Target voltage for memory in microvolt
+ * @low_temp_volt:	Target voltage when low temperature, in microvolt
+ * @low_temp_mem_volt:	Target voltage for memory when low temperature,
+ *			in microvolt
+ * @max_volt:		Maximum voltage in microvolt
+ * @max_mem_volt:	Maximum voltage for memory in microvolt
+ */
+struct temp_opp_table {
+	unsigned long rate;
+	unsigned long volt;
+	unsigned long mem_volt;
+	unsigned long low_temp_volt;
+	unsigned long low_temp_mem_volt;
+	unsigned long max_volt;
+	unsigned long max_mem_volt;
+};
+
+/**
+ * struct monitor_dev_info - structure for a system monitor device
+ * @dev:		Device registered by system monitor
+ * @low_temp_adjust_table:	Voltage margin for different OPPs when lowe
+ *				temperature
+ * @opp_table:		Frequency and voltage information of device
+ * @devp:		Device-specific system monitor profile
+ * @node:		Node in monitor_dev_list
+ * @high_limit_table:	Limit maximum frequency at different temperature,
+ *			but the frequency is also changed by thermal framework.
+ * @max_temp_freq_req:	CPU maximum frequency constraint changed according
+ *			to temperature.
+ * @min_sta_freq_req:   CPU minimum frequency constraint changed according
+ *			to system status.
+ * @max_sta_freq_req:   CPU maximum frequency constraint changed according
+ *			to system status.
+ * @dev_max_freq_req:	Devices maximum frequency constraint changed according
+ *			to temperature.
+ * @early_reg:		Supply regulator during kernel startup.
+ * @low_limit:		Limit maximum frequency when low temperature, in Hz
+ * @high_limit:		Limit maximum frequency when high temperature, in Hz
+ * @max_volt:		Maximum voltage in microvolt
+ * @low_temp_min_volt:	Minimum voltage of OPPs when low temperature, in
+ *			microvolt
+ * @high_temp_max_volt:	Maximum voltage when high temperature, in microvolt
+ * @video_4k_freq:	Maximum frequency when paly 4k video, in KHz
+ * @reboot_freq:	Limit maximum and minimum frequency when reboot, in KHz
+ * @status_min_limit:	Minimum frequency of some status frequency, in KHz
+ * @status_max_limit:	Minimum frequency of all status frequency, in KHz
+ * @early_min_volt:	Minimum voltage during kernel startup.
+ * @low_temp:		Low temperature trip point, in millicelsius
+ * @high_temp:		High temperature trip point, in millicelsius
+ * @temp_hysteresis:	A low hysteresis value on low_temp, in millicelsius
+ * @is_low_temp:	True if current temperature less than low_temp
+ * @is_high_temp:	True if current temperature greater than high_temp
+ * @is_low_temp_enabled:	True if device node contains low temperature
+ *				configuration
+ */
+struct monitor_dev_info {
+	struct device *dev;
+	struct volt_adjust_table *low_temp_adjust_table;
+	struct temp_opp_table *opp_table;
+	struct monitor_dev_profile *devp;
+	struct list_head node;
+	struct temp_freq_table *high_limit_table;
+	struct freq_qos_request max_temp_freq_req;
+	struct freq_qos_request min_sta_freq_req;
+	struct freq_qos_request max_sta_freq_req;
+	struct dev_pm_qos_request dev_max_freq_req;
+	struct regulator *early_reg;
+	unsigned long low_limit;
+	unsigned long high_limit;
+	unsigned long max_volt;
+	unsigned long low_temp_min_volt;
+	unsigned long high_temp_max_volt;
+	unsigned int video_4k_freq;
+	unsigned int reboot_freq;
+	unsigned int status_min_limit;
+	unsigned int status_max_limit;
+	unsigned int early_min_volt;
+	int low_temp;
+	int high_temp;
+	int temp_hysteresis;
+	bool is_low_temp;
+	bool is_high_temp;
+	bool is_low_temp_enabled;
+};
+
+struct monitor_dev_profile {
+	enum monitor_dev_type type;
+	void *data;
+	int (*low_temp_adjust)(struct monitor_dev_info *info, bool is_low);
+	int (*high_temp_adjust)(struct monitor_dev_info *info, bool is_low);
+	int (*check_rate_volt)(struct monitor_dev_info *info);
+	struct cpumask allowed_cpus;
+	struct rockchip_opp_info *opp_info;
+};
+
+#if IS_REACHABLE(CONFIG_ROCKCHIP_SYSTEM_MONITOR)
+struct monitor_dev_info *
+rockchip_system_monitor_register(struct device *dev,
+				 struct monitor_dev_profile *devp);
+void rockchip_system_monitor_unregister(struct monitor_dev_info *info);
+int rockchip_monitor_cpu_low_temp_adjust(struct monitor_dev_info *info,
+					 bool is_low);
+int rockchip_monitor_cpu_high_temp_adjust(struct monitor_dev_info *info,
+					  bool is_high);
+int rockchip_monitor_check_rate_volt(struct monitor_dev_info *info);
+int rockchip_monitor_dev_low_temp_adjust(struct monitor_dev_info *info,
+					 bool is_low);
+int rockchip_monitor_dev_high_temp_adjust(struct monitor_dev_info *info,
+					  bool is_high);
+int rockchip_monitor_suspend_low_temp_adjust(int cpu);
+int rockchip_system_monitor_register_notifier(struct notifier_block *nb);
+void rockchip_system_monitor_unregister_notifier(struct notifier_block *nb);
+#else
+static inline struct monitor_dev_info *
+rockchip_system_monitor_register(struct device *dev,
+				 struct monitor_dev_profile *devp)
+{
+	return ERR_PTR(-ENOTSUPP);
+};
+
+static inline void
+rockchip_system_monitor_unregister(struct monitor_dev_info *info)
+{
+}
+
+static inline int
+rockchip_monitor_cpu_low_temp_adjust(struct monitor_dev_info *info, bool is_low)
+{
+	return 0;
+};
+
+static inline int
+rockchip_monitor_cpu_high_temp_adjust(struct monitor_dev_info *info,
+				      bool is_high)
+{
+	return 0;
+};
+
+static inline int
+rockchip_monitor_check_rate_volt(struct monitor_dev_info *info)
+{
+	return 0;
+}
+
+static inline int
+rockchip_monitor_dev_low_temp_adjust(struct monitor_dev_info *info, bool is_low)
+{
+	return 0;
+};
+
+static inline int
+rockchip_monitor_dev_high_temp_adjust(struct monitor_dev_info *info,
+				      bool is_high)
+{
+	return 0;
+};
+
+static inline int rockchip_monitor_suspend_low_temp_adjust(int cpu)
+{
+	return 0;
+};
+
+static inline int
+rockchip_system_monitor_register_notifier(struct notifier_block *nb)
+{
+	return 0;
+};
+
+static inline void
+rockchip_system_monitor_unregister_notifier(struct notifier_block *nb)
+{
+};
+#endif /* CONFIG_ROCKCHIP_SYSTEM_MONITOR */
+
+#endif
